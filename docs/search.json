[
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "plan",
    "section": "",
    "text": "Writing Plan\n\n\nCurrent Progress\n\n\n\nSection\nPercentage\n\n\n\n\nVariability Intro\n30%\n\n\nIGAS Study\n90%\n\n\nHTW Study\n40%\n\n\n* Function Learning Review\n0%\n\n\n* Function Learning + Variability connection\n0%\n\n\n* Experimental Methods\n40%\n\n\n* Results\n10%\n\n\n* Modelling\n10%\n\n\n* Discussion\n0%\n\n\nSynthesis Model\n0%\n\n\nGeneral Discussion\n0%",
    "crumbs": [
      "plan"
    ]
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The factors that influence how learning generalizes are of great interest to those seeking to understanding the nature of the human learning system, and to those attempting to improve the efficacy of teaching and training programs. Such factors are likely legion, but the present effort will limit itself to two characteristics of the learning input - variability and frequency skew. Variability manipulations control either the number of unique instances learners are trained on, or how dispersed or spread out the instances are. Much less research has examined skew manipulations, which hold the number of instances constant, and instead vary the relative frequencies with which the training items occur. Both factors have been linked to generalization in past research. The present essay with will provide an extensive review and discussion of both topics. The review of the expansive literature on the effect of attempts to include at least some examples from each of the many relevant domains, but much of the work is biased towards the background of the author. Conversely, the literature on skew proved to be far sparser than anticipated, as well as being overrepresented in a region of the language learning literature that is somewhat distant from more conventional cognitive psychology. ## The study of variability\nIn their most generic form, studies assessing the “benefits of variability” hypothesis assign participants to either a constant or varied group for the training stage of the experiment. Then, subjects in both groups complete an identical testing stage which often consists items/conditions seen during training, and novel items/conditions. If the varied group performs better in the testing stage, this is taken for evidence of the benefits of variability hypothesis. Even in this relatively simple between-groups design, researchers must make a number of crucial decisions which will now be highlighted.\n\nWhat is varied. Unless the task is unidimensional, there will often be many variables that could be varied or held fixed, and the experiment will need to decide the dimension(s) along which the variation will occur. For instance, in a projectile throwing accuracy task – researchers might vary the distance from the target, the size of the target, the weight of the projectile. They might also vary a contextual variable not directly relevant to the task, but which will still be encoded by the subject on a trial by trial basis, e.g. the background color.\nThe amount of variation, relative to the control condition. The simplest comparison would be to compare a constant group who trains with 1 example/condition, against a varied group that trains from 2 examples/conditions. However, it is not uncommon in the literature for the varied condition to train from 3 or 4 conditions. For example, Catalano & Kleiner (1984) train varied subjects from 4 different velocities in their coincident timing task, and (Goode et al., 2008) have varied subjects’ practice with 3 different variants (i.e. different letter scrambles of the same word) of an anagram for a given word, while their constant participants view the same variant 3 different times. Alternatively, rather than a constant vs. varied comparison, subjects in all conditions might experience a variety of training items, but with one group experiencing a greater number of unique items (Nosofsky et al., 2018).\nLocations of the varied examples within the task-space. For tasks in which the stimuli or conditions exist in some continuous metric space, the experimenter must decide whether the varied instances are relatively close together (e.g. throwing a ball from a distance of 4 feet and 5 feet), or far apart (throwing from 4 feet and 20 feet). Spreading the varied training items further apart may be beneficial in terms of providing a more representative sample of the task space to the learner, however large distances may also result in significant differences in difficulty between the training examples, which can be a common confound in variability studies.\nThe proximity of the testing conditions to the training conditions of the varied and constant groups. Intuitively, the fairest form of comparison is to include testing conditions that are of an equivalent distance from both the varied and constant groups. However researchers might also attempt to demonstrate the benefits of variation as being sufficiently powerful to outperform constant training, even in cases where the constant group trained from a closer proximity to the testing conditions, or whose training conditions are identical to the testing conditions (Goode et al., 2008; Kerr & Booth, 1978).\n\nVariability Literature Review\nAn early and influential work on the influence of variability on category learning is that of Posner & Keele (1968). In an ambitious attempt to address the question of how category information is represented, the authors trained participants to categorize artificial dot patterns, manipulating whether learners were exposed to examples clustered close to the category prototypes (e.g. a low variability condition), or spread further away from the prototype (the varied-training group). It should be noted that both groups in this study were trained with the same number of unique instances and the manipulated difference was how spread out the instances were. The authors claim based on prior experiments using the same stimuli, that the training stimuli for the varied group were at least as far away from the testing stimuli as the training stimuli of the less-varied group. The authors interpreted their findings as evidence for the extraction of an abstraction or schema that is extracted and stored, and then over time becomes more likely to be the reference point from which generalization occurs, given that specific instances are thought to decay at a faster rate than prototypes or schema. The Posner and Keele study have been extremely influential and continues to be cited in contemporary research as clear evidence that schema abstraction underlies the benefits of varied training. It’s also referenced as a key influence in the development of Schema Theory of Motor Learning Schmidt (1975), which in turn influenced decades of investigations on the potential benefits of varied training in motor skill learning. However, the classic Posner & Keele study despite being far more carefully designed than many subsequent studies, and despite being a relative rarity in explicitly discussing and attempting to control for potential confounds of similarity between groups, may nevertheless be emblematic of a common issue in many investigations of the effects of varied training on learning. The problem with Posner & Keele’s conclusion was demonstrated clearly almost 3 decades later (Palmeri & Nosofsky, 2001), when researchers conducting a near replication of the original study also collected similarity judgements following training and performed multidimensional scaling analysis. Rather than being in the middle of the training stimuli as was the case in the physical stimuli space, the psychological representation of the prototype was shown reside at an extreme point, and generalization patterns by participants that would have seemed to warrant the learning of a prototype were then easily accounted for with only the assumption that the participants encoded instances. One of the primary concerns of the present paper is that many of the studies which purport to explain the benefits of variation via prototypes, schemas, or other abstractions, are often overlooking the potential of instance based similarity accounts.\n\n\n\nTraining variation has also been shown to promote transfer in motor learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes a priori that learners possess general motor programs for classes of movements, such as an underhand throw. When called up for use, such programs must be parameterized, as well as schema rules that determine how a motor program is parameterized or scaled for a particular movement. Schema theory predicts that varied training results in the formation of a more general schema-rule, which can allow for transfer to novel movements within a given movement class, such as an underhand throw (though it is agnostic to the development of the movement classes themselves). Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, investigators might train two groups of participants to throw a projectile at a target, with a constant group that throws from a single location, and a varied group that throws from multiple locations. Both groups are then tested from novel locations.\nOne of the earliest, and still often cited investigations of Schmidt’s benefits of variability hypothesis was the work of Kerr & Booth (1978). Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blind folded for testing and did not receive trial by trial feedback in this stage. However, at the halfway point of the testing stage they were allowed to see the landing location of the 4 beanbags they had thrown, and then completed the final 4 testing throws. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. Although this design does not directly assess the hypothesis of varied training producing superior generalization to constant training (since the constant group is not tested from a novel position), it nevertheless offers a compelling example of the merits of varied practice.\nOn occasion the Kerr and Booth design may be nested within a larger experimental design. One such study that used a movement timing task, wherein subjects had to move their hand from a starting location, to a target location, attempting to arrive at the target location at specific time following the onset of a cue (Wrisberg et al., 1987). This study utilized 4 different constant groups, and 3 varied groups, with one of the constant groups training under conditions identical to the testing conditions, and which were not trained on by any of the varied groups, e.g. the design of Kerr and Booth. However, in this case the varied group did not outperform the constant group. A more recent study attempting a slightly more direct replication of the original Kerr & Booth study (Willey & Liu, 2018), having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). However this study diverged from the original in that the participants were adults; they faced away from the target and threw the beanbag backwards over their bodies; they alternated using their right and left hands every 6 trials; and underwent a relatively extreme amount of training (20 sessions with 60 practice trials each, spread out over 5-7 weeks). Like Wrisberg et al. (1987), this study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups.\nSome support for the Kerr and Booth findings was found with a relatively less common experimental task of training participants in hitting a projectile at a target with the use of a racket (Green et al., 1995). Varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket. One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. Of course, this finding is less surprising than that of Kerr & Booth, given that varied subjects did have some prior exposure to the constant groups condition. This highlights an issue rarely discussed in the literature, of how much practice from an additional position might be necessary to induce benefits. Experimenters almost uniformly have varied participants train with an equivalent number of trials from each of their conditions\nOne of the few studies that has replicated the surprising result of varied outperforming constant, from the constant training condition, did so in the relatively distant domain of verbal manipulation (Goode et al., 2008). All participants trained to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g. LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had 3 prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups – solving 2 more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution – further suggesting that there may not have been a strong identity advantage in this task.\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters (Catalano & Kleiner, 1984; Chua et al., 2019; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976), or for a single constant group to train from just one of the conditions experienced by the varied participants (Pigott & Shapiro, 1984; Roller et al., 2001; Wrisberg & McLean, 1984; Wrisberg & Mead, 1981). A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition (Wrisberg et al., 1987; Wulf, 1991).\nOf particular relevant to the current essay is the early work of Catalano and Kleiner (1984), as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions – 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\nIn the category learning literature, the constant vs. varied comparison is much less suitable. Instead, researchers tend to compare a condition with many repetitions of a few items against condition with fewer repetitions of a wider array of exemplars. Much of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns (Homa & Vosburgh, 1976; Posner & Keele, 1968), where more varied or distorted training examples were often shown to produce superior generalize when categorizing novel exemplars. More recently, researchers have also begun to utilize more realistic stimuli in their experiments. Wahlheim et al. (2012) conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address on unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization (Nosofsky et al., 2018). For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions. Moreover, the pattern of results in this study could be nicely accounted for by an extended version of the Generalized Context Model.\nThe studies described thus far have studied the benefits of variability by exposing participants to a greater or lesser number of distinct examples during training. A distinct sub-literature within the category learning domain has focused much less on benefits derived from varied training, instead emphasizing how increased variability during the learning of a novel category influences how far the category boundary will then be generalized. The general approach is to train participants on examples from two categories, with the examples from one of the categories being more dispersed than the other. Participants are then tested with novel items located within ambiguous regions of the task space which allow the experimenters to assess whether the difference in variability influences how far participants generalize the category boundaries.\nCohen et al. (2001) trained subjects on two categories, one with much more variability than the other. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into a category with greater variation. Moreover, this effect was at odds of the predications of the baseline version of the GCM, thus providing some evidence that training variation may at least sometimes induce effects that cannot be entirely accounted for by exemplar-similarity accounts.\nFurther observations consonant with the results of Cohen et al. (2001) have since been observed in numerous investigations (Hahn et al., 2005; Hsu & Grifﬁths, 2010; Perlman et al., 2012; Sakamoto et al., 2008). The results of Sakamoto et al. (2008) are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\nThe study by Hahn et al. (2005), is also of particular interest to the present discussion. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with “neither”, in addition to responses to the two training categories. The “neither” response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The authors then used the standard GCM framework to compare a variety of similarity-based models to account for their results. Of particular interest are their evaluations of a category response bias parameter, and a similarity scaling parameter. A model fit improvement when the response bias parameter is allowed to vary between the high-variability and low-variability trained groups is taken to suggest a simple bias for responding with one of the trained categories over the other. Alternatively, an improvement in fit due to a separate similarity scaling parameter may reflect the groups being differentially sensitive to the distances between stimuli. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improvement significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items – accounting for their extended generalization gradients, but also for their poor performance in a recognition condition. Additional model comparisons suggested that this similarity rescaling applied across the entire stimulus space, rather than to the high variability category in particular.\nVariability effects have also been examined in the higher-level domain of how learners acquire novel concepts, and then instantiate (rather than merely recognize) that concept in untrained contexts (Braithwaite & Goldstone, 2015). This study trained participants on problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g. all involving people selecting objects) or in which the surface features were varied between examples (e.g. people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants – such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples.\nVariability has also been examined within the realm of language learning. A particularly impressive study is that of Perry et al. (2010). In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the tight condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. The varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category, a pattern consistent with much of the existing literature. More interesting was the superior performance of the varied group on a higher order generalization task – such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.\n\n\n\n\n\nA necessary consequence of varied training is that participants will have the experience of switching from one task condition to another. The number of switches can vary greatly, with the two extremes being varied participants completing all of their training trials in one before switching to the next condition (blocked sequencing), or if alternate between conditions on a trial by trial basis (random/intermixed/interleaved sequencing). Not long after the initial influx of schema-theory inspired studies testing the benefits of variability hypothesis it was shown that the influence of varied training might interact with the type of training sequence chosen by the experimenter (J. B. Shea & Morgan, 1979). In this seminal study, both groups of training subjects trained with the same number of trials of three separate movement patterns. A blocked group that completed all of their trials with one sequence before beginning the next sequence, and a random group that trained with all three movement patterns interspersed throughout the course of training. Participants were also randomly assigned to retention testing under either blocked or random sequence conditions, thus resulting in all four training-testing combinations of blocked-blocked; blocked-random; random-blocked; random-random. There was some effect of sequence context, such that both groups performed better when the testing sequence matched their training sequence. However, the main finding of interest was the advantage random-training, which resulted in superior testing performance than blocked training regardless of whether the testing stage had a blocked or random sequence, an effect observed both immediately after training, and in a follow up test ten days after the end of training.\nPrior to the influential Shea and Morgan (1979) study, studies investigating the benefits of variability hypothesis had utilized both blocked and random training schedules, often without comment or justification. It was later observed (Lee et al., 1985) that positive evidence for benefits of varied training seemed more likely to occur for studies that utilized random schedules. The theoretical basis of such studies was invariably an appeal to Schmidt’s schema theory; however schema theory made no clear predictions of an effect of study sequence on retention or generalization, thus prompting the need for alternate accounts. One such account, the elaborative processing account (J. B. Shea & Zimny, 1983), draws on the earlier work of Battig (Battig, 1966) and argues that randomly sequencing conditions during training promotes comparison and contrastive processes between those conditions, which result in a deeper understanding of the training task than could arise via blocked sequencing. Supporting evidence for elaborative processing comes in the form of random-sequence trained subjects self-reporting more nuanced mental representations of movement patterns following training (J. B. Shea & Zimny, 1983), and by manipulating whether subjects are able to perform comparisons during training (Wright et al., 1992). An alternative, though not incompatible account suggests that the benefits of random-sequencing are a result of such sequences forcing the learner to continually reconstruct the relevant motor task in working memory (Lee & Magill, 1985). Blocked training, on the other hand, allows the learner to maintain the same motor task in short term memory without decay for much of the training which facilitates training performance, but hinders ability to retrieve the appropriate motor memory in a later testing context. A much more recent study (Chua et al., 2019), replicates the standard findings of an advantage of varied training over constant training (expt 1, bean-bag throwing task), and of random training over blocked training (expt 2 & 3, bean-bag throwing & golf putting). The novelty of this study is that the experimenters queried subjects about their attentional focus throughout the training stage. In all three experiments varied or random trained-subjects reported significantly greater external attention (e.g. attending to the target distance), and constant or blocked subjects reported more internal attention (e.g. posture or hand position). The authors argue that the benefits of varied/random training may be mediated by changes in attentional focus, however the claims made in the paper seem to go far beyond what can be justified by the analyses reported – e.g. the increased external focus could be a simple byproduct of varied training. A stronger form of evidence that was not provided may have been to use multiple regression analyses to show that the testing advantage of the varied/random groups over the constant/blocked groups could be accounted for by the differences in self-reported attentional focus.\nOther task and participant effects\nOf course, the effects of varied training, and different training sequences, are likely to be far more complex than simply more varied training being better than less, or random training being better than blocked. Null effects of both manipulations have been reported (Magill & Hall, 1990; Van Rossum, 1990 for reviews), and a variety of moderators have emerged. In one of the earlier examples of the complex relationship between study sequence and learning (Del Rey et al., 1982), experimenters recruited participants who self-reported either large amounts, or very little experience with athletic actives, and then trained participants on a coincident timing task under with either a single constant training velocity, or with four training velocities with either blocked, or random training sequence conditions - resulting in 6 experimental conditions: (athlete vs. non-athlete) x (constant vs. blocked vs. random training). Athlete participants had superior performance during training, regardless of sequence condition, and training performance was superior for all subjects in the constant group, followed by blocked training in the middle, and then random training resulting in the worst training performance. Of greater interest is the pattern of testing results for novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received random training, followed by blocked, and then constant training. Non-athletes showed an almost opposite pattern, with superior performance for those who had constant training. A similar pattern was later observed in a golf-putting training study, wherein participants who had some prior golf experience benefited most from random-sequenced training, and participants with no golf experience benefited most from blocked training (Guadagnoli et al., 1999). More recently, the same pattern was observed in the concept learning literature (Braithwaite & Goldstone, 2015, expt 1). This study trained participants on a mathematical concept and found that participants who self-reported some prior experience with the concept improved more from pre-test to post-test after training with varied examples, while participants who reported no prior experience showed greater gains following training with highly similar examples.\nIn addition to the influence of prior experiences described above, ample evidence also suggests that numerous aspects of the experiment may also interact with the influence of variation. One important study examined the impact of the amount of training completed in a force production task (C. H. Shea et al., 1990). This study employed a typical blocked vs. random training procedure, but with the additional manipulation of separate groups receiving 50, 200, or 400 total training trials. For the group that received only 50 training trials retention was best when training had been blocked. However, for the conditions that received 200 or 400 training trials the pattern was reversed, with random training resulting in superior retention than blocked training. These results were taken to suggest that the benefits of randomization may take time to emerge. Another experimental factor shown to interact with training sequence is the complexity of the training task (Albaret & Thon, 1998). In addition to random or blocked training, participants in this study were assigned to train on a drawing task at one of three different levels of complexity (reproducing from memory shapes composed of 2, 3 or 4 components). On a transfer task 48 hours after the completion of training, only participants trained at the lower levels of task complexity (2 or 3 components) showed superior performance to the blocked condition. The authors suggest that the benefits of random sequencing, thought to arise from more elaborate cognitive processing, or the necessity of continually recalling task information from long term into short term memory, are more likely to be obscured as the complexity of the task forces the blocked participants to also engage in such processes.\nA final important influence of particular relevance to the practice-sequence literature concerns the exact structure of “random” sequencing. Although the term random is commonly used for convenience, experimenters do not typically leave the order of training entirely up to chance. Rather, the training sequence is often constrained such that each condition must occur a minimum number of times in each quartile of the training phase, thus resulting in an even distribution the conditions throughout training. While the assurance of the conditions being evenly spread throughout training is consistent across studies, other aspects of the sequence structure are a bit more idiosyncratic. Some researchers report setting a maximum number of consecutive repetitions, e.g. no more than 2 consecutive trials of the same condition (Del Rey et al., 1982; J. B. Shea & Morgan, 1979), or structure the random trials such that the same condition never occurs consecutively (Wulf, 1991). Also common is to structure experiments such that random condition really consists of many small blocks, where participants do a few trials of one condition consecutively and then switch to another condition (Chua et al., 2019; Willey & Liu, 2018; Wrisberg et al., 1987), resulting in much more switches than would arise if training was perfectly blocked. The question of whether such differences in the structure of random sequencing are consequential has been addressed experimentally a few times, in all cases consisting of a 1) a no-repeat random condition; 2) a blocked random condition (typically 3 or 4 repeats before a switch); and 3) a standard fully-blocked condition. Blocked-random training resulted in better performance than either repeat-random, or fully - blocked training in both a bean-bag throwing (Pigott & Shapiro, 1984), and basketball shot training study (Landin & Hebert, 1997), and in a replication plus extension of the seminal Shea and Morgan (1979) study, blocked-random training was equally effective as no-repeat random training, with both random structures leading to better performance than the fully-blocked training condition. Consequences on different study schedules have also been repeatedly observed in the category learning literature (Carvalho & Goldstone, 2014, 2017). This line of research has revealed that the effects of blocking vs. interleaving can depend on the structure of the category being learned, and also that the different schedules can result in the participants requiring different representations. A fruitful line of inquiry in the motor skill learning literature may be to attempt to identify whether structural aspects of the motor task interact with different training sequences in a reliable manner.\nNumerous researchers have attempted to provide coherent frameworks to account for the full range of influences of training variation and sequencing described above (along with many other effects not discussed). Such accounts are generally quite similar, invoking ideas of desirable levels of difficulty(Bjork & Bjork, 2011; R.A. Schmidt & Bjork, 1992), or optimal challenge points (Guadagnoli & Lee, 2004). They tend to start by describing the dissociation between acquisition performance (performance during training) and testing performance (delayed retention and/or transfer), most strikingly observed as varied/random training participants performing worse than their constant/blocked counterparts during the training stage of the study, but then outperforming the constant/blocked comparisons at a later retention or transfer stage. This observation is then used to justify the idea that the most enduring and generalizable learning occurs by training at an optimal level of training difficulty, with difficulty being some function of the experience of the learner, and the cognitive or visuomotor processing demands of the task. It then follows that the factors that tend to make training more difficult (i.e. increased variability or randomization), are more likely to be beneficial when the learner has some experience, or when the processing demands of the task are not too extreme (which may only occur after some experience with the task). Such frameworks may be helpful heuristics in some cases, but they also seem to be overly flexible such that any null result of some intervention might be accounted for by a suboptimal amount of training trials, or by suggesting the training task was too difficult. The development of computational models that can account for how changes in the parameters of the motor-skill task scale with difficulty, would be a great step forward.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#introduction",
    "href": "Introduction.html#introduction",
    "title": "Introduction",
    "section": "",
    "text": "The factors that influence how learning generalizes are of great interest to those seeking to understanding the nature of the human learning system, and to those attempting to improve the efficacy of teaching and training programs. Such factors are likely legion, but the present effort will limit itself to two characteristics of the learning input - variability and frequency skew. Variability manipulations control either the number of unique instances learners are trained on, or how dispersed or spread out the instances are. Much less research has examined skew manipulations, which hold the number of instances constant, and instead vary the relative frequencies with which the training items occur. Both factors have been linked to generalization in past research. The present essay with will provide an extensive review and discussion of both topics. The review of the expansive literature on the effect of attempts to include at least some examples from each of the many relevant domains, but much of the work is biased towards the background of the author. Conversely, the literature on skew proved to be far sparser than anticipated, as well as being overrepresented in a region of the language learning literature that is somewhat distant from more conventional cognitive psychology. ## The study of variability\nIn their most generic form, studies assessing the “benefits of variability” hypothesis assign participants to either a constant or varied group for the training stage of the experiment. Then, subjects in both groups complete an identical testing stage which often consists items/conditions seen during training, and novel items/conditions. If the varied group performs better in the testing stage, this is taken for evidence of the benefits of variability hypothesis. Even in this relatively simple between-groups design, researchers must make a number of crucial decisions which will now be highlighted.\n\nWhat is varied. Unless the task is unidimensional, there will often be many variables that could be varied or held fixed, and the experiment will need to decide the dimension(s) along which the variation will occur. For instance, in a projectile throwing accuracy task – researchers might vary the distance from the target, the size of the target, the weight of the projectile. They might also vary a contextual variable not directly relevant to the task, but which will still be encoded by the subject on a trial by trial basis, e.g. the background color.\nThe amount of variation, relative to the control condition. The simplest comparison would be to compare a constant group who trains with 1 example/condition, against a varied group that trains from 2 examples/conditions. However, it is not uncommon in the literature for the varied condition to train from 3 or 4 conditions. For example, Catalano & Kleiner (1984) train varied subjects from 4 different velocities in their coincident timing task, and (Goode et al., 2008) have varied subjects’ practice with 3 different variants (i.e. different letter scrambles of the same word) of an anagram for a given word, while their constant participants view the same variant 3 different times. Alternatively, rather than a constant vs. varied comparison, subjects in all conditions might experience a variety of training items, but with one group experiencing a greater number of unique items (Nosofsky et al., 2018).\nLocations of the varied examples within the task-space. For tasks in which the stimuli or conditions exist in some continuous metric space, the experimenter must decide whether the varied instances are relatively close together (e.g. throwing a ball from a distance of 4 feet and 5 feet), or far apart (throwing from 4 feet and 20 feet). Spreading the varied training items further apart may be beneficial in terms of providing a more representative sample of the task space to the learner, however large distances may also result in significant differences in difficulty between the training examples, which can be a common confound in variability studies.\nThe proximity of the testing conditions to the training conditions of the varied and constant groups. Intuitively, the fairest form of comparison is to include testing conditions that are of an equivalent distance from both the varied and constant groups. However researchers might also attempt to demonstrate the benefits of variation as being sufficiently powerful to outperform constant training, even in cases where the constant group trained from a closer proximity to the testing conditions, or whose training conditions are identical to the testing conditions (Goode et al., 2008; Kerr & Booth, 1978).\n\nVariability Literature Review\nAn early and influential work on the influence of variability on category learning is that of Posner & Keele (1968). In an ambitious attempt to address the question of how category information is represented, the authors trained participants to categorize artificial dot patterns, manipulating whether learners were exposed to examples clustered close to the category prototypes (e.g. a low variability condition), or spread further away from the prototype (the varied-training group). It should be noted that both groups in this study were trained with the same number of unique instances and the manipulated difference was how spread out the instances were. The authors claim based on prior experiments using the same stimuli, that the training stimuli for the varied group were at least as far away from the testing stimuli as the training stimuli of the less-varied group. The authors interpreted their findings as evidence for the extraction of an abstraction or schema that is extracted and stored, and then over time becomes more likely to be the reference point from which generalization occurs, given that specific instances are thought to decay at a faster rate than prototypes or schema. The Posner and Keele study have been extremely influential and continues to be cited in contemporary research as clear evidence that schema abstraction underlies the benefits of varied training. It’s also referenced as a key influence in the development of Schema Theory of Motor Learning Schmidt (1975), which in turn influenced decades of investigations on the potential benefits of varied training in motor skill learning. However, the classic Posner & Keele study despite being far more carefully designed than many subsequent studies, and despite being a relative rarity in explicitly discussing and attempting to control for potential confounds of similarity between groups, may nevertheless be emblematic of a common issue in many investigations of the effects of varied training on learning. The problem with Posner & Keele’s conclusion was demonstrated clearly almost 3 decades later (Palmeri & Nosofsky, 2001), when researchers conducting a near replication of the original study also collected similarity judgements following training and performed multidimensional scaling analysis. Rather than being in the middle of the training stimuli as was the case in the physical stimuli space, the psychological representation of the prototype was shown reside at an extreme point, and generalization patterns by participants that would have seemed to warrant the learning of a prototype were then easily accounted for with only the assumption that the participants encoded instances. One of the primary concerns of the present paper is that many of the studies which purport to explain the benefits of variation via prototypes, schemas, or other abstractions, are often overlooking the potential of instance based similarity accounts.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#motor-skill-learning",
    "href": "Introduction.html#motor-skill-learning",
    "title": "Introduction",
    "section": "",
    "text": "Training variation has also been shown to promote transfer in motor learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes a priori that learners possess general motor programs for classes of movements, such as an underhand throw. When called up for use, such programs must be parameterized, as well as schema rules that determine how a motor program is parameterized or scaled for a particular movement. Schema theory predicts that varied training results in the formation of a more general schema-rule, which can allow for transfer to novel movements within a given movement class, such as an underhand throw (though it is agnostic to the development of the movement classes themselves). Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, investigators might train two groups of participants to throw a projectile at a target, with a constant group that throws from a single location, and a varied group that throws from multiple locations. Both groups are then tested from novel locations.\nOne of the earliest, and still often cited investigations of Schmidt’s benefits of variability hypothesis was the work of Kerr & Booth (1978). Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blind folded for testing and did not receive trial by trial feedback in this stage. However, at the halfway point of the testing stage they were allowed to see the landing location of the 4 beanbags they had thrown, and then completed the final 4 testing throws. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. Although this design does not directly assess the hypothesis of varied training producing superior generalization to constant training (since the constant group is not tested from a novel position), it nevertheless offers a compelling example of the merits of varied practice.\nOn occasion the Kerr and Booth design may be nested within a larger experimental design. One such study that used a movement timing task, wherein subjects had to move their hand from a starting location, to a target location, attempting to arrive at the target location at specific time following the onset of a cue (Wrisberg et al., 1987). This study utilized 4 different constant groups, and 3 varied groups, with one of the constant groups training under conditions identical to the testing conditions, and which were not trained on by any of the varied groups, e.g. the design of Kerr and Booth. However, in this case the varied group did not outperform the constant group. A more recent study attempting a slightly more direct replication of the original Kerr & Booth study (Willey & Liu, 2018), having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). However this study diverged from the original in that the participants were adults; they faced away from the target and threw the beanbag backwards over their bodies; they alternated using their right and left hands every 6 trials; and underwent a relatively extreme amount of training (20 sessions with 60 practice trials each, spread out over 5-7 weeks). Like Wrisberg et al. (1987), this study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups.\nSome support for the Kerr and Booth findings was found with a relatively less common experimental task of training participants in hitting a projectile at a target with the use of a racket (Green et al., 1995). Varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket. One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. Of course, this finding is less surprising than that of Kerr & Booth, given that varied subjects did have some prior exposure to the constant groups condition. This highlights an issue rarely discussed in the literature, of how much practice from an additional position might be necessary to induce benefits. Experimenters almost uniformly have varied participants train with an equivalent number of trials from each of their conditions\nOne of the few studies that has replicated the surprising result of varied outperforming constant, from the constant training condition, did so in the relatively distant domain of verbal manipulation (Goode et al., 2008). All participants trained to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g. LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had 3 prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups – solving 2 more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution – further suggesting that there may not have been a strong identity advantage in this task.\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters (Catalano & Kleiner, 1984; Chua et al., 2019; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976), or for a single constant group to train from just one of the conditions experienced by the varied participants (Pigott & Shapiro, 1984; Roller et al., 2001; Wrisberg & McLean, 1984; Wrisberg & Mead, 1981). A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition (Wrisberg et al., 1987; Wulf, 1991).\nOf particular relevant to the current essay is the early work of Catalano and Kleiner (1984), as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions – 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\nIn the category learning literature, the constant vs. varied comparison is much less suitable. Instead, researchers tend to compare a condition with many repetitions of a few items against condition with fewer repetitions of a wider array of exemplars. Much of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns (Homa & Vosburgh, 1976; Posner & Keele, 1968), where more varied or distorted training examples were often shown to produce superior generalize when categorizing novel exemplars. More recently, researchers have also begun to utilize more realistic stimuli in their experiments. Wahlheim et al. (2012) conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address on unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization (Nosofsky et al., 2018). For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions. Moreover, the pattern of results in this study could be nicely accounted for by an extended version of the Generalized Context Model.\nThe studies described thus far have studied the benefits of variability by exposing participants to a greater or lesser number of distinct examples during training. A distinct sub-literature within the category learning domain has focused much less on benefits derived from varied training, instead emphasizing how increased variability during the learning of a novel category influences how far the category boundary will then be generalized. The general approach is to train participants on examples from two categories, with the examples from one of the categories being more dispersed than the other. Participants are then tested with novel items located within ambiguous regions of the task space which allow the experimenters to assess whether the difference in variability influences how far participants generalize the category boundaries.\nCohen et al. (2001) trained subjects on two categories, one with much more variability than the other. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into a category with greater variation. Moreover, this effect was at odds of the predications of the baseline version of the GCM, thus providing some evidence that training variation may at least sometimes induce effects that cannot be entirely accounted for by exemplar-similarity accounts.\nFurther observations consonant with the results of Cohen et al. (2001) have since been observed in numerous investigations (Hahn et al., 2005; Hsu & Grifﬁths, 2010; Perlman et al., 2012; Sakamoto et al., 2008). The results of Sakamoto et al. (2008) are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\nThe study by Hahn et al. (2005), is also of particular interest to the present discussion. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with “neither”, in addition to responses to the two training categories. The “neither” response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The authors then used the standard GCM framework to compare a variety of similarity-based models to account for their results. Of particular interest are their evaluations of a category response bias parameter, and a similarity scaling parameter. A model fit improvement when the response bias parameter is allowed to vary between the high-variability and low-variability trained groups is taken to suggest a simple bias for responding with one of the trained categories over the other. Alternatively, an improvement in fit due to a separate similarity scaling parameter may reflect the groups being differentially sensitive to the distances between stimuli. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improvement significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items – accounting for their extended generalization gradients, but also for their poor performance in a recognition condition. Additional model comparisons suggested that this similarity rescaling applied across the entire stimulus space, rather than to the high variability category in particular.\nVariability effects have also been examined in the higher-level domain of how learners acquire novel concepts, and then instantiate (rather than merely recognize) that concept in untrained contexts (Braithwaite & Goldstone, 2015). This study trained participants on problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g. all involving people selecting objects) or in which the surface features were varied between examples (e.g. people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants – such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples.\nVariability has also been examined within the realm of language learning. A particularly impressive study is that of Perry et al. (2010). In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the tight condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. The varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category, a pattern consistent with much of the existing literature. More interesting was the superior performance of the varied group on a higher order generalization task – such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#some-complications-to-the-influence-of-variability",
    "href": "Introduction.html#some-complications-to-the-influence-of-variability",
    "title": "Introduction",
    "section": "",
    "text": "A necessary consequence of varied training is that participants will have the experience of switching from one task condition to another. The number of switches can vary greatly, with the two extremes being varied participants completing all of their training trials in one before switching to the next condition (blocked sequencing), or if alternate between conditions on a trial by trial basis (random/intermixed/interleaved sequencing). Not long after the initial influx of schema-theory inspired studies testing the benefits of variability hypothesis it was shown that the influence of varied training might interact with the type of training sequence chosen by the experimenter (J. B. Shea & Morgan, 1979). In this seminal study, both groups of training subjects trained with the same number of trials of three separate movement patterns. A blocked group that completed all of their trials with one sequence before beginning the next sequence, and a random group that trained with all three movement patterns interspersed throughout the course of training. Participants were also randomly assigned to retention testing under either blocked or random sequence conditions, thus resulting in all four training-testing combinations of blocked-blocked; blocked-random; random-blocked; random-random. There was some effect of sequence context, such that both groups performed better when the testing sequence matched their training sequence. However, the main finding of interest was the advantage random-training, which resulted in superior testing performance than blocked training regardless of whether the testing stage had a blocked or random sequence, an effect observed both immediately after training, and in a follow up test ten days after the end of training.\nPrior to the influential Shea and Morgan (1979) study, studies investigating the benefits of variability hypothesis had utilized both blocked and random training schedules, often without comment or justification. It was later observed (Lee et al., 1985) that positive evidence for benefits of varied training seemed more likely to occur for studies that utilized random schedules. The theoretical basis of such studies was invariably an appeal to Schmidt’s schema theory; however schema theory made no clear predictions of an effect of study sequence on retention or generalization, thus prompting the need for alternate accounts. One such account, the elaborative processing account (J. B. Shea & Zimny, 1983), draws on the earlier work of Battig (Battig, 1966) and argues that randomly sequencing conditions during training promotes comparison and contrastive processes between those conditions, which result in a deeper understanding of the training task than could arise via blocked sequencing. Supporting evidence for elaborative processing comes in the form of random-sequence trained subjects self-reporting more nuanced mental representations of movement patterns following training (J. B. Shea & Zimny, 1983), and by manipulating whether subjects are able to perform comparisons during training (Wright et al., 1992). An alternative, though not incompatible account suggests that the benefits of random-sequencing are a result of such sequences forcing the learner to continually reconstruct the relevant motor task in working memory (Lee & Magill, 1985). Blocked training, on the other hand, allows the learner to maintain the same motor task in short term memory without decay for much of the training which facilitates training performance, but hinders ability to retrieve the appropriate motor memory in a later testing context. A much more recent study (Chua et al., 2019), replicates the standard findings of an advantage of varied training over constant training (expt 1, bean-bag throwing task), and of random training over blocked training (expt 2 & 3, bean-bag throwing & golf putting). The novelty of this study is that the experimenters queried subjects about their attentional focus throughout the training stage. In all three experiments varied or random trained-subjects reported significantly greater external attention (e.g. attending to the target distance), and constant or blocked subjects reported more internal attention (e.g. posture or hand position). The authors argue that the benefits of varied/random training may be mediated by changes in attentional focus, however the claims made in the paper seem to go far beyond what can be justified by the analyses reported – e.g. the increased external focus could be a simple byproduct of varied training. A stronger form of evidence that was not provided may have been to use multiple regression analyses to show that the testing advantage of the varied/random groups over the constant/blocked groups could be accounted for by the differences in self-reported attentional focus.\nOther task and participant effects\nOf course, the effects of varied training, and different training sequences, are likely to be far more complex than simply more varied training being better than less, or random training being better than blocked. Null effects of both manipulations have been reported (Magill & Hall, 1990; Van Rossum, 1990 for reviews), and a variety of moderators have emerged. In one of the earlier examples of the complex relationship between study sequence and learning (Del Rey et al., 1982), experimenters recruited participants who self-reported either large amounts, or very little experience with athletic actives, and then trained participants on a coincident timing task under with either a single constant training velocity, or with four training velocities with either blocked, or random training sequence conditions - resulting in 6 experimental conditions: (athlete vs. non-athlete) x (constant vs. blocked vs. random training). Athlete participants had superior performance during training, regardless of sequence condition, and training performance was superior for all subjects in the constant group, followed by blocked training in the middle, and then random training resulting in the worst training performance. Of greater interest is the pattern of testing results for novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received random training, followed by blocked, and then constant training. Non-athletes showed an almost opposite pattern, with superior performance for those who had constant training. A similar pattern was later observed in a golf-putting training study, wherein participants who had some prior golf experience benefited most from random-sequenced training, and participants with no golf experience benefited most from blocked training (Guadagnoli et al., 1999). More recently, the same pattern was observed in the concept learning literature (Braithwaite & Goldstone, 2015, expt 1). This study trained participants on a mathematical concept and found that participants who self-reported some prior experience with the concept improved more from pre-test to post-test after training with varied examples, while participants who reported no prior experience showed greater gains following training with highly similar examples.\nIn addition to the influence of prior experiences described above, ample evidence also suggests that numerous aspects of the experiment may also interact with the influence of variation. One important study examined the impact of the amount of training completed in a force production task (C. H. Shea et al., 1990). This study employed a typical blocked vs. random training procedure, but with the additional manipulation of separate groups receiving 50, 200, or 400 total training trials. For the group that received only 50 training trials retention was best when training had been blocked. However, for the conditions that received 200 or 400 training trials the pattern was reversed, with random training resulting in superior retention than blocked training. These results were taken to suggest that the benefits of randomization may take time to emerge. Another experimental factor shown to interact with training sequence is the complexity of the training task (Albaret & Thon, 1998). In addition to random or blocked training, participants in this study were assigned to train on a drawing task at one of three different levels of complexity (reproducing from memory shapes composed of 2, 3 or 4 components). On a transfer task 48 hours after the completion of training, only participants trained at the lower levels of task complexity (2 or 3 components) showed superior performance to the blocked condition. The authors suggest that the benefits of random sequencing, thought to arise from more elaborate cognitive processing, or the necessity of continually recalling task information from long term into short term memory, are more likely to be obscured as the complexity of the task forces the blocked participants to also engage in such processes.\nA final important influence of particular relevance to the practice-sequence literature concerns the exact structure of “random” sequencing. Although the term random is commonly used for convenience, experimenters do not typically leave the order of training entirely up to chance. Rather, the training sequence is often constrained such that each condition must occur a minimum number of times in each quartile of the training phase, thus resulting in an even distribution the conditions throughout training. While the assurance of the conditions being evenly spread throughout training is consistent across studies, other aspects of the sequence structure are a bit more idiosyncratic. Some researchers report setting a maximum number of consecutive repetitions, e.g. no more than 2 consecutive trials of the same condition (Del Rey et al., 1982; J. B. Shea & Morgan, 1979), or structure the random trials such that the same condition never occurs consecutively (Wulf, 1991). Also common is to structure experiments such that random condition really consists of many small blocks, where participants do a few trials of one condition consecutively and then switch to another condition (Chua et al., 2019; Willey & Liu, 2018; Wrisberg et al., 1987), resulting in much more switches than would arise if training was perfectly blocked. The question of whether such differences in the structure of random sequencing are consequential has been addressed experimentally a few times, in all cases consisting of a 1) a no-repeat random condition; 2) a blocked random condition (typically 3 or 4 repeats before a switch); and 3) a standard fully-blocked condition. Blocked-random training resulted in better performance than either repeat-random, or fully - blocked training in both a bean-bag throwing (Pigott & Shapiro, 1984), and basketball shot training study (Landin & Hebert, 1997), and in a replication plus extension of the seminal Shea and Morgan (1979) study, blocked-random training was equally effective as no-repeat random training, with both random structures leading to better performance than the fully-blocked training condition. Consequences on different study schedules have also been repeatedly observed in the category learning literature (Carvalho & Goldstone, 2014, 2017). This line of research has revealed that the effects of blocking vs. interleaving can depend on the structure of the category being learned, and also that the different schedules can result in the participants requiring different representations. A fruitful line of inquiry in the motor skill learning literature may be to attempt to identify whether structural aspects of the motor task interact with different training sequences in a reliable manner.\nNumerous researchers have attempted to provide coherent frameworks to account for the full range of influences of training variation and sequencing described above (along with many other effects not discussed). Such accounts are generally quite similar, invoking ideas of desirable levels of difficulty(Bjork & Bjork, 2011; R.A. Schmidt & Bjork, 1992), or optimal challenge points (Guadagnoli & Lee, 2004). They tend to start by describing the dissociation between acquisition performance (performance during training) and testing performance (delayed retention and/or transfer), most strikingly observed as varied/random training participants performing worse than their constant/blocked counterparts during the training stage of the study, but then outperforming the constant/blocked comparisons at a later retention or transfer stage. This observation is then used to justify the idea that the most enduring and generalizable learning occurs by training at an optimal level of training difficulty, with difficulty being some function of the experience of the learner, and the cognitive or visuomotor processing demands of the task. It then follows that the factors that tend to make training more difficult (i.e. increased variability or randomization), are more likely to be beneficial when the learner has some experience, or when the processing demands of the task are not too extreme (which may only occur after some experience with the task). Such frameworks may be helpful heuristics in some cases, but they also seem to be overly flexible such that any null result of some intervention might be accounted for by a suboptimal amount of training trials, or by suggesting the training task was too difficult. The development of computational models that can account for how changes in the parameters of the motor-skill task scale with difficulty, would be a great step forward.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "HTW.html",
    "href": "HTW.html",
    "title": "HTW Project",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse,knitr,kableExtra)",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#participants",
    "href": "HTW.html#participants",
    "title": "HTW Project",
    "section": "Participants",
    "text": "Participants\nData was collected from 647 participants (after exclusions). The results shown below consider data from subjects in our initial experiment, which consisted of 196 participants (106 constant, 90 varied). The follow-up experiments entailed minor manipulations: 1) reversing the velocity bands that were trained on vs. novel during testing; 2) providing ordinal rather than numerical feedback during training (e.g. correct, too low, too high). The data from these subsequent experiments are largely consistently with our initial results shown below.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#task",
    "href": "HTW.html#task",
    "title": "HTW Project",
    "section": "Task",
    "text": "Task\nWe developed a novel visuomotor extrapolation task, termed the “Hit The Wall” (HTW) task, wherein participants learned to launch a projectile such that it hit a rectangle at the far end of the screen with an appropriate amount of force. Although the projectile had both x and y velocity components, only the x-dimension was relevant for the task.  Link to task demo",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#design",
    "href": "HTW.html#design",
    "title": "HTW Project",
    "section": "Design",
    "text": "Design\n\n90 training trials split evenly divided between velocity bands. Varied training with 3 velocity bands and Constant training with 1 band.\nNo-feedback testing from 3 novel extrapolation bands. 15 trials each.  \nNo-feedbacd testing from the 3 bands used during the training phase (2 of which were novel for the constant group). 9 trials each.\nFeedback testing for each of the 3 extrapolation bands. 10 trials each.\n\n\n\n\n\nExperiment Procedure",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#training",
    "href": "HTW.html#training",
    "title": "HTW Project",
    "section": "Training",
    "text": "Training\nTraining performance is shown in Results Figure 2A. All groups show improvement from each of their training velocity-bands (i.e. decreasing average distance from target). In the velocity band trained at by both groups (800-1000), the constant group maintains a superior level of performance from the early through the final stages of training. This difference is unsurprising given that the constant group had 3x more practice trials from that band.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#testing",
    "href": "HTW.html#testing",
    "title": "HTW Project",
    "section": "Testing",
    "text": "Testing\nFor evaluating testing performance, we consider 3 separate metrics. 1) The average absolute deviation from the correct velocity, 2) The % of throws in which the wall was hit with the correct velocity and 3) The average x velocity produced.\nResults Figure 2B shows the average velocity produced for all 6 bands that were tested. At least at the aggregate level, both conditions were able to differentiate all 6 bands in the correct order, despite only having received training feedback for 1/6 (constant) or 3/6 (varied) bands during training. Participants in both groups also had a bias towards greatly overestimating the correct velocity for band 100-300, for which both groups had an average of greater than 500.\n\n\nCode\nsumStats = dtest %&gt;% group_by(sbjCode,vbLabel,condit,throwCategory) %&gt;%\n  summarise(vxMean=mean(vxCapped),vxMedian=median(vxCapped),vxSd=sd(vxCapped),.groups = 'keep') %&gt;%group_by(vbLabel,condit,throwCategory) %&gt;%\n  summarise(groupMean=round(mean(vxMean),0),groupMedian=round(mean(vxMedian),0),groupSd=round(mean(vxSd,na.rm=TRUE),0),.groups = 'keep') %&gt;%\n  mutate(meanLab=paste0(\"Mean=\",groupMean),medianLab=paste0(\"Median=\",groupMedian),sdLab=paste0(\"Sd=\",groupSd)) %&gt;%\n  mutate(sumStatLab=paste0(meanLab,\"\\n\",medianLab,\"\\n\",sdLab))\n\nfig2aCap=str_wrap(\"Figure 2B: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Top figure displays mean deviation from correct velocity. Bottom figure displays the average % of trials where participants hit the wall with the correct velocity. Error bars indicate standard error of the mean. \" ,width=170)\n\ndtest %&gt;% group_by(sbjCode,vbLabel,condit,throwCategory) %&gt;%\n  summarise(vxMean=mean(vxCapped),lowBound=first(bandInt),highBound=first(highBound),\n            vbLag=first(vbLag),vbLead=first(vbLead),.groups = 'keep') %&gt;%\n  ggplot(aes(x=vbLabel,y=vxMean,fill=throwCategory))+\n  geom_half_violin(color=NA)+ # remove border color\n  geom_half_boxplot(position=position_nudge(x=-0.05),side=\"r\",outlier.shape = NA,center=TRUE,\n                    errorbar.draw = FALSE,width=.25)+\n  geom_half_point(transformation = position_jitter(width = 0.05, height = 0.05),size=.3,aes(color=throwCategory))+\n  facet_wrap(~condit,scale=\"free_x\")+\n  geom_rect(data=vbRect,aes(xmin=vbLag,xmax=vbLead,ymin=lowBound,ymax=highBound,fill=throwCategory),alpha=.3)+\n  geom_text(data=sumStats,aes(y=2090,label = sumStatLab),size=2.5)+\n  bandLines4+\n  #geom_text(data=sumStats,aes(x=throwCategory,y=2100,label = groupMean),size=2, vjust = -0.5)+\n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2))+\n  scale_fill_discrete(name=\"Velocity Band\")+\n  scale_color_discrete(guide=\"none\")+  # remove extra legend\n  theme(legend.position='none',\n        plot.title=element_text(face=\"bold\"),\n        axis.title.x=element_text(face=\"bold\"),\n        axis.title.y=element_text(face=\"bold\"),\n        axis.text.x = element_text(size = 7.5))+\n  ylab(\"Mean X Velocity\")+xlab(\"Target Velocity Band\") +\n   labs(title=\"2B. Testing Performance (no-feedback) - X-Velocity Per Band\",\n        caption=fig2aCap)+\n  theme(plot.caption=element_text(hjust=0,face=\"italic\"))\n\n\nAs is reflected in Results Figure 2C, the constant group performed significantly better than the varied group at the 3 testing bands of greatest interest. Both groups tended to perform worse for testing bands further away from their training conditions. The varied group had a slight advantage for bands 1000-1200 and 1200-1400, which were repeats from training for the varied participants, but novel for the constant participants.\n\n\nCode\ngbDev&lt;-dtest %&gt;% group_by(sbjCode,vbLabel,condit,throwCategory) %&gt;% \n  summarise(distMean=mean(distCapped),.groups = 'keep') %&gt;% \n  mutate(meanDevCapped=ifelse(distMean&gt;900,900,distMean)) %&gt;%\n  ggplot(aes(x=vbLabel,y=meanDevCapped,fill=condit))+\n  stat_summary(geom=\"bar\",fun=mean,position=dodge,alpha=.7)+\n   stat_summary(geom=\"errorbar\",fun.data=mean_se,alpha=.8,width=.5,position=dodge)+\n #ggbeeswarm::geom_quasirandom(aes(),dodge.width=.9,alpha=.15,size=.3)+\n  scale_y_continuous(breaks=round(seq(0,1000,by=200),2))+\n  ylab(\"Mean Absoulte Distance From Boundary\")+xlab(\"Target Velocity Band\") +\nscale_fill_discrete(name=\"Training Condition\",labels=c(\"Constant\",\"Varied\"))+scale_color_discrete(guide=\"none\")+\n  theme( plot.title=element_text(size=9),\n        axis.title.x=element_text(face=\"bold\",size=11),\n        axis.title.y=element_text(face=\"bold\",size=11),\n        axis.text.x = element_text(size = 7.5),\n        legend.position=\"top\")+\n  labs(title=\"\")#Testing - Mean Absolute Distance From Boundary\nleg=ggpubr::get_legend(gbDev)\ngbDev&lt;- gbDev+theme(legend.position='none')\n  \ngbHit&lt;-dtest %&gt;% group_by(sbjCode,condit,vbLabel,expMode,testMode) %&gt;% \n  summarise(nHitsTest=sum(dist==0),n=n(),Percent_Hit=nHitsTest/n,.groups = 'keep') %&gt;% \n  ggplot(aes(x=vbLabel,y=Percent_Hit,fill=condit))+\n  stat_summary(geom=\"bar\",fun=mean,position=dodge,alpha=.7)+\n   stat_summary(geom=\"errorbar\",fun.data=mean_se,alpha=.8,width=.5,position=dodge)+\n# ggbeeswarm::geom_quasirandom(aes(),dodge.width=.9,alpha=.15,size=.3)+\n  ylab(\"% of throws with correct velocity\")+xlab(\"Target Velocity Band\") + \n  scale_fill_discrete(guide='none')+scale_color_discrete(guide=\"none\")+\n  theme( plot.title=element_text(size=9),\n        axis.title.x=element_text(face=\"bold\",size=11),\n        axis.title.y=element_text(face=\"bold\",size=11),\n        axis.text.x = element_text(size = 7.5),\n        legend.position=\"top\")+\n  labs(title=\"\")#Testing -% of hits\n\n\n\ngtitle=\"2C. Testing Performance\"\ntitle = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0)+theme(plot.margin = margin(0, 0, 0, 7))\ncaptionText=str_wrap(\"Figure 2C: Bands 100-300, 350-550 and 600-800 are novel extrapolations for both groups. Band 800-1000 was a training band for both groups. Bands 1000-1200, and 1200-1400 were trained for the varied group, and novel for the constant group.  Right side figure displays mean deviation from correct velocity band (lower values correspond to better performance). Bottom Left displays the average % of trials where participants hit the wall with the correct velocity (higher values correspond got better performance). Error bars indicate standard error of the mean. \",150)\ncapt=ggdraw()+draw_label(captionText,fontface = 'italic',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))\n\nplot_grid(title,NULL,leg,NULL,gbDev,gbHit,capt,NULL,ncol=2,rel_heights=c(.1,.1,1,.1),rel_widths=c(1,1))",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#alm-exam-description",
    "href": "HTW.html#alm-exam-description",
    "title": "HTW Project",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nDelosh et al. (1997) introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke’s influential ALCOVE model of categorization (Kruscke 1992).\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nSee Table 2A for a full specification of the equations that define ALM and EXAM.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#model-equations",
    "href": "HTW.html#model-equations",
    "title": "HTW Project",
    "section": "Model Equations",
    "text": "Model Equations\n\nCode\ntext_tbl &lt;- data.frame(\n    'Step'=c(\"Input Activation\",\"Output Activation\",\"Output Probability\",\"Mean Output\",\"Feedback Activation\",\"Update Weights\",\"Extrapolation\",\"\"),\n    'Equation' = c(\"$a_i$(X) = $\\\\frac{e^{-c \\\\cdot (X-X_i)^2}}{ \\\\sum_{k=1}^Me^{-c \\\\cdot (X-X_i)^2}}$\", \n                   '$O_j$(X) = $\\\\sum_{k=1}^Mw_{ji} \\\\cdot a_i(X)$',\n                   '$P[Y_j | X] = \\\\frac{O_i(X)}{\\\\sum_{k=1}^Mo_k(X)}$',\n                   \"$m(x) = \\\\sum_{j=1}^LY_j \\\\cdot \\\\bigg[\\\\frac{O_j(X)}{\\\\sum_{k=1}^Lo_k(X)}\\\\bigg]$\",\n                   \"$f_j(Z)=e^{-c\\\\cdot(Z-Y_j)^2}$\",\n                   \"$w_{ji}(t+1)=w_{ji}(t)+\\\\alpha \\\\cdot {f_i(Z(t))-O_j(X(t))} \\\\cdot a_i(X(t))$\",\n                   \"$P[X_i|X] = \\\\frac{a_i(X)}{\\\\sum_{k=1}^Ma_k(X)}$\",\n                   \"$E[Y|X_i]=m(X_i) + \\\\bigg[\\\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\\\bigg] \\\\cdot[X-X_i]$\"),\n    \n    'Description'= c(\n            \"Activation of each input node, $X_i$, is a function of the Gaussian similarity between the node value and stimulus X. \",\n            \"Activation of each Output unit $O_j$ is the weighted sum of the input activations and association weights\",\n            \"Each output node has associated response, $Y_j$. The probability of response $Y_j$ is determined by the ratio of output activations\",\n            \"The response to stimulus x is the weighted average of the response probabilities\",\n            \"After responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response  \",\n            \"Delta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\",\n            \"Novel test stimulus X activates input nodes associated with trained stimuli\",\n            \"Slope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)\")\n)\ntext_tbl$Step=cell_spec(text_tbl$Step,font_size=12)\ntext_tbl$Equation=cell_spec(text_tbl$Equation,font_size=18)\nalmTable=kable(text_tbl, 'html', \n  booktabs=T, escape = F, align='l',\n  caption = '&lt;span style = \"color:black;\"&gt;&lt;center&gt;&lt;strong&gt;Table 1: ALM & EXAM Equations&lt;/strong&gt;&lt;/center&gt;&lt;/span&gt;',\n  col.names=c(\"\",\"Equation\",\"Description\")) %&gt;%\n  kable_styling(position=\"left\",bootstrap_options = c(\"hover\")) %&gt;%\n  column_spec(1, bold = F,border_right=T) %&gt;%\n  column_spec(2, width = '10cm')%&gt;%\n  column_spec(3, width = '15cm') %&gt;%\n  pack_rows(\"ALM Activation & Response\",1,4,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"ALM Learning\",5,6,bold=FALSE,italic=TRUE) %&gt;%\n  pack_rows(\"EXAM\",7,8,bold=FALSE,italic=TRUE)\n  #save_kable(file=\"almTable.html\",self_contained=T)\ncat(almTable)\n\n\n\n\n\nTable 1: ALM & EXAM Equations\n\n\n\n\n\n\n\n\nEquation\n\n\nDescription\n\n\n\n\n\n\nALM Activation & Response\n\n\n\n\nInput Activation\n\n\n\\(a_i\\)(X) = \\(\\frac{e^{-c \\cdot (X-X_i)^2}}{ \\sum_{k=1}^Me^{-c \\cdot (X-X_i)^2}}\\)\n\n\nActivation of each input node, \\(X_i\\), is a function of the Gaussian similarity between the node value and stimulus X.\n\n\n\n\nOutput Activation\n\n\n\\(O_j\\)(X) = \\(\\sum_{k=1}^Mw_{ji} \\cdot a_i(X)\\)\n\n\nActivation of each Output unit \\(O_j\\) is the weighted sum of the input activations and association weights\n\n\n\n\nOutput Probability\n\n\n\\(P[Y_j | X] = \\frac{O_i(X)}{\\sum_{k=1}^Mo_k(X)}\\)\n\n\nEach output node has associated response, \\(Y_j\\). The probability of response \\(Y_j\\) is determined by the ratio of output activations\n\n\n\n\nMean Output\n\n\n\\(m(x) = \\sum_{j=1}^LY_j \\cdot \\bigg[\\frac{O_j(X)}{\\sum_{k=1}^Lo_k(X)}\\bigg]\\)\n\n\nThe response to stimulus x is the weighted average of the response probabilities\n\n\n\n\nALM Learning\n\n\n\n\nFeedback Activation\n\n\n\\(f_j(Z)=e^{-c\\cdot(Z-Y_j)^2}\\)\n\n\nAfter responding, feedback signal Z is presented, activating each output node via the Gaussian similarity to the ideal response\n\n\n\n\nUpdate Weights\n\n\n\\(w_{ji}(t+1)=w_{ji}(t)+\\alpha \\cdot {f_i(Z(t))-O_j(X(t))} \\cdot a_i(X(t))\\)\n\n\nDelta rule to update weights. Magnitude of weight changes controlled by learning rate parameter alpha.\n\n\n\n\nEXAM\n\n\n\n\nExtrapolation\n\n\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^Ma_k(X)}\\)\n\n\nNovel test stimulus X activates input nodes associated with trained stimuli\n\n\n\n\n\n\n\n\\(E[Y|X_i]=m(X_i) + \\bigg[\\frac{m(X_{i+1})-m(X_{i-1})}{X_{i+1} - X_{i-1}} \\bigg] \\cdot[X-X_i]\\)\n\n\nSlope value computed from nearest training instances and then added to the response associated with the nearest training instance,m(x)",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "HTW.html#model-fitting-and-comparison",
    "href": "HTW.html#model-fitting-and-comparison",
    "title": "HTW Project",
    "section": "Model Fitting and Comparison",
    "text": "Model Fitting and Comparison\nFollowing the procedure used by McDaniel & Busemeyer (2009), we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models will be fit directly to the trial by trial data of each individual participants, both by minimizing the root-mean squared deviation (RMSE), and by maximizing log likelihood. Because ALM has been shown to do poorly at accounting for human patterns extrapolation (DeLosh et al., 1997), we will also fit the extended EXAM version of the model, which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing. (albaretDifferentialEffectsTask1998?)",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Discussion.html",
    "href": "Discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "library(xaringanthemer) # Create a xaringan style in a temporary file xaringan_themer_css &lt;- tempfile(“xaringan-themer-”, fileext = “.css”)\nstyle_xaringan( text_color = “#4a4a4a”, header_color = “#9f2042”, background_color = “#fcfcfc”, inverse_background_color = “#222222”, link_color = “#9f2042”, # code_highlight_color = rgba(255,255,0,0.5), inverse_text_color= “#4a4a4a”, inverse_header_color= “#222222”, inverse_link_color= “#9f2042”, title_slide_background_color= “#fcfcfc”, title_slide_text_color= “#9f2042”, header_background_color= “#222222”, header_background_text_color= “#222222”, outfile = “Style/xaringan_themer_css.css” # omit in your slides to write the # styles to xaringan-themer.css )\ne2&lt;- readRDS(‘data/igas_e2_cleanedData-final.rds’)%&gt;% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity) solSpace &lt;- e2 %&gt;% filter(trialType==11) #solSpace %&gt;% ggplot(aes(x=X_Velocity,y=Y_Velocity)) + geom_point(aes(colour=ThrowPosition),alpha=0.58) + ggtitle(““)\nsolSpace\\(Result = ifelse(solSpace\\)ThrowPosition==400,“400”,solSpace\\(ThrowPosition) solSpace\\)Result = ifelse(solSpace\\(ThrowPosition==500,\"500\",solSpace\\)Result) solSpace\\(Result= ifelse(solSpace\\)ThrowPosition==625,“625”,solSpace\\(Result) solSpace\\)Result = ifelse(solSpace\\(ThrowPosition==675,\"675\",solSpace\\)Result) solSpace\\(Result = ifelse(solSpace\\)ThrowPosition==800,“800”,solSpace\\(Result) solSpace\\)Result = ifelse(solSpace\\(ThrowPosition==900,\"900\",solSpace\\)Result) theme_set(theme_classic())",
    "crumbs": [
      "Discussion"
    ]
  },
  {
    "objectID": "IGAS.html",
    "href": "IGAS.html",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "",
    "text": "Pdf of the journal article\nLink to online version of journal article\nCode\npacman::p_load(tidyr,papaja, knitr, tinytex, RColorBrewer, kableExtra, cowplot, patchwork)\nsource('Functions/IGAS_ProcessFunctions.R')\n\ntheme_set(theme_classic())\n# load the processed data from experiment 1 and 2\ne1 &lt;- readRDS(\"data/igas_e1_cleanedData-final.rds\")%&gt;% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity,stageInt=as.numeric(as.character(experimentStage)))\ne2&lt;- readRDS('data/igas_e2_cleanedData-final.rds')%&gt;% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity)\n# load subject similarity data - computed with the IGAS model in 'IGAS-SimModel.R'\ne2_sim &lt;- readRDS('data/IGAS_Similarity-Performance.rds')\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\ndefaultContrasts = options()$contrasts\ntheme_set(theme_classic())\n\ndodge &lt;- position_dodge(width = 0.9)\ne2GrpPos &lt;- c(\"400\",\"500\",\"625\",\"675\",\"800\",\"900\")\ne2Grp &lt;- paste(\"Constant\",\"Constant\", \"Constant\",\"Constant\",\"Constant\",\"Constant\", \"Varied\")\ne2Labels &lt;- paste(c(\"400\\n Constant\",\"500\\n Constant\",\"625\\n Constant\",\"675\\n Constant\",\n                   \"800\\n Constant\",\"900\\n Constant\",\"500-800\\n Varied\"),sep=\"\")\n\ne1Pos &lt;- c(\"610\",\"760\",\"835\",\"910\")\ne1Var &lt;- paste(\"Varied Train Position\",\"Constant Train Position\", \"Novel Position\", \"Varied Training Position\")\ne1Labels&lt;- paste(c(\"610\\n Varied Trained\",\"760\\n Constant Trained\",\"835\\n Novel Location\",\"910\\n Varied Trained\"),sep=\"\")",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#similarity-and-instance-based-approaches-to-transfer-of-learning",
    "href": "IGAS.html#similarity-and-instance-based-approaches-to-transfer-of-learning",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Similarity and instance-based approaches to transfer of learning",
    "text": "Similarity and instance-based approaches to transfer of learning\nNotions of similarity have long played a central role in many prominent models of generalization of learning, as well as in the longstanding theoretical issue of whether learners abstract an aggregate, summary representation, or if they simply store individual instances. Early models of learning often assumed that discrete experiences with some task or category were not stored individually in memory, but instead promoted the formation of a summary representation, often referred to as a prototype or schema, and that exposure to novel examples would then prompt the retrieval of whichever preexisting prototype was most similar (Posner & Keele, 1968). Prototype models were later challenged by the success of instance-based or exemplar models – which were shown to provide an account of generalization as good or better than prototype models, with the advantage of not assuming the explicit construction of an internal prototype (Estes, 1994; Hintzman, 1984; Medin & Schaffer, 1978; Nosofsky, 1986 ). Instance-based models assume that learners encode each experience with a task as a separate instance/exemplar/trace, and that each encoded trace is in turn compared against novel stimuli. As the number of stored instances increases, so does the likelihood that some previously stored instance will be retrieved to aid in the performance of a novel task. Stored instances are retrieved in the context of novel stimuli or tasks if they are sufficiently similar, thus suggesting that the process of computing similarity is of central importance to generalization.\nSimilarity, defined in this literature as a function of psychological distance between instances or categories, has provided a successful account of generalization across numerous tasks and domains. In an influential study demonstrating an ordinal similarity effect, experimenters employed a numerosity judgment task in which participants quickly report the number of dots flashed on a screen. Performance (in terms of response times to new patterns) on novel dot configurations varied as an inverse function of their similarity to previously trained dot configurations Palmeri (1997). That is, performance was better on novel configurations moderately similar to trained configurations than to configurations with low-similarity, and also better on low-similarity configurations than to even less similar, unrelated configurations. Instance-based approaches have had some success accounting for performance in certain sub-domains of motor learning (Cohen & Rosenbaum, 2004; Crump & Logan, 2010, 2010; Meigh et al., 2018; Poldrack et al., 1999; Wifall et al., 2017) trained participants to type words on an unfamiliar keyboard, while constraining the letters composing the training words to a pre-specified letter set. Following training, typing speed was tested on previously experienced words composed of previously experienced letters; novel words composed of letters from the trained letter set; and novel words composed of letters from an untrained letter set. Consistent with an instance-based account, transfer performance was graded such that participants were fastest at typing the words they had previously trained on, followed by novel words composed of letters they had trained on, and slowest performance for new words composed of untrained letters.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#the-effect-of-training-variability-on-transfer",
    "href": "IGAS.html#the-effect-of-training-variability-on-transfer",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "The effect of training variability on transfer",
    "text": "The effect of training variability on transfer\nWhile similarity-based models account for transfer by the degree of similarity between previous and new experiences, a largely separate body of research has focused on improving transfer by manipulating characteristics of the initial training stage. Such characteristics have included training difficulty, spacing, temporal order, feedback schedules, and the primary focus of the current work – variability of training examples.\nResearch on the effects of varied training typically compares participants trained under constant, or minimal variability conditions to those trained from a variety of examples or conditions (Czyż, 2021; Soderstrom & Bjork, 2015). Varied training has been shown to influence learning in myriad domains including categorization of simple stimuli (Hahn et al., 2005; Maddox & Filoteo, 2011; Posner & Keele, 1968), complex categorization (Nosofsky et al., 2018), language learning (Jones & Brandt, 2020; Perry et al., 2010; Twomey et al., 2018; Wonnacott et al., 2012), anagram completion (Goode et al., 2008), trajectory extrapolation (Fulvio et al., 2014), task switching (Sabah et al., 2019), associative learning (Lee et al., 2019), visual search (George & Egner, 2021; Gonzalez & Madhavan, 2011; Kelley & Yantis, 2009), voice identity learning (Lavan et al., 2019), simple motor learning (Braun et al., 2009; Kerr & Booth, 1978; Roller et al., 2001; Willey & Liu, 2018a), sports training North et al. (2019), and training on a complex video game (Seow et al., 2019).\nTraining variation has received a particularly large amount of attention within the domain of visuomotor skill learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. According to Schema Theory, learners possess general motor programs for classes of movements (e.g. throwing a ball with an underhand movement), as well as schema rules that determine how a motor program is parameterized or scaled for a particular movement. Schema theory predicts that varied training results in the formation of a more general schema-rule, which can allow for transfer to novel movements within a given movement class. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices from multiple levels of a task-relevant dimension that remains invariant for the constant group. For example, investigators might train two groups of participants to throw a projectile at a target, with a constant group that throws from a single location, and a varied group that throws from multiple locations. Both groups are then tested from novel locations. Empirically observed benefits of the varied-trained group are then attributed to the variation they received during training, a finding observed in numerous studies (Catalano & Kleiner, 1984; Chua et al., 2019; Goodwin et al., 1998; Kerr & Booth, 1978; Wulf, 1991), and the benefits of this variation are typically thought to be mediated by the development of a more general schema for the throwing motion.\nOf course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation (DeLosh et al., 1997; Sinkeviciute et al., 2019; Wrisberg et al., 1987), and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials, or the type of transfer being measured (Berniker et al., 2014; Braithwaite & Goldstone, 2015; Hahn et al., 2005; Lavan et al., 2019; North et al., 2019; Sadakata & McQueen, 2014; Zaman et al., 2021).",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#issues-with-previous-research",
    "href": "IGAS.html#issues-with-previous-research",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Issues with Previous Research",
    "text": "Issues with Previous Research\nAlthough the benefits of training variation in visuomotor skill learning have been observed many times, null findings have also been repeatedly found, leading some researchers to question the veracity of the variability of practice hypothesis (Newell, 2003; Van Rossum, 1990). Critics have also pointed out that investigations of the effects of training variability, of the sort described above, often fail to control for the effect of similarity between training and testing conditions. For training tasks in which participants have numerous degrees of freedom (e.g. projectile throwing tasks where participants control the x and y velocity of the projectile), varied groups are likely to experience a wider range of the task space over the course of their training (e.g. more unique combinations of x and y velocities). Experimenters may attempt to account for this possibility by ensuring that the training location(s) of the varied and constant groups are an equal distance away from the eventual transfer locations, such that their training throws are, on average, equally similar to throws that would lead to good performance at the transfer locations. However, even this level of experimental control may still be insufficient to rule out the effect of similarity on transfer. Given that psychological similarity is typically best described as either a Gaussian or exponentially decaying function of psychological distance (Ennis et al., 1988; Ghahramani et al., 1996; Logan, 1988; Nosofsky, 1992; Shepard, 1987; Thoroughman & Taylor, 2005 ), it is plausible that a subset of the most similar training instances could have a disproportionate impact on generalization to transfer conditions, even if the average distance between training and transfer conditions is identical between groups. Figure 1 demonstrates the consequences of a generalization gradient that drops off as a Gaussian function of distance from training, as compared to a linear drop-off.\n\n\nCode\np=2\nc&lt;- .0002\nsimdat &lt;- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %&gt;%\n                     mutate(plotjitter=ifelse(condit==\"varied\",0,7),\n                            linScale=ifelse(condit==\"varied\",980,1000),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genLinear=1000-abs(x-train.position)+plotjitter) %&gt;% \n  #group_by(condit) %&gt;% mutate(scaleLinear=(genLinear-min(genLinear))/(max(genLinear)-min(genLinear))) \n  group_by(x,condit) %&gt;%\n  reframe(genGauss=mean(genGauss),genLinear=mean(genLinear)/linScale,.groups = 'keep')\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs &lt;- list(geom_line(alpha=.7,size=.4),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=447,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=450,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=659,y=1.05,label=\"Constant\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=657,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=847,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=850,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1),\n                        legend.position=\"none\"))\n\nip1 &lt;- simdat  %&gt;% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"\")\nip2 &lt;- simdat %&gt;%  ggplot(aes(x,y=genLinear,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")\n\nplot_grid(ip1,ip2,ncol=2,rel_heights=c(1))\n\n\n\n\n\n\n\n\nFigure 1: Left panel- Generalization predicted from a simple model that assumes a linear generalization function. A varied group (red vertical lines indicate the 2 training locations) trained from positions 400 and 800, and a constant group (blue vertical line), trained from position 600. Right panel- if a Gaussian generalization function is assumed, then varied training (400, 800) is predicted to result in better generalization to positions close to 400 and 800 than does constant training at 600. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\n\n\n\n\n\nIn addition to largely overlooking the potential for non-linear generalization to confound interpretations of training manipulations, the visuomotor skill learning literature also rarely considers alternatives to schema representations (Chamberlin & Magill, 1992b). Although schema-theory remains influential within certain literatures, instance or exemplar-based models have accounted for human behavior across myriad domains (Jamieson et al., 2022; Logan, 2002). As mentioned above, instance based accounts have been shown to perform well on a variety of different tasks with motoric components (Crump & Logan, 2010; Gandolfo et al., 1996; Meigh et al., 2018; Rosenbaum et al., 1995; van Dam & Ernst, 2015). However, such accounts have received little attention within the subdomain of visuomotor skill learning focused on the benefits of varied training.\nThe present work examines whether the commonly observed benefits of varied training can be accounted for by between-group differences in similarity between training and testing throws. We first attempt to replicate previous work finding an advantage of varied training over constant training in a projectile launching task. We then examine the extent to which this advantage can be explained by an instance-based similarity model.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#methods",
    "href": "IGAS.html#methods",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Methods",
    "text": "Methods\n\nSample Size Estimation\nTo obtain an independent estimate of effect size, we identified previous investigations which included between-subjects contrasts of varied and constant conditions following training on an accuracy based projectile launching task (Chua et al., 2019; Goodwin et al., 1998; Kerr & Booth, 1978; Wulf, 1991). We then averaged effects across these studies, yielding a Cohens f =.43. The GPower 3.1 software package (Faul et al., 2009), 2009) was then used to determine that a power of 80% requires a sample size of at least 23 participants per condition. All experiments reported in the present manuscript exceed this minimum number of participants per condition.\n\n\nParticipants\nParticipants were recruited from an undergraduate population that is 63% female and consists almost entirely of individuals aged 18-22 years. A total of 110 Indiana University psychology students participated in Experiment 1. We subsequently excluded 34 participants poor performance at one of the dependent measures of the task (2.5-3 standard deviations worse than the median subject at the task) or for displaying a pattern of responses that was clearly indicative of a lack of engagement with the task (e.g. simply dropping the ball on each trial rather than throwing it at the target), or for reporting that they completed the experiment on a phone or tablet device, despite the instructions not to use one of these devices. A total of 74 participants were retained for the final analyses, 35 in the varied group and 39 in the constant group.\n\n\nTask\nThe experimental task was programmed in JavaScript, using packages from the Phaser physics engine (https://phaser.io) and the jsPsych library (de Leeuw, 2015). The stimuli, presented on a black background, consisted of a circular blue ball – controlled by the participant via the mouse or trackpad cursor; a rectangular green target; a red rectangular barrier located between the ball and the target; and an orange square within which the participant could control the ball before releasing it in a throw towards the target. Because the task was administered online, the absolute distance between stimuli could vary depending on the size of the computer monitor being used, but the relative distance between the stimuli was held constant. Likewise, the distance between the center of the target, and the training and testing locations was scaled such that relative distances were preserved regardless of screen size. For the sake of brevity, subsequent mentions of this relative distance between stimuli, or the position where the ball landed in relation to the center of the target, will be referred to simply as distance. Figure 2 displays the layout of the task, as it would appear to a participant at the start of a trial, with the ball appearing in the center of the orange square. Using a mouse or trackpad, participants click down on the ball to take control of the ball, connecting the movement of the ball to the movement of the cursor. Participants can then “wind up” the ball by dragging it (within the confines of the orange square) and then launch the ball by releasing the cursor. If the ball does not land on the target, participants are presented with feedback in red text at the top right of the screen, on how many units away they were from the center of the target. If the ball was thrown outside of the boundary of the screen participants are given feedback as to how far away from the target center the ball would have been if it had continued its trajectory. If the ball strikes the barrier (from the side or by landing on top), feedback is presented telling participants to avoid hitting the barrier. If participants drag the ball outside of the orange square before releasing it, the trial terminates, and they are reminded to release the ball within the orange square. If the ball lands on the target, feedback is presented in green text, confirming that the target was hit, and presenting additional feedback on how many units away the ball was from the exact center of the target.\nLink to abbrevaited example of task.\n\n\nCode\nmf &lt;- cowplot::ggdraw()+cowplot::draw_image(\"Assets/methodsFig1.png\",hjust=0)+theme(plot.margin = margin(0, 0, 0, 0))\nplot_grid(mf,ncol=1)\n\n\n\n\n\n\n\n\nFigure 2: The stimuli of the task consisted of a blue ball, which the participants would launch at the green target, while avoiding the red barrier. On each trial, the ball would appear in the center of the orange square, with the position of the orange square varying between experimental conditions. Participants were constrained to release the ball within the square",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#results",
    "href": "IGAS.html#results",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Results",
    "text": "Results",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#data-processing-and-statistical-packages",
    "href": "IGAS.html#data-processing-and-statistical-packages",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Data Processing and Statistical Packages",
    "text": "Data Processing and Statistical Packages\nTo prepare the data, we first removed trials that were not easily interpretable as performance indicators in our task. Removed trials included: 1) those in which participants dragged the ball outside of the orange starting box without releasing it, 2) trials in which participants clicked on the ball, and then immediately released it, causing the ball to drop straight down, 3) outlier trials in which the ball was thrown more than 2.5 standard deviations further than the average throw (calculated separately for each throwing position), and 4) trials in which the ball struck the barrier. The primary measure of performance used in all analyses was the absolute distance away from the center of the target. The absolute distance was calculated on every trial, and then averaged within each subject to yield a single performance score, for each position. A consistent pattern across training and testing phases in both experiments was for participants to perform worse from throwing positions further away from the target – a pattern which we refer to as the difficulty of the positions. However, there were no interactions between throwing position and training conditions, allowing us to collapse across positions in cases where contrasts for specific positions were not of interest. All data processing and statistical analyses were performed in R version 4.03 (R Core Team, 2020). ANOVAs for group comparisons were performed using the rstatix package (Kassambara, 2021)****.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#training-phase",
    "href": "IGAS.html#training-phase",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Training Phase",
    "text": "Training Phase\nFigure 3 below shows aggregate training performance binned into three stages representing the beginning, middle, and end of the training phase. Because the two conditions trained from target distances that were not equally difficult, it was not possible to directly compare performance between conditions in the training phase. Our focus for the training data analysis was instead to establish that participants did improve their performance over the course of training, and to examine whether there was any interaction between training stage and condition. Descriptive statistics for the intermittent testing phase are provided in the supplementary materials.\nWe performed an ANOVA comparison with stage as a within-group factor and condition as between-group factor. The analysis revealed a significant effect of training stage F(2,142)=62.4, p&lt;.001, \\(\\eta^{2}_G\\) = .17, such that performance improved over the course of training There was no significant effect of condition F(1,71)=1.42, p=.24, \\(\\eta^{2}_G\\) = .02, and no significant interaction between condition and training stage, F(2,142)=.10, p=.91, \\(\\eta^{2}_G\\) &lt; .01.\n\n\nCode\nexp1TrainPosition &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1TrainPosition3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\n\ne1train2 &lt;- exp1TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",fun=mean,position=dodge)+\n  facet_wrap(~conditType,ncol=2)+\n  stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  xlab(\"Training Location(s)\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)\n\n\n#plot_grid(title,e1train2,capt,ncol=1,rel_heights=c(.18,1,.15))\nplot_grid(e1train2,ncol=1)\n\n\n\n\n\n\n\n\nFigure 3: Training performance for varied and constant participants binned into three stages. Shorter bars indicate better performance (ball landing closer to the center of the target). Error bars indicate standard error of the mean.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#testing-phase",
    "href": "IGAS.html#testing-phase",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Testing Phase",
    "text": "Testing Phase\nIn Experiment 1, a single constant-trained group was compared against a single varied-trained group. At the transfer phase, all participants were tested from 3 positions: 1) the positions(s) from their own training, 2) the training position(s) of the other group, and 3) a position novel to both groups. Overall, group performance was compared with a mixed type III ANOVA, with condition (varied vs. constant) as a between-subject factor and throwing location as a within-subject variable. The effect of throwing position was strong, F(3,213) = 56.12, p&lt;.001, η2G = .23. The effect of training condition was significant F(1,71)=8.19, p&lt;.01, η2G = .07. There was no significant interaction between group and position, F(3,213)=1.81, p=.15, η2G = .01.\n\n\nCode\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n#manuscript plot\ne1test1=exp1.Test %&gt;% ggplot(aes(x=positionX,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",fun=mean,position=dodge)+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)+scale_x_discrete(name=\"Testing Location\",labels=e1Labels)\n\ne1test1\n\n\n\n\n\n\n\n\nFigure 4: Testing performance for each of the 4 testing positions, compared between training conditions. Positions 610 and 910 were trained on by the varied group, and novel for the constant group. Position 760 was trained on by the constant group, and novel for the varied group. Position 835 was novel for both groups. Shorter bars are indicative of better performance (the ball landing closer to the center of the target). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nCode\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n\ntest= exp1.Test %&gt;% dplyr::rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n test=test %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n test = test %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n test=test[,1:3]\n\nkable(test,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;%   \n  kable_classic() #%&gt;% kableExtra::footnote(general=captionText,general_title = \"\")\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n610\n132.48(50.85)\n104.2(38.92)\n\n\n760\n207.26(89.19)\n167.12(72.29)\n\n\n835\n249.13(105.92)\n197.22(109.71)\n\n\n910\n289.36(122.48)\n212.86(113.93)",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#discussion",
    "href": "IGAS.html#discussion",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Discussion",
    "text": "Discussion\nIn Experiment 1, we found that varied training resulted in superior testing performance than constant training, from both a position novel to both groups, and from the position at which the constant group was trained, which was novel to the varied condition. The superiority of varied training over constant training even at the constant training position is of particular note, given that testing at this position should have been highly similar for participants in the constant condition. It should also be noted, though, that testing at the constant trained position is not exactly identical to training from that position, given that the context of testing is different in several ways from that of training, such as the testing trials from the different positions being intermixed, as well as a simple change in context as a function of time. Such contextual differences will be further considered in the General Discussion.\nIn addition to the variation of throwing position during training, the participants in the varied condition of Experiment 1 also received training practice from the closest/easiest position, as well as from the furthest/most difficult position that would later be encountered by all participants during testing. The varied condition also had the potential advantage of interpolating both of the novel positions from which they would later be tested. Experiment 2 thus sought to address these issues by comparing a varied condition to multiple constant conditions.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#methods-1",
    "href": "IGAS.html#methods-1",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Methods",
    "text": "Methods\n\nParticipants\nA total of 306 Indiana University psychology students participated in Experiment 2, which was also conducted online. As was the case in experiment 1, the undergraduate population from which we recruited participants was 63% female and primarily composed of 18–22-year-old individuals. Using the same procedure as experiment 1, we excluded 98 participants for exceptionally poor performance at one of the dependent measures of the task, or for displaying a pattern of responses indicative of a lack of engagement with the task. A total of 208 participants were included in the final analyses with 31 in the varied group and 32, 28, 37, 25, 29, 26 participants in the constant groups training from location 400, 500, 625, 675, 800, and 900, respectively. All participants were compensated with course credit.\n\n\nTask and Procedure\nThe task of Experiment 2 was identical to that of Experiment 1, in all but some minor adjustments to the height of the barrier, and the relative distance between the barrier and the target. Additionally, the intermittent testing trials featured in experiment 1 were not utilized in experiment 2, and all training and testing trials were presented with feedback. An abbreviated demo of the task used for Experiment 2 can be found at (https://pcl.sitehost.iu.edu/tg/demos/igas_expt2_demo.html).\nThe procedure for Experiment 2 was also quite similar to experiment 1. Participants completed 140 training trials, all of which were from the same position for the constant groups and split evenly (70 trials each - randomized) for the varied group. In the testing phase, participants completed 30 trials from each of the six locations that had been used separately across each of the constant groups during training. Each of the constant groups thus experience one trained location and five novel throwing locations in the testing phase, while the varied group experiences 2 previously trained, and 4 novel locations.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#results-1",
    "href": "IGAS.html#results-1",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Results",
    "text": "Results\n\nData Processing and Statistical Packages\nAfter confirming that condition and throwing position did not have any significant interactions, we standardized performance within each position, and then average across position to yield a single performance measure per participant. This standardization did not influence our pattern of results. As in experiment 1, we performed type III ANOVA’s due to our unbalanced design, however the pattern of results presented below is not altered if type 1 or type III tests are used instead. The statistical software for the primary analyses was the same as for experiment 1. Individual learning rates in the testing phase, compared between groups in the supplementary analyses, were fit using the TEfit package in R (Cochrane, 2020).\n\n\nTraining Phase\nThe different training conditions trained from positions that were not equivalently difficult and are thus not easily amenable to comparison. As previously stated, the primary interest of the training data is confirmation that some learning did occur. ?@fig-e2train depicts the training performance of the varied group alongside that of the aggregate of the six constant groups (5a), and each of the 6 separate constant groups (5b). An ANOVA comparison with training stage (beginning, middle, end) as a within-group factor and group (the varied condition vs. the 6 constant conditions collapsed together) as a between-subject factor revealed no significant effect of group on training performance, F(1,206)=.55,p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,412)=77.91, p&lt;.001, \\(\\eta^{2}_G\\) =.05, and no significant interaction between group and training stage, F(2,412)=.489 p=.61, \\(\\eta^{2}_G\\) &lt;.01. We also tested for a difference in training performance between the varied group and the two constant groups that trained matching throwing positions (i.e., the constant groups training from position 500, and position 800). The results of our ANOVA on this limited dataset mirrors that of the full-group analysis, with no significant effect of group F(1,86)=.48, p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,172)=56.29, p&lt;.001, \\(\\eta^{2}_G\\) =.11, and no significant interaction between group and training stage, F(2,172)=.341 p=.71, \\(\\eta^{2}_G\\) &lt;.01.\n\n\nCode\ne2$stage &lt;- factor(e2$stage, levels = c(\"Beginning\", \"Middle\", \"End\",\"Transfer\"),ordered = TRUE)\n\nexp2TrainPosition &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter))%&gt;% as.data.frame()\n\nexp2TrainPosition3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05))%&gt;% as.data.frame()\n\nexp2Train &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter)) %&gt;% as.data.frame()\n\nexp2Train3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\ntransfer &lt;- filter(e2, stage==\"Transfer\") %&gt;% droplevels() %&gt;% select(-trainHalf,-initialVelocityY,ThrowPosition2)%&gt;% ungroup()\ntransfer &lt;- transfer %&gt;% group_by(positionX) %&gt;% mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\ntransfer &lt;- transfer %&gt;% group_by(sbjCode,positionX) %&gt;% mutate(ind=1,testPosIndex=cumsum(ind),posN=max(testPosIndex)) %&gt;%\n  select(-ind) %&gt;% mutate(testHalf = case_when(testPosIndex&lt;15 ~\"1st Half\",testPosIndex&gt;=15 ~\"2nd Half\")) %&gt;% convert_as_factor(testHalf)\n\nvariedTest &lt;- transfer %&gt;% filter(condit==7) %&gt;% mutate(extrapolate=ifelse(positionX==\"900\" | positionX==\"400\",\"extrapolation\",\"interpolation\")) \nconstantTest &lt;- transfer %&gt;% filter(condit!=7) %&gt;% mutate(extrapolate=ifelse(distFromTrain==0,\"interpolation\",\"extrapolation\"))\n\ntransfer &lt;- rbind(variedTest,constantTest)\ntransfer&lt;- transfer %&gt;% mutate(novel=ifelse(distFromTrain3==0,\"trainedLocation\",\"novelLocation\"))%&gt;% convert_as_factor(novel,extrapolate)\n\ntransfer &lt;- transfer %&gt;% relocate(sbjCode,condit2,Group,conditType2,stage,trial,novel,extrapolate,positionX,AbsDistFromCenter,globalAvg,globalSd,scaledDev,distFromTrain3) %&gt;% ungroup()\n\n\n# novelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0) %&gt;% select(-globalAvg,-globalSd,-scaledDev)%&gt;% droplevels() %&gt;% ungroup()\n# novelAll &lt;- novelAll %&gt;% group_by(positionX) %&gt;%\n#  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n#   group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\nnovelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0)\nnovelAllMatched &lt;- novelAll %&gt;% filter(condit!=5,condit!=2)\n\n\nconstantIden &lt;- transfer %&gt;% filter(condit !=7,distFromTrain==0) # only constant groups from their training position\nvariedTest &lt;- transfer %&gt;% filter(condit==7) # only varied testing\nvariedVsIden &lt;- rbind(constantIden,variedTest) # all varied combined with constant identity\n\n\nvariedNovel &lt;- variedTest %&gt;% filter(distFromTrain3 !=0) # removes 500 and 800 from varied\nconstantIden2 &lt;- transfer %&gt;% filter(condit !=7,condit!=5,condit!=2,distFromTrain==0) # only constant groups from training position 400,625,675,900\nvariedVsNovelIden &lt;- rbind(constantIden2,variedNovel) # novel positions for varied, trained for constant\n\nexp2.Test &lt;- transfer %&gt;%group_by(sbjCode,conditType,positionX,ThrowPosition)%&gt;%\n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test2 &lt;- exp2.Test %&gt;% group_by(sbjCode,conditType)%&gt;%\n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test7 &lt;- transfer %&gt;%group_by(Group2,sbjCode,positionX,Group,conditType,ThrowPosition4) %&gt;% \n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\nexp2.Test7.agg &lt;- exp2.Test7  %&gt;%group_by(Group2,sbjCode,Group,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\nexp2.Test7.agg2 &lt;- exp2.Test7  %&gt;%group_by(sbjCode,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\n\n\n\nCode\n### New - 3 stage\ne2train1&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=stage,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=stage,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+facet_wrap(~conditType,ncol=2)+\n  ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Stage\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0.0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"A\")\n\ne2train2&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  facet_wrap(~conditType,ncol=2)+stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Location(s)\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"B\")\n\n#plot_grid(e2train1,e2train2,ncol=1)\n\ne2train1\ne2train2\n\n\n\n\n\n\n\n\nFigure 5: Training performance for the six constant conditions, and the varied condition, binned into three stages. On the left side, the six constant groups are averaged together, as are the two training positions for the varied group. On the right side, the six constant groups are shown separately, with each set of bars representing the beginning, middle, and end of training for a single constant group that trained from the position indicated on the x-axis. Figure 5b also shows training performance separately for both of the throwing locations trained by the varied group. Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Training performance for the six constant conditions, and the varied condition, binned into three stages. On the left side, the six constant groups are averaged together, as are the two training positions for the varied group. On the right side, the six constant groups are shown separately, with each set of bars representing the beginning, middle, and end of training for a single constant group that trained from the position indicated on the x-axis. Figure 5b also shows training performance separately for both of the throwing locations trained by the varied group. Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\nTesting Phase\nIn Experiment 2, a single varied condition (trained from two positions, 500 and 800), was compared against six separate constant groups (trained from a single position, 400, 500, 625, 675, 800 or 900). For the testing phase, all participants were tested from all six positions, four of which were novel for the varied condition, and five of which were novel for each of the constant groups. For a general comparison, we took the absolute deviations for each throwing position and computed standardized scores across all participants, and then averaged across throwing position. The six constant groups were then collapsed together allowing us to make a simple comparison between training conditions (constant vs. varied). A type III between-subjects ANOVA was performed, yielding a significant effect of condition F(1,206)=4.33, p=.039, \\(\\eta^{2}_G\\) =.02. Descriptive statistics for each condition are shown in table 2. In Figure 7 visualizes the consistent advantage of the varied condition over the constant groups across the testing positions. Figure 7 shows performance between the varied condition and the individual constant groups.\n\n\nCode\n# manuscript plot\ne2test1&lt;-exp2.Test %&gt;% ggplot(aes(x=ThrowPosition,y=MeanTargetDeviance,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25)+\n  ggtitle(\"A\")\n\n\ne2test2&lt;-exp2.Test7 %&gt;% \n  ggplot(aes(x=Group,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",position=position_dodge(),fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=position_dodge())+\n  facet_wrap(~ThrowPosition4)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25,\n        axis.text.x = element_text(size = 7,angle=45,hjust=1))+\n  scale_x_discrete(name=\" Training Group\",labels=e2Labels)+ggtitle(\"B\")\n\ne2test1 / e2test2\n\n\n\n\n\n\n\n\nFigure 7: Testing phase performance from each of the six testing positions. The six constant conditions are averaged together into a single constant group, compared against the single varied-trained group.B) Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\n\nCode\ntab2= exp2.Test %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n tab2=tab2 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n tab2 = tab2 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% \n   select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab2=tab2[,1:3]\n\n\nkable(tab2,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 1: Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n100.59(46.3)\n83.92(33.76)\n\n\n500\n152.28(69.82)\n134.38(61.38)\n\n\n625\n211.21(90.95)\n183.51(75.92)\n\n\n675\n233.32(93.35)\n206.32(94.64)\n\n\n800\n283.24(102.85)\n242.65(89.73)\n\n\n900\n343.51(114.33)\n289.62(110.07)\n\n\n\n\n\n\n\n\n\n\nNext, we compared the testing performance of constant and varied groups from only positions that participants had not encountered during training. Constant participants each had 5 novel positions, whereas varied participants tested from 4 novel positions (400,625,675,900). We first standardized performance within in each position, and then averaged across positions. Here again, we found a significant effect of condition (constant vs. varied): F(1,206)=4.30, p=.039, \\(\\eta^{2}_G\\) = .02 .\n\n\nCode\nsum.novelAll &lt;- novelAll %&gt;% group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ntab3=sum.novelAll %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\n tab3=tab3 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \n tab3 = tab3 %&gt;% \n   mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab3=tab3[,1:3]\n\n\n\nkable(tab3,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 2: Testing performance from novel positions. Includes data only from positions that were not encountered during the training stage (e.g. excludes positions 500 and 800 for the varied group, and one of the six locations for each of the constant groups). Table presents Mean absolute deviations from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n98.84(45.31)\n83.92(33.76)\n\n\n500\n152.12(69.94)\nNA\n\n\n625\n212.91(92.76)\n183.51(75.92)\n\n\n675\n232.9(95.53)\n206.32(94.64)\n\n\n800\n285.91(102.81)\nNA\n\n\n900\n346.96(111.35)\n289.62(110.07)\n\n\n\n\n\n\n\n\n\n\nFinally, corresponding to the comparison of position 760 from experiment 1, we compared the test performance of the varied group against the constant group from only the positions that the constant groups trained. Such positions were novel to the varied group (thus this analysis omitted two constant groups that trained from positions 500 or 800 as those positions were not novel to the varied group). Figure 8 displays the particular subset of comparisons utilized for this analysis. Again, we standardized performance within each position before performing the analyses on the aggregated data. In this case, the effect of condition did not reach statistical significance F(1,149)=3.14, p=.079, \\(\\eta^{2}_G\\) = .02. Table 4 provides descriptive statistics.\n\n\nCode\nsum.variedVsNovelIden &lt;- variedVsNovelIden  %&gt;%\n  group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ne2Test2 &lt;- sum.variedVsNovelIden %&gt;% ggplot(aes(x=positionX,y=MeanTargetDev,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)\n\n\ne2Test2\n\n\n\n\n\n\n\n\nFigure 8: A comparison of throwing location that are identical to those trained by the constant participants (e.g. constant participants trained at position 900, tested from position 900), which are also novel to the varied-trained participants (thus excluding positions 500 and 800). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nCode\ntab4=sum.variedVsNovelIden %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\ntab4=tab4 %&gt;% group_by(Condition) %&gt;% \n   mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \ntab4 = tab4 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab4=tab4[,1:3]\n\nkable(tab4,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 3: Testing performance from the locations trained by constant participants and novel to varied participants. Locations 500 and 800 are not included as these were trained by the varied participants. Table presents Mean absolute deviation from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n108.85(50.63)\n83.92(33.76)\n\n\n625\n204.75(84.66)\n183.51(75.92)\n\n\n675\n235.75(81.15)\n206.32(94.64)\n\n\n900\n323.5(130.9)\n289.62(110.07)",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#discussion-1",
    "href": "IGAS.html#discussion-1",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Discussion",
    "text": "Discussion\nThe results of experiment 2 largely conform to the findings of experiment 1. Participants in both varied and constant conditions improved at the task during the training phase. We did not observe the common finding of training under varied conditions producing worse performance during acquisition than training under constant conditions (Catalano & Kleiner, 1984; Wrisberg et al., 1987), which has been suggested to relate to the subsequent benefits of varied training in retention and generalization testing (Soderstrom & Bjork, 2015). However our finding of no difference in training performance between constant and varied groups has been observed in previous work (Chua et al., 2019; Moxley, 1979; Pigott & Shapiro, 1984).\nIn the testing phase, our varied group significantly outperformed the constant conditions in both a general comparison, and in an analysis limited to novel throwing positions. The observed benefit of varied over constant training echoes the findings of many previous visuomotor skill learning studies that have continued to emerge since the introduction of Schmidt’s influential Schema Theory (Catalano & Kleiner, 1984; Chua et al., 2019; Goodwin et al., 1998; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976; Pigott & Shapiro, 1984; Roller et al., 2001; Schmidt, 1975; Willey & Liu, 2018b; Wrisberg et al., 1987; Wulf, 1991). We also join a much smaller set of research to observe this pattern in a computerized task (Seow et al., 2019). One departure from the experiment 1 findings concerns the pattern wherein the varied group outperformed the constant group even from the training position of the constant group, which was significant in experiment 1, but did not reach significance in experiment 2. Although this pattern has been observed elsewhere in the literature (Goode et al., 2008; Kerr & Booth, 1978), the overall evidence for this effect appears to be far weaker than for the more general benefit of varied training in conditions novel to all training groups.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#fitting-model-parameters-separately-by-group",
    "href": "IGAS.html#fitting-model-parameters-separately-by-group",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Fitting model parameters separately by group",
    "text": "Fitting model parameters separately by group\nTo directly control for similarity in Experiment 2, we developed a model-based measure of the similarity between training throws and testing conditions. This similarity measure was a significant predictor of testing performance, e.g., participants whose training throws were more similar to throws that resulted in target hits from the testing positions, tended to perform better during the testing phase. Importantly, the similarity measure did not explain away the group-level benefits of varied training, which remained significant in our linear model predicting testing performance after similarity was added to the model. However, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum & Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the c parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky & Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions.\nTo address this possibility, we compared the original instance-based model of similarity fit against a modified model which separately fits the generalization parameter, c, to varied and constant participants. To perform this parameter fitting, we used the optim function in R, and fit the model to find the c value(s) that maximized the correlation between similarity and testing performance.\nBoth models generate distinct similarity values between training and testing locations. Much like the analyses in Experiment 2, these similarity values are regressed against testing performance in models of the form shown below. As was the case previously, testing performance is defined as the mean absolute distance from the center of the target (with a separate score for each participant, from each position).\nLinear models 1 and 3 both show that similarity is a significant predictor of testing performance (p&lt;.01). Of greater interest is the difference between linear model 2, in which similarity is computed from a single c value fit from all participants (Similarity1c), with linear model 4, which fits the c parameter separately between groups (Similarity2c). In linear model 2, the effect of training group remains significant when controlling for Similarity1c (p&lt;.01), with the varied group still performing significantly better. However, in linear model 4 the addition of the Similarity2c predictor results in the effect of training group becoming nonsignificant (p=.40), suggesting that the effect of varied vs. constant training is accounted for by the Similarity2c predictor. Next, to further establish a difference between the models, we performed nested model comparisons using ANOVA, to see if the addition of the training group parameter led to a significant improvement in model performance. In the first comparison, ANOVA(Linear Model 1, Linear Model 2), the addition of the training group predictor significantly improved the performance of the model (F=22.07, p&lt;.01). However, in the second model comparison, ANOVA (Linear model 3, Linear Model 4) found no improvement in model performance with the addition of the training group predictor (F=1.61, p=.20).\nFinally, we sought to confirm that similarity values generated from the adjusted Similarity2c model had more predictive power than those generated from the original Similarity1c model. Using the BIC function in R, we compared BIC values between linear model 1 (BIC=14604.00) and linear model 3 (BIC = 14587.64). The lower BIC value of model 3 suggests a modest advantage for predicting performance using a similarity measure computed with two c values over similarity computed with a single c value. When fit with separate c values, the best fitting c parameters for the model consistently optimized such that the c value for the varied group (c=.00008) was smaller in magnitude than the c value for the constant group(c= .00011). Recall that similarity decreases as a Gaussian function of distance (equation 1 above), and a smaller value of c will result in a more gradual drop-off in similarity as the distance between training throws and testing solutions increases.\nIn summary, our modeling suggests that an instance-based model which assumes equivalent generalization gradients between constant and varied trained participants is unable to account for the extent of benefits of varied over constant training observed at testing. The evidence for this in the comparative model fits is that when a varied/constant dummy-coded variable for condition is explicitly added to the model, the variable adds a significant contribution to the prediction of test performance, with the variable condition yielding better performance than the constant conditions. However, if the instance-based generalization model is modified to assume that the training groups can differ in the steepness of their generalization gradient, by incorporating a separate generalization parameter for each group, then the instance-based model can account for our experimental results without explicitly taking training group into account. Henceforth this model will be referred to as the Instance-based Generalization with Adaptive Similarity (IGAS) model.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#limitations",
    "href": "IGAS.html#limitations",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Limitations",
    "text": "Limitations\nA limitation of this study concerns the ordering of the testing/transfer trials at the conclusion of both experiments. Participants were tested from each separate position (4 in Experiment 1, 6 in Experiment 2) in a random, intermixed order. Because the varied group was trained from two positions that were also randomly ordered, they may have benefited from experience with this type of sequencing, whereas the constant groups had no experience with switching between positions trial to trial. This concern is somewhat ameliorated by the fact that the testing phase performance of the constant groups from their trained position was not significantly worse than their level of performance at the end of the training phase, suggesting that they were not harmed by random ordering of positions during testing. It should also be noted that the computerized task utilized in the present work is relatively simple compared to many of the real-world tasks utilized in prior research. It is thus conceivable that the effect of variability in more complex tasks is distinct from the process put forward in the present work. An important challenge for future work will be to assess the extent to which IGAS can account for generalization in relatively complex tasks with far more degrees of freedom.\nIt is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g. the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "IGAS.html#conclusion",
    "href": "IGAS.html#conclusion",
    "title": "An instance-based model account of the benefits of varied practice in visuomotor skill",
    "section": "Conclusion",
    "text": "Conclusion\nOur experiments demonstrate a reliable benefit of varied training in a simple projectile launching task. Such results were accounted for by an instance-based model that assumes that varied training results in the computation of a broader similarity-based generalization gradient. Instance-based models augmented with this assumption may be a valuable approach towards better understanding skill generalization and transfer.",
    "crumbs": [
      "An instance-based model account of the benefits of varied practice in visuomotor skill"
    ]
  },
  {
    "objectID": "Outline.html",
    "href": "Outline.html",
    "title": "Dissertation Outline",
    "section": "",
    "text": "In Project 1, I programmed a simple projectile launching task to serve as a conceptual replication of an influential paradigm in the visuomotor skill learning literature. Several of the canonical empirical patterns are replicated, with the varied trained participants tending to perform better during testing in both experiments. A major issue with previous research in the cross-disciplinary “benefits of variability” literature is that many previous works do not adequately control for the similarity between training and testing conditions. Such issues arise when both from failures to consider the possibility of non-linear generalization, and from often the unquestioned assumption that participants are acquiring, and then generalizing from prototype or schema-based representations. I introduce a theoretically motivated method of explicitly quantifying the similarity between training experience and testing condition. The resulting similarity quantity can then be used to explicitly control for similarity (by adding it as a covariate to the statistical model). The effect of variability remains significant while controlling for similarity, which I argue is a more rigorous demonstration of the effect of variability on testing performance than what is typically provided with standard methods. I conclude by introducing an extended version of the model that assumes training variation influences the steepness of the generalization gradient. With this flexible similarity mechanism, the group-level effect of variability can then be accounted for within the similarity-based generalization framework.\n\n\n\n\nIn Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\n\nhttps://tegorman13.github.io/DP/\n\n\n\n\n\n\nProject 1\n\nAbstract\nIntroduction\n\nSimilarity and instance-based approaches to transfer of learning\nThe effect of training variability on transfer\nIssues with Previous Research\n\nExperiment 1\n\nMethods\n\nSample Size Estimation\nParticipants\nTask\n\nResults\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\nDiscussion\n\nExperiment 2\n\nMethods\n\nParticipants\nTask and Procedure\n\nResults\n\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\n\nDiscussion\n\nComputational Model\n\nFitting model parameters separately by group\n\nGeneral Discussion\n\nLimitations\nConclusion\n\nReferences\n\nProject 2\n\nIntroduction\nMethods\n\nParticipants\nTask\nDesign\n\nResults\n\nTraining\nTesting\n\nModeling\n\nALM & Exam Description\nModel Equations\nModel Fitting and Comparison\n\nReferences\n\nProject 3\n\nOverview\nMethods\n\nDataset and Game Description\nSplit-Test Data\n\nTrial-by-trial influence of variability\n\nRandomization\nMeasuring Trial-by-trial variability\n\nComputational Modelling\n\nSimilarity Between Trials\nMeasurement model of learning and performance\n\nReferences"
  },
  {
    "objectID": "Outline.html#dissertation-outline",
    "href": "Outline.html#dissertation-outline",
    "title": "Dissertation Outline",
    "section": "",
    "text": "In Project 1, I programmed a simple projectile launching task to serve as a conceptual replication of an influential paradigm in the visuomotor skill learning literature. Several of the canonical empirical patterns are replicated, with the varied trained participants tending to perform better during testing in both experiments. A major issue with previous research in the cross-disciplinary “benefits of variability” literature is that many previous works do not adequately control for the similarity between training and testing conditions. Such issues arise when both from failures to consider the possibility of non-linear generalization, and from often the unquestioned assumption that participants are acquiring, and then generalizing from prototype or schema-based representations. I introduce a theoretically motivated method of explicitly quantifying the similarity between training experience and testing condition. The resulting similarity quantity can then be used to explicitly control for similarity (by adding it as a covariate to the statistical model). The effect of variability remains significant while controlling for similarity, which I argue is a more rigorous demonstration of the effect of variability on testing performance than what is typically provided with standard methods. I conclude by introducing an extended version of the model that assumes training variation influences the steepness of the generalization gradient. With this flexible similarity mechanism, the group-level effect of variability can then be accounted for within the similarity-based generalization framework.\n\n\n\n\nIn Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\n\nhttps://tegorman13.github.io/DP/\n\n\n\n\n\n\nProject 1\n\nAbstract\nIntroduction\n\nSimilarity and instance-based approaches to transfer of learning\nThe effect of training variability on transfer\nIssues with Previous Research\n\nExperiment 1\n\nMethods\n\nSample Size Estimation\nParticipants\nTask\n\nResults\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\nDiscussion\n\nExperiment 2\n\nMethods\n\nParticipants\nTask and Procedure\n\nResults\n\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\n\nDiscussion\n\nComputational Model\n\nFitting model parameters separately by group\n\nGeneral Discussion\n\nLimitations\nConclusion\n\nReferences\n\nProject 2\n\nIntroduction\nMethods\n\nParticipants\nTask\nDesign\n\nResults\n\nTraining\nTesting\n\nModeling\n\nALM & Exam Description\nModel Equations\nModel Fitting and Comparison\n\nReferences\n\nProject 3\n\nOverview\nMethods\n\nDataset and Game Description\nSplit-Test Data\n\nTrial-by-trial influence of variability\n\nRandomization\nMeasuring Trial-by-trial variability\n\nComputational Modelling\n\nSimilarity Between Trials\nMeasurement model of learning and performance\n\nReferences"
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Dissertation Manuscript",
    "section": "",
    "text": "HTML\n   \n  \n  \n    HTML (new window)\n  \n\n  \n    PDF\n   \n  \n   \n    Manuscript-md",
    "crumbs": [
      "Dissertation Manuscript"
    ]
  }
]