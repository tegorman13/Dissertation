[
  {
    "objectID": "Sections/full.html#varied-training-and-generalization",
    "href": "Sections/full.html#varied-training-and-generalization",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Varied Training and Generalization",
    "text": "Varied Training and Generalization\nVaried training has been shown to influence learning in a wide array of different tasks and domains, including categorization (Hahn et al., 2005; Maddox & Filoteo, 2011; Morgenstern et al., 2019; Nosofsky et al., 2019; Plebanek & James, 2021; Posner & Keele, 1968), language learning (Brekelmans et al., 2022; Jones & Brandt, 2020; Perry et al., 2010; Twomey et al., 2018; Wonnacott et al., 2012), anagram completion (Goode et al., 2008), perceptual learning (Lovibond et al., 2020; Manenti et al., 2023; Robson et al., 2022; Zaman et al., 2021), trajectory extrapolation (Fulvio et al., 2014), cognitive control tasks (Moshon-Cohen et al., 2024; Sabah et al., 2019), associative learning (Fan et al., 2022; Lee et al., 2019; Livesey & McLaren, 2019; Prada & Garcia-Marques, 2020; Reichmann et al., 2023), visual search (George & Egner, 2021; Gonzalez & Madhavan, 2011; T. A. Kelley & Yantis, 2009), voice identity learning (Lavan et al., 2019), face recognition (Burton et al., 2016; Honig et al., 2022; Menon et al., 2015), the perception of social group heterogeneity (Gershman & Cikara, 2023; Konovalova & Le Mens, 2020; Linville & Fischer, 1993; Park & Hastie, 1987) , simple motor learning (Braun et al., 2009; Kerr & Booth, 1978; Roller et al., 2001; Willey & Liu, 2018a), sports training (Breslin et al., 2012; Green et al., 1995; North et al., 2019), and complex skill learning (Hacques et al., 2022; Huet et al., 2011; Seow et al., 2019). See Czyż (2021) or Raviv et al. (2022) for more detailed reviews.\nResearch on the effects of varied training typically manipulates variability in one of two ways. In the first approach, a high variability group is exposed to a greater number of unique instances during training, while a low variability group receives fewer unique instances with more repetitions. Alternatively, both groups may receive the same number of unique instances, but the high variability group’s instances are more widely distributed or spread out in the relevant psychological space, while the low variability group’s instances are clustered more tightly together. Researchers then compare the training groups in terms of their performance during the training phase, as well as their generalization performance during a testing phase. Researchers usually compare the performance of the two groups during both the training phase and a subsequent testing phase. The primary theoretical interest is often to assess the influence of training variability on generalization to novel testing items or conditions. However, the test may also include some or all of the items that were used during the training stage, allowing for an assessment of whether the variability manipulation influenced the learning of the trained items themselves, or to easily measure how much performance degrades as a function of how far away testing items are from the training items.\nThe influence of training variability has received a large amount of attention in the domain of sensorimotor skill learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes that learners possess general motor programs for a class of movements (e.g., an underhand throw). When called up for use motor programs are parameterized by schema rules which determine how the motor program is parameterized or scaled to the particular demands of the current task. Schema theory predicts that variable training facilitates the formation of more robust schemas, which will result in improved generalization or transfer. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, studies using a projectile throwing task might assign participants to either constant training that practice throwing from a single location, or to a varied group that throws from multiple locations. Following training, both groups are then tested from novel throwing locations (Pacheco & Newell, 2018; Pigott & Shapiro, 1984; Willey & Liu, 2018a; Wulf, 1991).\nOne of the earliest and still often cited investigations of Schmidt’s benefits of variability hypothesis was the work of Kerr & Booth (1978). Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blindfolded for testing and did not receive trial by trial feedback in this stage. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. This result provides particularly strong evidence for the benefits of varied practice, as the varied group outperformed the constant group even when tested at the “home-turf” distance that the constant group had exclusively practiced. A similar pattern of results was observed in another study wherein varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket (Green et al., 1995). One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. However, as is the case with many of the patterns commonly observed in the “benefits of variability” literature, the pattern wherein the varied group outperfroms the constant group even from the constants group’s home turf has not been consistently replicated. One recent study attempted a near replication of the Kerr & Booth study (Willey & Liu, 2018b), having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). This study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups. However, this study diverged from the original in that the participants were adults; and the amount of training was much greater (20 sessions with 60 practice trials each, spread out over 5-7 weeks).\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters (Catalano & Kleiner, 1984; Chua et al., 2019; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976), or for a single constant group to train from just one of the conditions experienced by the varied participants (Pigott & Shapiro, 1984; Roller et al., 2001; Wrisberg & McLean, 1984; Wrisberg & Mead, 1983). A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition (Wrisberg et al., 1987; Wulf & Schmidt, 1997). Of particular relevance to the current work is the early study of Catalano & Kleiner (1984), as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions – 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\nBenefits of varied training have also been observed in many studies outside of the sensorimotor domain. Goode et al. (2008) trained participants to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g., LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had three prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups – solving two more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution – further suggesting that there may not have been a strong identity advantage in this task.\nIn the domain of category learning, the constant vs. varied comparison is much less suitable. Instead, researchers will typically employ designs where all training groups encounter numerous stimuli, but one group experiences a greater number of unique exemplars (Brunstein & Gonzalez, 2011; Doyle & Hourihan, 2016; Hosch et al., 2023; Nosofsky et al., 2019; Wahlheim et al., 2012), or designs where the number of unique training exemplars is held constant, but one group trains with items that are more dispersed, or spread out across the category space (Bowman & Zeithamova, 2020; Homa & Vosburgh, 1976; Hu & Nosofsky, 2024; Maddox & Filoteo, 2011; Posner & Keele, 1968).\nMuch of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns (Homa & Vosburgh, 1976; Posner & Keele, 1968). A seminal study by Posner & Keele (1968) trained participants to categorize artificial dot patterns, manipulating whether learners were trained with low variability examples clustered close to the category prototypes (i.e. low distortion training patterns), or higher-variability patterns spread further away from the prototype (i.e. high-distortion patterns). Participants that received training on more highly-distorted items showed superior generalization to novel high distortion patterns in the subsequent testing phase. It should be noted that unlike the sensorimotor studies discussed earlier, the Posner & Keele (1968) study did not present low-varied and high-varied participants with an equal number of training rathers, but instead had participants remain in the training stage of the experiment until they reached a criterion level of performance. This train-until-criterion procedure led to the high-variability condition participants tending to complete a larger number of training trials before switching to the testing stage. More recent work (Hu & Nosofsky, 2024), also used dot pattern categories, but matched the number of training trials across conditions. Under this procedure, higher-variability participants tended to reach lower levels of performance by the end of the training stage. The results in the testing phase were the opposite of Posner & Keele (1968), with the low-variability training group showing superior generalization to novel high-distortion patterns (as well as generalization to novel patterns of low or medium distortion levels). However, whether this discrepancy is solely a result of the different training procedures is unclear, as the studies also differed in the nature of the prototype patterns used. Posner & Keele (1968) utilized simpler, recognizable prototypes (e.g., a triangle, the letter M, the letter F), while Hu & Nosofsky (2024) employed random prototype patterns.\nRecent studies have also begun utilizing more complex or realistic sitmuli when assessing the influence of variability on category learning. Wahlheim et al. (2012) conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address an unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization (Nosofsky et al., 2019). For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions.\nA distinct sub-literature within the category learning domain has examined how the variability or dispersion of the categories themselves influences generalization to ambiguous regions of the category space (e.g., the region between the two categories). The general approach is to train participants with examples from a high variability category and a low variability category. Participants are then tested with novel items located within ambiguous regions of the category space which allow the experimenters to assess whether the difference in category variability influenced how far participants generalize the category boundaries. A. L. Cohen et al. (2001) conducted two experiments with this basic paradigm. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into the category with greater variation.\nFurther observations of widened generalization following varied training have since been observed in numerous investigations (Hahn et al., 2005; Hosch et al., 2023; Hsu & Griffiths, 2010; Perlman et al., 2012; Sakamoto et al., 2008; but see Stewart & Chater, 2002; L.-X. Yang & Wu, 2014; and Seitz et al., 2023). The results of Sakamoto et al. (2008) are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\nThe work of Hahn et al. (2005), is also of particular interest to the present work. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with “neither”, in addition to responses to the two training categories. The “neither” response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The author compared a variety of similarity-based models based around the Generalized Context Model (Nosofsky, 1986) to account for their results, manipulating whether a response-bias or similarity-scaling parameter was fit separately between variability conditions. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improve significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items. This model accounted for both the extended generalization gradients of the varied participants, and also for their poorer performance in a recognition condition.\nVariability has also been examined in the learning of higher-order linguistic categories (Perry et al., 2010). In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the “tight” condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. In the testing phase, the varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category. More interesting was the superior performance of the varied group on a higher order generalization task – such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.\nOf course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation (DeLosh et al., 1997; Sinkeviciute et al., 2019; Van Rossum, 1990; Wrisberg et al., 1987), and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials, or the type of transfer being measured (Berniker et al., 2014; Braithwaite & Goldstone, 2015; Hahn et al., 2005; Lavan et al., 2019; North et al., 2019; Sadakata & McQueen, 2014; Zaman et al., 2021).\nIn an example of a more complex influence of training variation, (Braithwaite & Goldstone, 2015) trained participants on example problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g., all involving people selecting objects) or in which the surface features were varied between examples (e.g., people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants – such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples. Interactions between prior experience and the influence of varied training have also been observed in sensorimotor learning (Del Rey et al., 1982; Guadagnoli et al., 1999). Del Rey et al. (1982) recruited participants who self-reported either extensive, or very little experience with athletic activities, and then trained participants on a coincident timing task under with either a single constant training velocity, with one of several varied training procedures. Unsurprisingly, athlete participants had superior performance during training, regardless of condition, and training performance was superior for all subjects in the constant group. Of greater interest is the pattern of testing results from novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received variable training. Non-athletes showed the opposite pattern, with superior performance for those who had constant training.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#existing-theoretical-frameworks",
    "href": "Sections/full.html#existing-theoretical-frameworks",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Existing Theoretical Frameworks",
    "text": "Existing Theoretical Frameworks\nA number of theoretical frameworks have been proposed to conceptually explain the effects of varied training on learning and generalization. Schema theory (described in more detail above), posts that varied practice leads to the formation of more flexible motor schemas, which then facilitate generalization (Schmidt, 1975). The desirable difficulties framework (Bjork & Bjork, 2011; Soderstrom & Bjork, 2015) proposes that variable practice conditions may impair initial performance but then enhance longer-term retention and transfer. Similarly, the challenge point framework (Guadagnoli & Lee, 2004) contends that training variation induces optimal learning occurs insofar as it causes the difficulty of practice tasks to be appropriately matched to the learner’s capabilities, but may also be detrimental if the amount of variation causes the task to be too difficult.\nWhile these frameworks offer valuable conceptual accounts, there has been a limited application of computational modeling efforts aimed at quantitatively assessing and comparing the learning and generalization mechanisms which may be underlying the influence of variability in visuomotor skill learning. In contrast, the effects of variability have received more formal computational treatment in other domains, such as category learning Hu & Nosofsky (2024), language learning (Jones & Brandt, 2020), and function learning (DeLosh et al., 1997). A primary goal of the current dissertation is to to address this gap by adapting and applying modeling approaches from these other domains to investigate the effects of training variability in visuomotor skill learning and function learning tasks.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#the-current-work",
    "href": "Sections/full.html#the-current-work",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "The current work",
    "text": "The current work\nThe overarching purpose of this dissertation is to investigate the effects of training variability on learning and generalization within visuomotor skill learning and function learning. Our investigation is structured into two main projects, each employing distinct experimental paradigms and computational modeling frameworks to elucidate how and when variability in training enhances or impedes subsequent generalization.\nIn Project 1, we investigated the influence of varied practice in a simple visuomotor projectile launching task. Experiments 1 and 2 compared the performance of constant and varied training groups to assess potential benefits of variability on transfer to novel testing conditions. To account for the observed empirical effects, we introduced the Instance-based Generalization with Adaptive Similarity (IGAS) model. IGAS provides a novel computational approach for quantifying the similarity between training experiences and transfer conditions, while also allowing for variability to influence the generalization gradient itself.\nProject 2 shifted focus to the domain of function learning by employing a visuomotor extrapolation task. Across three experiments, we examined how constant and varied training regimes affected learning, discrimination between stimuli, and the ability to extrapolate to novel regions of the function’s input space. To model human performance in this task, we fit the influential Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM) to individual participant data using advanced Bayesian parameter estimation techniques.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#abstract",
    "href": "Sections/full.html#abstract",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Abstract",
    "text": "Abstract\nExposing learners to variability during training has been demonstrated to improve performance in subsequent transfer testing. Such variability benefits are often accounted for by assuming that learners are developing some general task schema or structure. However, much of this research has neglected to account for differences in similarity between varied and constant training conditions. In a between-groups manipulation, we trained participants on a simple projectile launching task, with either varied or constant conditions. We replicate previous findings showing a transfer advantage of varied over constant training. Furthermore, we show that a standard similarity model is insufficient to account for the benefits of variation, but, if the model is adjusted to assume that varied learners are tuned towards a broader generalization gradient, then a similarity-based model is sufficient to explain the observed benefits of variation. Our results therefore suggest that some variability benefits can be accommodated within instance-based models without positing the learning of some schemata or structure.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#introduction-1",
    "href": "Sections/full.html#introduction-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Introduction",
    "text": "Introduction\n\nSimilarity and instance-based approaches to transfer of learning\nEarly models of learning often assumed that discrete experiences with some task or category were not stored individually in memory, but instead promoted the formation of a summary representation, often referred to as a prototype or schema, and that exposure to novel examples would then prompt the retrieval of whichever preexisting prototype was most similar. In addition to being a landmark study on the influence of training variability, Posner & Keele (1968) (described above) also put forward an influential argument concerning the nature of the mental representations acquired during learning - namely that learners tend to abstract a prototype, or aggregate representation of the dot pattern categories, rather than encoding each individual stimuli. Recall that participants are trained on only on distortions of the category prototypes (e.g., low, medium or high distortions), never encountering the exact prototypes during the training stage. Then, in the testing phase, participants are tested with the prototype patterns, their old training items, and novel low, medium and high distortions. The authors found that participants had the highest testing accuracy for the previously unseen prototype patterns, followed by the old training items, and then the novel low, medium and high distortions. The authors interpreted this pattern as evidence that participants had acquired prototype representation of the category, as opposed to storing each individual training instance, and that generalization was based on the similarity of the testing items to the learned prototype representations. Posner & Keele (1968) has been extremely influential, and continues to be cited as evidence that prototype abstraction underlies the benefits of varied training. It’s also referenced as a key influence in the development of the “Schema Theory of Motor Learning” Schmidt (1975), which in turn influenced decades of research on the potential benefits of varied training in motor skill learning. However a number of the core assumptions utilized by Posner & Keele (1968) were later called into question both empirically and with competing theoretical accounts (Hintzman, 1984, 1986; Knapp & Anderson, 1984; McClelland & Rumelhart, 1985; Nosofsky & Kruschke, 1992; Palmeri & Nosofsky, 2001; Zaki & Nosofsky, 2007). Palmeri & Nosofsky (2001) demonstrated both the dangers of assuming that psychological representations mimic the metric stimulus space, as well the viability of models with simpler representational assumptions. These authors conducted a near replication of the Posner & Keele (1968) study, but also had participants provide similarity judgements of the dot pattern stimuli after completing the training phase. A multidimensional scaling analysis of the similarity judgements revelead that the psychological representations of the prototype stimuli were not located in the middle of the training stimuli, but were instead extreme points in the psychological space. The authors also demonstrated the generalization patterns of Posner & Keele (1968) could be accounted for by an exemplar-based model, without any need to assume the abstraction of a prototype.\nInstance-based, or exemplar-based models generally assume that learners encode each experience with a task as a separate instance/exemplar/trace, and that each encoded trace is in turn compared against novel stimuli (Estes, 1994; Hintzman, 1984; Jamieson et al., 2022; Medin & Schaffer, 1978; Nosofsky, 1986). As the number of stored instances increases, so does the likelihood that some previously stored instance will be retrieved to aid in the performance of a novel task. Stored instances are retrieved in the context of novel stimuli or tasks if they are sufficiently similar, thus suggesting that the process of computing similarity is of central importance to generalization.\nSimilarity, defined in this literature as a function of psychological distance between instances or categories, has provided a successful account of generalization across numerous tasks and domains. In an influential study demonstrating an ordinal similarity effect, experimenters employed a numerosity judgment task in which participants quickly report the number of dots flashed on a screen. Performance (in terms of response times to new patterns) on novel dot configurations varied as an inverse function of their similarity to previously trained dot configurations Palmeri (1997). That is, performance was better on novel configurations moderately similar to trained configurations than to configurations with low-similarity, and also better on low-similarity configurations than to even less similar, unrelated configurations. Instance-based similarity approaches have had some success accounting for performance in certain sub-domains of motor learning (R. G. Cohen & Rosenbaum, 2004; Crump & Logan, 2010; Meigh et al., 2018; Poldrack et al., 1999; Wifall et al., 2017). Crump & Logan (2010) trained participants to type words on an unfamiliar keyboard, while constraining the letters composing the training words to a pre-specified letter set. Following training, typing speed was tested on previously experienced words composed of previously experienced letters; novel words composed of letters from the trained letter set; and novel words composed of letters from an untrained letter set. Consistent with an instance-based account, transfer performance was graded such that participants were fastest at typing the words they had previously trained on, followed by novel words composed of letters they had trained on, and slowest performance for new words composed of untrained letters.\n\n\nIssues with Previous Research\nAlthough the benefits of training variation in visuomotor skill learning have been observed many times, null findings have also been repeatedly found, leading some researchers to question the veracity of the variability of practice hypothesis (Newell, 2003; Van Rossum, 1990). Critics have also pointed out that investigations of the effects of training variability, of the sort described above, often fail to control for the effect of similarity between training and testing conditions. For training tasks in which participants have numerous degrees of freedom (e.g., projectile throwing tasks where participants control the x and y velocity of the projectile), varied groups are likely to experience a wider range of the task space over the course of their training (e.g., more unique combinations of x and y velocities). Experimenters may attempt to account for this possibility by ensuring that the training location(s) of the varied and constant groups are an equal distance away from the eventual transfer locations, such that their training throws are, on average, equally similar to throws that would lead to good performance at the transfer locations. However, even this level of experimental control may still be insufficient to rule out the effect of similarity on transfer. Given that psychological similarity is typically best described as either a Gaussian or exponentially decaying function of psychological distance (Ennis et al., 1988; Ghahramani et al., 1996; Logan, 1988; Nosofsky, 1992; Shepard, 1987; Thoroughman & Taylor, 2005), it is plausible that a subset of the most similar training instances could have a disproportionate impact on generalization to transfer conditions, even if the average distance between training and transfer conditions is identical between groups. Figure 1 demonstrates the consequences of a generalization gradient that drops off as a Gaussian function of distance from training, as compared to a linear drop-off.\n\n\nDisplay code\np=2\nc&lt;- .0002\nsimdat &lt;- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %&gt;%\n                     mutate(plotjitter=ifelse(condit==\"varied\",0,7),\n                            linScale=ifelse(condit==\"varied\",980,1000),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genLinear=1000-abs(x-train.position)+plotjitter) %&gt;% \n  #group_by(condit) %&gt;% mutate(scaleLinear=(genLinear-min(genLinear))/(max(genLinear)-min(genLinear))) \n  group_by(x,condit) %&gt;%\n  reframe(genGauss=mean(genGauss),genLinear=mean(genLinear)/linScale,.groups = 'keep')\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs &lt;- list(geom_line(alpha=.7,size=.4),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=447,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=450,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=659,y=1.05,label=\"Constant\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=657,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=847,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=850,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1),\n                        legend.position=\"none\"))\n\nip1 &lt;- simdat  %&gt;% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"\")\nip2 &lt;- simdat %&gt;%  ggplot(aes(x,y=genLinear,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")\n\nplot_grid(ip1,ip2,ncol=2,rel_heights=c(1))\n\n\n\n\n\n\n\n\nFigure 1: Left panel- Generalization predicted from a simple model that assumes a linear generalization function. A varied group (red vertical lines indicate the 2 training locations) trained from positions 400 and 800, and a constant group (blue vertical line), trained from position 600. Right panel- if a Gaussian generalization function is assumed, then varied training (400, 800) is predicted to result in better generalization to positions close to 400 and 800 than does constant training at 600. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\n\n\n\n\n\nIn addition to largely overlooking the potential for non-linear generalization to confound interpretations of training manipulations, the visuomotor skill learning literature also rarely considers alternatives to schema representations (Chamberlin & Magill, 1992b). Although schema-theory remains influential within certain literatures, instance or exemplar-based models have accounted for human behavior across myriad domains (Jamieson et al., 2022; Logan, 2002). As mentioned above, instance based accounts have been shown to perform well on a variety of different tasks with motoric components (Crump & Logan, 2010; Gandolfo et al., 1996; Meigh et al., 2018; Rosenbaum et al., 1995; van Dam & Ernst, 2015). However, such accounts have received little attention within the subdomain of visuomotor skill learning focused on the benefits of varied training.\nThe present work examines whether the commonly observed benefits of varied training can be accounted for by a theoretrically motivated measure of the similarity between training throws and the testing solution space. We first attempt to replicate previous work finding an advantage of varied training over constant training in a projectile launching task. We then examine the extent to which this advantage can be explained by an instance-based similarity model.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#experiment-1",
    "href": "Sections/full.html#experiment-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 1",
    "text": "Experiment 1\n\nMethods\n\nSample Size Estimation\nTo obtain an independent estimate of effect size, we identified previous investigations which included between-subjects contrasts of varied and constant conditions following training on an accuracy based projectile launching task (Chua et al., 2019; Goodwin et al., 1998; Kerr & Booth, 1978; Wulf, 1991). We then averaged effects across these studies, yielding a Cohen’s f =.43. The GPower 3.1 software package (Faul et al., 2009) was then used to determine that a power of 80% requires a sample size of at least 23 participants per condition. All experiments reported in the present manuscript exceed this minimum number of participants per condition.\n\n\nParticipants\nParticipants were recruited from an undergraduate population that is 63% female and consists almost entirely of individuals aged 18 to 22 years. A total of 110 Indiana University psychology students participated in Experiment 1. We subsequently excluded 34 participants for poor performance on one of the dependent measures of the task (2.5-3 standard deviations worse than the median subject at the task) or for displaying a pattern of responses that was clearly indicative of a lack of engagement with the task (e.g., simply dropping the ball on each trial rather than throwing it at the target), or for reporting that they completed the experiment on a phone or tablet device, despite the instructions not to use one of these devices. A total of 74 participants were retained for the final analyses, 35 in the varied group and 39 in the constant group.\n\n\nTask\nThe experimental task was programmed in JavaScript, using packages from the Phaser physics engine (https://phaser.io) and the jsPsych library (de Leeuw, 2015). The stimuli, presented on a black background, consisted of a circular blue ball - controlled by the participant via the mouse or trackpad cursor; a rectangular green target; a red rectangular barrier located between the ball and the target; and an orange square within which the participant could control the ball before releasing it in a throw towards the target. Because the task was administered online, the absolute distance between stimuli could vary depending on the size of the computer monitor being used, but the relative distance between the stimuli was held constant. Likewise, the distance between the center of the target and the training and testing locations was scaled such that relative distances were preserved regardless of screen size. For the sake of brevity, subsequent mentions of this relative distance between stimuli, or the position where the ball landed in relation to the center of the target, will be referred to simply as distance. Figure 2 displays the layout of the task, as it would appear to a participant at the start of a trial, with the ball appearing in the center of the orange square. Using a mouse or trackpad, participants click down on the ball to take control of the ball, connecting the movement of the ball to the movement of the cursor. Participants can then “wind up” the ball by dragging it (within the confines of the orange square) and then launch the ball by releasing the cursor. If the ball does not land on the target, participants are presented with feedback in red text at the top right of the screen, specifying how many scaled units away the ball was from the center of the target. If the ball was thrown outside of the boundary of the screen participants are given feedback as to how far away from the target center the ball would have been if it had continued its trajectory. If the ball strikes the barrier (from the side or by landing on top), feedback is presented telling participants to avoid hitting the barrier. If participants drag the ball outside of the orange square before releasing it, the trial terminates, and they are reminded to release the ball within the orange square. If the ball lands on the target, feedback is presented in green text, confirming that the target was hit, and presenting additional feedback on how many units away the ball was from the exact center of the target.\nLink to abbreviated example of task.\n\n\n\n\n\n\n\n\nFigure 2: The stimuli of the task consisted of a blue ball, which the participants would launch at the green target, while avoiding the red barrier. On each trial, the ball would appear in the center of the orange square, with the position of the orange square varying between experimental conditions. Participants were constrained to release the ball within the square.\n\n\n\n\n\n\n\nProcedure\nParticipants first electronically consented to participate, and then read instructions for the task which explained how to control the ball, and the goal of throwing the ball as close to the center of the target as possible. The training phase was split into 10 blocks of 20 trials, for a total of 200 training trials. Participants in the constant condition trained exclusively from a single location (760 scaled units from the target center). Participants in the varied condition trained from two locations (610 and 910 scaled units from the target center), encountering each location 100 times. The sequence of throwing locations was pseudo-random for the varied group, with the constraint that within every block of 20 training throws both training locations would occur 10 times. Participants in both conditions also received intermittent testing trials after every 20 training trials. Intermittent testing trials provided no feedback of any kind. The ball would disappear from view as soon as it left the orange square, and participants were prompted to start the next trial without receiving any information about the accuracy of the throw. Each intermittent testing stage consisted of two trials from each of the three training positions (i.e. all participants executed two trials each from Positions 610, 760, and 910 during each of the 10 intermittent testing stages). Following training, all participants completed a final testing phase from four positions: 1) their training location, 2) the training location(s) of the other group, 3) a location novel to both groups. The testing phase consisted of 15 trials from each of the four locations, presented in a randomized order. All trials in the final testing phase included feedback. After finishing the final testing portion of the study, participants were queried as to whether they completed the study using a mouse, a trackpad, or some other device (this information was used in the exclusion process described above). Finally, participants were debriefed as to the hypotheses and manipulation of the study.\n\n\n\nResults\n\nData Processing and Statistical Packages\nTo prepare the data, we removed trials that were not easily interpretable as performance indicators in our task. Removed trials included: 1) those in which participants dragged the ball outside of the orange starting box without releasing it, 2) trials in which participants clicked on the ball, and then immediately released it, causing the ball to drop straight down, 3) outlier trials in which the ball was thrown more than 2.5 standard deviations further than the average throw (calculated separately for each throwing position), and 4) trials in which the ball struck the barrier. The primary measure of performance used in all analyses was the absolute distance away from the center of the target. The absolute distance was calculated on every trial, and then averaged within each subject to yield a single performance score, for each position. A consistent pattern across training and testing phases in both experiments was for participants to perform worse from throwing positions further away from the target – a pattern which we refer to as the difficulty of the positions. However, there were no interactions between throwing position and training conditions, allowing us to collapse across positions in cases where contrasts for specific positions were not of interest. All data processing and statistical analyses were performed in R version 4.32 (Team, 2020). ANOVAs for group comparisons were performed using the rstatix package (Kassambara, 2021).\n\n\nTraining Phase\nFigure 3 below shows aggregate training performance binned into three stages representing the beginning, middle, and end of the training phase. Because the two conditions trained from target distances that were not equally difficult, it was not possible to directly compare performance between conditions in the training phase. Our focus for the training data analysis was instead to establish that participants did improve their performance over the course of training, and to examine whether there was any interaction between training stage and condition. Descriptive statistics for the intermittent testing phase are provided in the supplementary materials.\nWe performed an ANOVA comparison with stage as a within-group factor and condition as between-group factor. The analysis revealed a significant effect of training stage F(2,142)=62.4, p&lt;.001, \\(\\eta^{2}_G\\) = .17, such that performance improved over the course of training. There was no significant effect of condition F(1,71)=1.42, p=.24, \\(\\eta^{2}_G\\) = .02, and no significant interaction between condition and training stage, F(2,142)=.10, p=.91, \\(\\eta^{2}_G\\) &lt; .01.\n\n\nDisplay code\nexp1TrainPosition &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1TrainPosition3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\n\ne1train2 &lt;- exp1TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",fun=mean,position=dodge)+\n  facet_wrap(~conditType,ncol=2)+\n  stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  xlab(\"Training Location(s)\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)\n\n\n#plot_grid(title,e1train2,capt,ncol=1,rel_heights=c(.18,1,.15))\nplot_grid(e1train2,ncol=1)\n\n\n\n\n\n\n\n\nFigure 3: Training performance for varied and constant participants binned into three stages. Shorter bars indicate better performance (ball landing closer to the center of the target). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\nTesting Phase\nIn Experiment 1, a single constant-trained group was compared against a single varied-trained group. At the transfer phase, all participants were tested from 3 positions: 1) the positions(s) from their own training, 2) the training position(s) of the other group, and 3) a position novel to both groups. Overall, group performance was compared with a mixed type III ANOVA, with condition (varied vs. constant) as a between-subject factor and throwing location as a within-subject variable. The effect of throwing position was strong, F(3,213) = 56.12, p&lt;.001, η2G = .23. The effect of training condition was significant F(1,71)=8.19, p&lt;.01, η2G = .07. There was no significant interaction between group and position, F(3,213)=1.81, p=.15, η2G = .01.\n\n\nDisplay code\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n#manuscript plot\ne1test1=exp1.Test %&gt;% ggplot(aes(x=positionX,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",fun=mean,position=dodge)+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+\n  guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(legend.title.align=.25, \n  axis.text.x = element_text(size = 8.5,angle=0),\n  legend.position=\"top\")+\n  scale_x_discrete(name=\"Testing Location\",labels=e1Labels)\n\ne1test1\n\n\n\n\n\n\n\n\nFigure 4: Testing performance for each of the 4 testing positions, compared between training conditions. Positions 610 and 910 were trained on by the varied group, and novel for the constant group. Position 760 was trained on by the constant group, and novel for the varied group. Position 835 was novel for both groups. Shorter bars are indicative of better performance (the ball landing closer to the center of the target). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nDisplay code\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n\ntest= exp1.Test %&gt;% dplyr::rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n test=test %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n test = test %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n test=test[,1:3]\n\nkable(test,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  #%&gt;% kableExtra::kable_styling(position=\"left\") # %&gt;%  # kable_classic() #%&gt;% kableExtra::footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 1: Testing performance for varied and constant groups in experiment 1. Mean absolute deviation from the center of the target, with standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n610\n132.48(50.85)\n104.2(38.92)\n\n\n760\n207.26(89.19)\n167.12(72.29)\n\n\n835\n249.13(105.92)\n197.22(109.71)\n\n\n910\n289.36(122.48)\n212.86(113.93)\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nIn Experiment 1, we found that varied training resulted in superior testing performance than constant training, from both a position novel to both groups, and from the position at which the constant group was trained, which was novel to the varied condition. The superiority of varied training over constant training even at the constant training position is of particular note, given that testing at this position should have been highly similar for participants in the constant condition. It should also be noted, though, that testing at the constant trained position is not exactly identical to training from that position, given that the context of testing is different in several ways from that of training, such as the testing trials from the different positions being intermixed, as well as a simple change in context as a function of time. Such contextual differences will be further considered in the General Discussion.\nIn addition to the variation of throwing position during training, the participants in the varied condition of Experiment 1 also received training practice from the closest/easiest position, as well as from the furthest/most difficult position that would later be encountered by all participants during testing. The varied condition also had the potential advantage of interpolating both of the novel positions from which they would later be tested. Experiment 2 thus sought to address these issues by comparing a varied condition to multiple constant conditions.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#experiment-2",
    "href": "Sections/full.html#experiment-2",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn Experiment 2, we sought to replicate our findings from Experiment 1 with a new sample of participants, while also addressing the possibility of the pattern of results in Experiment 1 being explained by some idiosyncrasy of the particular training location of the constant group relative to the varied group. To this end, Experiment 2 employed the same basic procedure as Experiment 1, but was designed with six separate constant groups each trained from one of six different locations (400, 500, 625, 675, 800, or 900), and a varied group trained from two locations (500 and 800). Participants in all seven groups were then tested from each of the 6 unique positions.\n\nMethods\n\nParticipants\nA total of 306 Indiana University psychology students participated in Experiment 2, which was also conducted online. As was the case in Experiment 1, the undergraduate population from which we recruited participants was 63% female and primarily composed of 18–22-year-old individuals. Using the same procedure as Experiment 1, we excluded 98 participants for exceptionally poor performance at one of the dependent measures of the task, or for displaying a pattern of responses indicative of a lack of engagement with the task. A total of 208 participants were included in the final analyses with 31 in the varied group and 32, 28, 37, 25, 29, 26 participants in the constant groups training from location 400, 500, 625, 675, 800, and 900, respectively. All participants were compensated with course credit.\n\n\nTask and Procedure\nThe task of Experiment 2 was identical to that of Experiment 1, in all but some minor adjustments to the height of the barrier, and the relative distance between the barrier and the target. Additionally, the intermittent testing trials featured in experiment 1 were not utilized in Experiment 2. An abbreviated demo of the task used for Experiment 2 can be found at (https://pcl.sitehost.iu.edu/tg/demos/igas_expt2_demo.html).\nThe procedure for Experiment 2 was also quite similar to Experiment 1. Participants completed 140 training trials, all of which were from the same position for the constant groups and split evenly (70 trials each - randomized) for the varied group. In the testing phase, participants completed 30 trials from each of the six locations that had been used separately across each of the constant groups during training. Each of the constant groups thus experienced one trained location and five novel throwing locations in the testing phase, while the varied group experiences 2 previously trained, and 4 novel locations.\n\n\n\nResults\n\nData Processing and Statistical Packages\nAfter confirming that condition and throwing position did not have any significant interactions, we standardized performance within each position, and then average across position to yield a single performance measure per participant. This standardization did not influence our pattern of results. As in Experiment 1, we performed type III ANOVAs due to our unbalanced design, however the pattern of results presented below is not altered if type 1 or type III tests are used instead. The statistical software for the primary analyses was the same as for Experiment 1. Individual learning rates in the testing phase, compared between groups in the supplementary analyses, were fit using the TEfit package in R (Cochrane, 2020).\n\n\nTraining Phase\nThe different training conditions trained from positions that were not equivalently difficult and are thus not easily amenable to comparison. As previously stated, the primary interest of the training data is confirmation that some learning did occur. Figure 5 depicts the training performance of the varied group alongside that of the aggregate of the six constant groups (5a), and each of the 6 separate constant groups (5b). An ANOVA comparison with training stage (beginning, middle, end) as a within-group factor and group (the varied condition vs. the 6 constant conditions collapsed together) as a between-subject factor revealed no significant effect of group on training performance, F(1,206)=.55,p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,412)=77.91, p&lt;.001, \\(\\eta^{2}_G\\) =.05, and no significant interaction between group and training stage, F(2,412)=.489 p=.61, \\(\\eta^{2}_G\\) &lt;.01. We also tested for a difference in training performance between the varied group and the two constant groups that trained matching throwing positions (i.e., the constant groups training from position 500, and position 800). The results of our ANOVA on this limited dataset mirrors that of the full-group analysis, with no significant effect of group F(1,86)=.48, p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,172)=56.29, p&lt;.001, \\(\\eta^{2}_G\\) =.11, and no significant interaction between group and training stage, F(2,172)=.341 p=.71, \\(\\eta^{2}_G\\) &lt;.01.\n\n\nDisplay code\ne2$stage &lt;- factor(e2$stage, levels = c(\"Beginning\", \"Middle\", \"End\",\"Transfer\"),ordered = TRUE)\n\nexp2TrainPosition &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter))%&gt;% as.data.frame()\n\nexp2TrainPosition3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05))%&gt;% as.data.frame()\n\nexp2Train &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter)) %&gt;% as.data.frame()\n\nexp2Train3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\ntransfer &lt;- filter(e2, stage==\"Transfer\") %&gt;% droplevels() %&gt;% select(-trainHalf,-initialVelocityY,ThrowPosition2)%&gt;% ungroup()\ntransfer &lt;- transfer %&gt;% group_by(positionX) %&gt;% mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\ntransfer &lt;- transfer %&gt;% group_by(sbjCode,positionX) %&gt;% mutate(ind=1,testPosIndex=cumsum(ind),posN=max(testPosIndex)) %&gt;%\n  select(-ind) %&gt;% mutate(testHalf = case_when(testPosIndex&lt;15 ~\"1st Half\",testPosIndex&gt;=15 ~\"2nd Half\")) %&gt;% rstatix::convert_as_factor(testHalf)\n\nvariedTest &lt;- transfer %&gt;% filter(condit==7) %&gt;% mutate(extrapolate=ifelse(positionX==\"900\" | positionX==\"400\",\"extrapolation\",\"interpolation\")) \nconstantTest &lt;- transfer %&gt;% filter(condit!=7) %&gt;% mutate(extrapolate=ifelse(distFromTrain==0,\"interpolation\",\"extrapolation\"))\n\ntransfer &lt;- rbind(variedTest,constantTest)\ntransfer&lt;- transfer %&gt;% mutate(novel=ifelse(distFromTrain3==0,\"trainedLocation\",\"novelLocation\"))%&gt;% rstatix::convert_as_factor(novel,extrapolate)\n\ntransfer &lt;- transfer %&gt;% relocate(sbjCode,condit2,Group,conditType2,stage,trial,novel,extrapolate,positionX,AbsDistFromCenter,globalAvg,globalSd,scaledDev,distFromTrain3) %&gt;% ungroup()\n\n\n# novelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0) %&gt;% select(-globalAvg,-globalSd,-scaledDev)%&gt;% droplevels() %&gt;% ungroup()\n# novelAll &lt;- novelAll %&gt;% group_by(positionX) %&gt;%\n#  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n#   group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\nnovelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0)\nnovelAllMatched &lt;- novelAll %&gt;% filter(condit!=5,condit!=2)\n\n\nconstantIden &lt;- transfer %&gt;% filter(condit !=7,distFromTrain==0) # only constant groups from their training position\nvariedTest &lt;- transfer %&gt;% filter(condit==7) # only varied testing\nvariedVsIden &lt;- rbind(constantIden,variedTest) # all varied combined with constant identity\n\n\nvariedNovel &lt;- variedTest %&gt;% filter(distFromTrain3 !=0) # removes 500 and 800 from varied\nconstantIden2 &lt;- transfer %&gt;% filter(condit !=7,condit!=5,condit!=2,distFromTrain==0) # only constant groups from training position 400,625,675,900\nvariedVsNovelIden &lt;- rbind(constantIden2,variedNovel) # novel positions for varied, trained for constant\n\nexp2.Test &lt;- transfer %&gt;%group_by(sbjCode,conditType,positionX,ThrowPosition)%&gt;%\n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test2 &lt;- exp2.Test %&gt;% group_by(sbjCode,conditType)%&gt;%\n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test7 &lt;- transfer %&gt;%group_by(Group2,sbjCode,positionX,Group,conditType,ThrowPosition4) %&gt;% \n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\nexp2.Test7.agg &lt;- exp2.Test7  %&gt;%group_by(Group2,sbjCode,Group,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\nexp2.Test7.agg2 &lt;- exp2.Test7  %&gt;%group_by(sbjCode,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\n\n\n\nDisplay code\n### New - 3 stage\ne2train1&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=stage,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=stage,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+facet_wrap(~conditType,ncol=2)+\n  ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Stage\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0.0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"A\")\n\ne2train2&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  facet_wrap(~conditType,ncol=2)+stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Location(s)\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"B\")\n\n#plot_grid(e2train1,e2train2,ncol=1)\n\ne2train1/e2train2\n\n\n\n\n\n\n\n\nFigure 5: Training performance for the six constant conditions, and the varied condition, binned into three stages. On the left side, the six constant groups are averaged together, as are the two training positions for the varied group. On the right side, the six constant groups are shown separately, with each set of bars representing the beginning, middle, and end of training for a single constant group that trained from the position indicated on the x-axis. Figure 5b also shows training performance separately for both of the throwing locations trained by the varied group. Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\nTesting Phase\nIn Experiment 2, a single varied condition (trained from two positions, 500 and 800), was compared against six separate constant groups (trained from a single position, 400, 500, 625, 675, 800 or 900). For the testing phase, all participants were tested from all six positions, four of which were novel for the varied condition, and five of which were novel for each of the constant groups. For a general comparison, we took the absolute deviations for each throwing position and computed standardized scores across all participants, and then averaged across throwing position. The six constant groups were then collapsed together allowing us to make a simple comparison between training conditions (constant vs. varied). A type III between-subjects ANOVA was performed, yielding a significant effect of condition F(1,206)=4.33, p=.039, \\(\\eta^{2}_G\\) =.02. Descriptive statistics for each condition are shown in table 2. In Figure 6 visualizes the consistent advantage of the varied condition over the constant groups across the testing positions. Figure 6 shows performance between the varied condition and the individual constant groups.\n\n\nDisplay code\n# manuscript plot\ne2test1&lt;-exp2.Test %&gt;% ggplot(aes(x=ThrowPosition,y=MeanTargetDeviance,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25)+\n  ggtitle(\"A\")\n\n\ne2test2&lt;-exp2.Test7 %&gt;% \n  ggplot(aes(x=Group,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",position=position_dodge(),fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=position_dodge())+\n  facet_wrap(~ThrowPosition4)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25,\n        axis.text.x = element_text(size = 7,angle=45,hjust=1))+\n  scale_x_discrete(name=\" Training Group\",labels=e2Labels)+ggtitle(\"B\")\n\ne2test1 / e2test2\n\n\n\n\n\n\n\n\nFigure 6: Testing phase performance from each of the six testing positions. The six constant conditions are averaged together into a single constant group, compared against the single varied-trained group.B) Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\n\nDisplay code\ntab2= exp2.Test %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n tab2=tab2 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n tab2 = tab2 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% \n   select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab2=tab2[,1:3]\n\n\nkable(tab2,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  #%&gt;% kableExtra::kable_styling(position=\"left\") #%&gt;% kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 2: Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n100.59(46.3)\n83.92(33.76)\n\n\n500\n152.28(69.82)\n134.38(61.38)\n\n\n625\n211.21(90.95)\n183.51(75.92)\n\n\n675\n233.32(93.35)\n206.32(94.64)\n\n\n800\n283.24(102.85)\n242.65(89.73)\n\n\n900\n343.51(114.33)\n289.62(110.07)\n\n\n\n\n\n\n\n\nNext, we compared the testing performance of constant and varied groups from only positions that participants had not encountered during training. Constant participants each had 5 novel positions, whereas varied participants tested from 4 novel positions (400,625,675,900). We first standardized performance within in each position, and then averaged across positions. Here again, we found a significant effect of condition (constant vs. varied): F(1,206)=4.30, p=.039, \\(\\eta^{2}_G\\) = .02 .\n\n\nDisplay code\nsum.novelAll &lt;- novelAll %&gt;% group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ntab3=sum.novelAll %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\n tab3=tab3 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \n tab3 = tab3 %&gt;% \n   mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab3=tab3[,1:3]\n\n\n\nkable(tab3,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\")) # %&gt;% kableExtra::kable_styling(position=\"left\") #%&gt;% kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 3: Testing performance from novel positions. Includes data only from positions that were not encountered during the training stage (e.g., excludes positions 500 and 800 for the varied group, and one of the six locations for each of the constant groups). Table presents Mean absolute deviations from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n98.84(45.31)\n83.92(33.76)\n\n\n500\n152.12(69.94)\nNA\n\n\n625\n212.91(92.76)\n183.51(75.92)\n\n\n675\n232.9(95.53)\n206.32(94.64)\n\n\n800\n285.91(102.81)\nNA\n\n\n900\n346.96(111.35)\n289.62(110.07)\n\n\n\n\n\n\n\n\nFinally, corresponding to the comparison of position 760 from Experiment 1, we compared the test performance of the varied group against the constant group from only the positions that the constant groups trained. Such positions were novel to the varied group (thus this analysis omitted two constant groups that trained from positions 500 or 800 as those positions were not novel to the varied group). Figure 7 displays the particular subset of comparisons utilized for this analysis. Again, we standardized performance within each position before performing the analyses on the aggregated data. In this case, the effect of condition did not reach statistical significance F(1,149)=3.14, p=.079, \\(\\eta^{2}_G\\) = .02. Table 4 provides descriptive statistics.\n\n\nDisplay code\nsum.variedVsNovelIden &lt;- variedVsNovelIden  %&gt;%\n  group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ne2Test2 &lt;- sum.variedVsNovelIden %&gt;% ggplot(aes(x=positionX,y=MeanTargetDev,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)\n\n\ne2Test2\n\n\n\n\n\n\n\n\nFigure 7: A comparison of throwing location that are identical to those trained by the constant participants (e.g., constant participants trained at position 900, tested from position 900), which are also novel to the varied-trained participants (thus excluding positions 500 and 800). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nDisplay code\ntab4=sum.variedVsNovelIden %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\ntab4=tab4 %&gt;% group_by(Condition) %&gt;% \n   mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \ntab4 = tab4 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab4=tab4[,1:3]\n\nkable(tab4,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\")) # %&gt;% kableExtra::kable_styling(position=\"left\") #%&gt;% kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 4: Testing performance from the locations trained by constant participants and novel to varied participants. Locations 500 and 800 are not included as these were trained by the varied participants. Table presents Mean absolute deviation from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n108.85(50.63)\n83.92(33.76)\n\n\n625\n204.75(84.66)\n183.51(75.92)\n\n\n675\n235.75(81.15)\n206.32(94.64)\n\n\n900\n323.5(130.9)\n289.62(110.07)\n\n\n\n\n\n\n\n\n\n\n\nExperiment 2 Discussion\nThe results of Experiment 2 largely conform to the findings of Experiment 1. Participants in both varied and constant conditions improved at the task during the training phase. We did not observe the common finding of training under varied conditions producing worse performance during acquisition than training under constant conditions (Catalano & Kleiner, 1984; Wrisberg et al., 1987), which has been suggested to relate to the subsequent benefits of varied training in retention and generalization testing (Soderstrom & Bjork, 2015). However our finding of no difference in training performance between constant and varied groups has been observed in previous work (Chua et al., 2019; Moxley, 1979; Pigott & Shapiro, 1984).\nIn the testing phase, our varied group significantly outperformed the constant conditions in both a general comparison, and in an analysis limited to novel throwing positions. The observed benefit of varied over constant training echoes the findings of many previous visuomotor skill learning studies that have continued to emerge since the introduction of Schmidt’s influential Schema Theory (Catalano & Kleiner, 1984; Chua et al., 2019; Goodwin et al., 1998; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976; Pigott & Shapiro, 1984; Roller et al., 2001; Schmidt, 1975; Willey & Liu, 2018b; Wrisberg et al., 1987; Wulf, 1991). We also join a much smaller set of research to observe this pattern in a computerized task (Seow et al., 2019). One departure from the experiment 1 findings concerns the pattern wherein the varied group outperformed the constant group even from the training position of the constant group, which was significant in experiment 1, but did not reach significance in experiment 2. Although this pattern has been observed elsewhere in the literature (Goode et al., 2008; Kerr & Booth, 1978), the overall evidence for this effect appears to be far weaker than for the more general benefit of varied training in conditions novel to all training groups.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#computational-model",
    "href": "Sections/full.html#computational-model",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Computational Model",
    "text": "Computational Model\nControlling for the similarity between training and testing. The primary goal of Experiment 2 was to examine whether the benefits of variability would persist after accounting for individual differences in the similarity between trained and tested throwing locations. To this end, we modelled each throw as a two-dimensional point in the space of x and y velocities applied to the projectile at the moment of release. For each participant, we took each individual training throw, and computed the similarity between that throw and the entire population of throws within the solution space for each of the 6 testing positions. We defined the solution space empirically as the set of all combinations of x and y throw velocities that resulted in hitting the target. We then summed each of the trial-level similarities to produce a single similarity for each testing position score relating how the participant threw the ball during training and the solutions that would result in target hits from each of the six testing positions – thus resulting in six separate similarity scores for each participant. Figure 8 visualizes the solution space for each location and illustrates how different combinations of x and y velocity result in successfully striking the target from different launching positions. As illustrated in Figure 8, the solution throws represent just a small fraction of the entire space of velocity combinations used by participants throughout the experiment.\n\n\nDisplay code\ntaskspace &lt;- e2 %&gt;% filter(AbsDistFromCenter&lt;900)\ntaskspace$hitOrMiss &lt;- ifelse(taskspace$trialType==11,\"Hit Target\",\"Missed Target\")\n\nsolSpace &lt;- e2 %&gt;% filter(trialType==11)\n#solSpace %&gt;% ggplot(aes(x=X_Velocity,y=Y_Velocity)) + geom_point(aes(colour=ThrowPosition),alpha=0.58) + ggtitle(\"\") \n\nsolSpace$Result = ifelse(solSpace$ThrowPosition==400,\"400\",solSpace$ThrowPosition)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==500,\"500\",solSpace$Result)\nsolSpace$Result= ifelse(solSpace$ThrowPosition==625,\"625\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==675,\"675\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==800,\"800\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==900,\"900\",solSpace$Result)\n\n\nmissSpace &lt;- e2 %&gt;% filter(trialType !=11)\nmissSpace$Result = \"Missed Target\"\nsolSpace$Result &lt;- solSpace$Result\n\n# the usual method of changing the legend title does not seem to work after the colours are manually scaled. \n# multiplied velocoties by -1 to make the axes less confusing\nss=solSpace %&gt;% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1)) + \n  geom_point(aes(colour=Result),alpha=0.6) + \n  scale_color_manual(values =brewer.pal(n=6,name=\"Set1\"))+\n  labs(colour=\"Target Hit Thrown from Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\")+ggtitle(\"A\")\n\nfullSpace &lt;- rbind(missSpace,solSpace)\n\nfs&lt;- fullSpace %&gt;% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1,colour=Result)) + \n  geom_point(aes(),alpha=0.6) + scale_color_manual(values =brewer.pal(n=7,name=\"Set1\"))+\n  labs(colour=\"Target Hit or Miss From Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\") +ggtitle(\"B\")\n\nlibrary(patchwork)\nss/fs\n\n\n\n\n\n\n\n\nFigure 8: A) A visual representation of the combinations of throw parameters (x and y velocities applied to the ball at launch), which resulted in target hits during the testing phase. This empirical solution space was compiled from all of the participants in Experiment 2. B) shows the solution space within the context of all of the throws made throughout the testing phase of the experiment.\n\n\n\n\n\nFor each individual trial, the Euclidean distance (Equation 1) was computed between the velocity components (x and y) of that trial and the velocity components of each individual solution throw for each of the 6 positions from which participants would be tested in the final phase of the study. The P parameter in Equation 1 is set equal to 2, reflecting a Gaussian similarity gradient. Then, as per an instance-based model of similarity (Logan, 2002; Nosofsky, 1992), these distances were multiplied by a sensitivity parameter, c, and then exponentiated to yield a similarity value. The parameter c controls the rate with which similarity-based generalization drops off as the Euclidean distance between two throws in x- and y-velocity space increases. If c has a large value, then even a small difference between two throws’ velocities greatly decreases the extent of generalization from one to the other. A small value for c produces broad generalization from one throw to another despite relatively large differences in their velocities. The similarity values for each training individual throw made by a given participant were then summed to yield a final similarity score, with a separate score computed for each of the 6 testing positions. The final similarity score is construable as index of how accurate the throws a participant made during the training phase would be for each of the testing positions.\nEquation 1: \\[ Similarity_{I,J} = \\sum_{i=I}\\sum_{j=J} (e^{-c^\\cdot d^{p}_{i,j}}) \\]\nEquation 2: \\[ d_{i,j} = \\sqrt{(x_{Train_i}-x_{Solution_j})^2 + (y_{Train_i}-y_{Solution_j})^2 } \\]\nA simple linear regression revealed that these similarity scores were significantly predictive of performance in the transfer stage, t =-15.88, p&lt;.01, \\(r^2\\)=.17, such that greater similarity between training throws and solution spaces for each of the test locations resulted in better performance. We then repeated the group comparisons above while including similarity as a covariate in the model. Comparing the varied and constant groups in testing performance from all testing positions yielded a significant effect of similarity, F(1, 205)=85.66, p&lt;.001, \\(\\eta^{2}_G\\) =.29, and also a significant effect of condition (varied vs. constant), F(1, 205)=6.03, p=.015, \\(\\eta^{2}_G\\) =.03. The group comparison limited to only novel locations for the varied group pit against trained location for the constant group resulted in a significant effect of similarity, F(1,148)=31.12, p&lt;.001, \\(\\eta^{2}_G\\) =.18 as well as for condition F(1,148)=11.55, p&lt;.001, \\(\\eta^{2}_G\\) =.07. For all comparisons, the pattern of results was consistent with the initial findings from Experiment 2, with the varied group still performing significantly better than the constant group.\n\nFitting model parameters separately by group\nTo directly control for similarity in Experiment 2, we developed a model-based measure of the similarity between training throws and testing conditions. This similarity measure was a significant predictor of testing performance, e.g., participants whose training throws were more similar to throws that resulted in target hits from the testing positions, tended to perform better during the testing phase. Importantly, the similarity measure did not explain away the group-level benefits of varied training, which remained significant in our linear model predicting testing performance after similarity was added to the model. However, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum & Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the c parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky & Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions.\nTo address this possibility, we compared the original instance-based model of similarity fit against a modified model which separately fits the generalization parameter, c, to varied and constant participants. To perform this parameter fitting, we used the optim function in R, and fit the model to find the c value(s) that maximized the correlation between similarity and testing performance.\nBoth models generate distinct similarity values between training and testing locations. Much like the analyses in Experiment 2, these similarity values are regressed against testing performance in models of the form shown below. As was the case previously, testing performance is defined as the mean absolute distance from the center of the target (with a separate score for each participant, from each position).\nLinear models 1 and 3 both show that similarity is a significant predictor of testing performance (p&lt;.01). Of greater interest is the difference between linear model 2, in which similarity is computed from a single c value fit from all participants (Similarity1c), with linear model 4, which fits the c parameter separately between groups (Similarity2c). In linear model 2, the effect of training group remains significant when controlling for Similarity1c (p&lt;.01), with the varied group still performing significantly better. However, in linear model 4 the addition of the Similarity2c predictor results in the effect of training group becoming nonsignificant (p=.40), suggesting that the effect of varied vs. constant training is accounted for by the Similarity2c predictor. Next, to further establish a difference between the models, we performed nested model comparisons using ANOVA, to see if the addition of the training group parameter led to a significant improvement in model performance. In the first comparison, ANOVA(Linear Model 1, Linear Model 2), the addition of the training group predictor significantly improved the performance of the model (F=22.07, p&lt;.01). However, in the second model comparison, ANOVA (Linear model 3, Linear Model 4) found no improvement in model performance with the addition of the training group predictor (F=1.61, p=.20).\nFinally, we sought to confirm that similarity values generated from the adjusted Similarity2c model had more predictive power than those generated from the original Similarity1c model. Using the BIC function in R, we compared BIC values between linear model 1 (BIC=14604.00) and linear model 3 (BIC = 14587.64). The lower BIC value of model 3 suggests a modest advantage for predicting performance using a similarity measure computed with two c values over similarity computed with a single c value. When fit with separate c values, the best fitting c parameters for the model consistently optimized such that the c value for the varied group (c=.00008) was smaller in magnitude than the c value for the constant group(c= .00011). Recall that similarity decreases as a Gaussian function of distance (equation 1 above), and a smaller value of c will result in a more gradual drop-off in similarity as the distance between training throws and testing solutions increases.\nIn summary, our modeling suggests that an instance-based model which assumes equivalent generalization gradients between constant and varied trained participants is unable to account for the extent of benefits of varied over constant training observed at testing. The evidence for this in the comparative model fits is that when a varied/constant dummy-coded variable for condition is explicitly added to the model, the variable adds a significant contribution to the prediction of test performance, with the variable condition yielding better performance than the constant conditions. However, if the instance-based generalization model is modified to assume that the training groups can differ in the steepness of their generalization gradient, by incorporating a separate generalization parameter for each group, then the instance-based model can account for our experimental results without explicitly taking training group into account. Henceforth this model will be referred to as the Instance-based Generalization with Adaptive Similarity (IGAS) model.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#project-1-general-discussion",
    "href": "Sections/full.html#project-1-general-discussion",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1 General Discussion",
    "text": "Project 1 General Discussion\nAcross two experiments, we found evidence in support of the benefits of variability hypothesis in a simple, computerized projectile throwing task. Generalization was observed in both constant and varied participants, in that both groups tended to perform better at novel positions in the testing phase than did participants who started with those positions in the training phase. However, varied trained participants consistently performed better than constant trained participants, in terms of both the testing phase in general, and in a comparison that only included untrained positions. We also found some evidence for the less commonly observed pattern wherein varied-trained participants outperform constant-trained participants even from conditions identical to the constant group training (Goode et al., 2008; Green et al., 1995; Kerr & Booth, 1978). In Experiment 1 varied participants performed significantly better on this identity comparison. In Experiment 2, the comparison was not significant initially, but became significant after controlling for the similarity measure that incorporates only a single value for the steepness of similarity-based generalization (c). Furthermore, we showed that the general pattern of results from Experiment 2 could be parsimoniously accommodated by an instance-based similarity model, but only with the assumption that constant and varied participants generalize their training experience to different degrees. Our results thus suggest that the benefits of variation cannot be explained by the varied-trained participants simply covering a broader range of the task space. Rather, the modeling suggests that varied participants also learn to adaptively tune their generalization function such that throwing locations generalize more broadly to one another than they do in the constant condition. A learning system could end up adopting a higher c value in the constant than variable training conditions by monitoring the trial-by-trial variability of the training items. The c parameter would be adapted downwards when adjacent training items are dissimilar to each other and adapted upwards when adjacent training items are the same. In this fashion, contextually appropriate c values could be empirically learned. This learning procedure would capture the insight that if a situation has a high amount variability, then the learner should be predisposed toward thinking that subsequent test items will also show considerable variability, in which case generalization gradients should be broad, as is achieved by low values for c.\nAlso of interest is whether the IGAS model can predict the pattern of results wherein the varied condition outperforms the constant condition even from the position on which the constant condition trained. Although our models were fit using all of the Experiment 2 training and testing data, not just that of the identity comparisons, in Figure 9 we demonstrate how a simplified version of the IGAS model could in principle produce such a pattern. In addition to the assumption of differential generalization between varied and constant conditions, our simplified model makes explicit an assumption that is incorporated into the full IGAS model – namely that even when being tested from a position identical to that which was trained, there are always some psychological contextual differences between training and testing throws, resulting in a non-zero dissimilarity.\n\n\nDisplay code\n# \n\np=2\nc&lt;- .000002\n\ntrainingTestingDifference=2000;\ncvaried=.00002\ncconstant=.0005\nsimdat &lt;- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %&gt;%\n                     mutate(c2=ifelse(condit==\"varied\",cvaried,cconstant),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genGaussDist=exp(-c*(trainingTestingDifference+abs((x-train.position)^p))),\n                            genGauss2=exp(-c2*(abs((x-train.position)^p))),\n                            genGaussDist2=exp(-c2*(trainingTestingDifference+abs((x-train.position)^p))),\n                            ) %&gt;% \n  group_by(x,condit) %&gt;%\n  summarise(genGauss=mean(genGauss),genGauss2=mean(genGauss2),genGaussDist=mean(genGaussDist),genGaussDist2=mean(genGaussDist2),.groups='keep')\n\n\n#plot(x,exp(c*(trainingTestingDifference+abs(x-800)))+exp(c*(trainingTestingDifference+abs(x-400)))\n\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs &lt;- list(geom_line(alpha=.7),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=455,y=1.05,label=\"Varied\",size=3.0),\n                  annotate(geom=\"text\",x=455,y=.97,label=\"Training\",size=3.0),\n                  annotate(geom=\"text\",x=662,y=1.05,label=\"Constant\",size=3.0),\n                  annotate(geom=\"text\",x=657,y=.97,label=\"Training\",size=3.0),\n                  annotate(geom=\"text\",x=855,y=1.05,label=\"Varied\",size=3.0),\n                  annotate(geom=\"text\",x=855,y=.97,label=\"Training\",size=3.0),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n                        legend.position=\"none\"))\n\nip1 &lt;- simdat  %&gt;% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")+ggtitle(\"Identical context, 1c\")\nip2 &lt;- simdat %&gt;%  ggplot(aes(x,y=genGauss2,group=condit,col=condit))+plotSpecs+ylab(\"\")+ggtitle(\"Identical context, 2c\")\nip3 &lt;- simdat  %&gt;% ggplot(aes(x,y=genGaussDist,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")+\n  ggtitle(\"Added distance due to context, 1c\")+theme(plot.margin = margin(0, 0, 0, 1))\nip4 &lt;- simdat %&gt;%  ggplot(aes(x,y=genGaussDist2,group=condit,col=condit))+plotSpecs+ylab(\"\")+\n  ggtitle(\"Added distance due to context, 2c\")+theme(plot.margin = margin(0, 0, 0, 1))\n# gridExtra::grid.arrange(ip1,ip2,ip3,ip4,ncol=2)\n\n#gtitle=\"Figure 9.\"\n#title = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))\n#p1 &lt;- plot_grid(title,NULL,ip1,ip2,ip3,ip4,ncol=2,rel_heights=c(.1,.8,.8))\np1 &lt;- (ip1+ip2) / (ip3+ip4)\n\np1\n\n\n\n\n\n\n\n\nFigure 9: A simple model depicting the necessity of both of two separately fit generalization parameters, c, and a positive distance between training and testing contexts, in order for an instance model to predict a pattern of varied training from stimuli 400 and 800 outperforming constant training from position 600 at a test position of 600. For the top left panel, in which the generalization model assumes a single c value (-.008) for both varied and constant conditions, and identical contexts across training and testing, the equation which generates the varied condition is - Amount of Generalization = \\(e^{(c\\cdot|x-800|)} + e^{(c\\cdot|x-400|)}\\), whereas the constant group generalization is generated from \\(2\\cdot e^{(c\\cdot|x-600|)}\\). For the top right panel, the c constants in the original equations are different for the 2 conditions, with \\(c=-.002\\) for the varied condition, and \\(c=-.008\\) for the constant condition. The bottom two panels are generated from identical equations to those immediately above, except for the addition of extra distance (100 units) to reflect the assumption of some change in context between training and testing conditions. Thus, the generalization model for the varied condition in the bottom-right panel is of the form - Amount of Generalization = \\(e^{(c_{varied}\\cdot|x-800|)}+e^{(c_{varied}\\cdot|x-400|)}\\) .\n\n\n\n\n\nAs mentioned above, the idea that learners flexibly adjust their generalization gradient based on prior experience does have precedent in the domains of category learning (Aha & Goldstone, 1992; Briscoe & Feldman, 2011; Hahn et al., 2005; Lamberts, 1994; Op de Beeck et al., 2008), and sensorimotor adaptation (Marongelli & Thoroughman, 2013; Taylor & Ivry, 2013; Thoroughman & Taylor, 2005). Lamberts (1994) showed that a simple manipulation of background knowledge during a categorization test resulted in participants generalizing their training experience more or less broadly, and moreover that such a pattern could be captured by allowing the generalization parameter of an instance-based similarity model to be fit separately between conditions. The flexible generalization parameter has also successfully accounted for generalization behavior in cases where participants have been trained on categories that differ in their relative variability (Hahn et al., 2005; Sakamoto et al., 2006). However, to the best of our knowledge, IGAS is the first instance-based similarity model that has been put forward to account for the effect of varied training in a visuomotor skill task. Although IGAS was inspired by work in the domain of category learning, its success in a distinct domain may not be surprising in light of the numerous prior observations that at least certain aspects of learning and generalization may operate under common principles across different tasks and domains (Censor et al., 2012; Hills et al., 2010; Jamieson et al., 2022; Law & Gold, 2010; Roark et al., 2021; Rosenbaum et al., 2001; Vigo et al., 2018; Wall et al., 2021; Wu et al., 2020; J. Yang et al., 2020).\nOur modelling approach does differ from category learning implementations of instance-based models in several ways. One such difference is the nature of the training instances that are assumed to be stored. In category learning studies, instances are represented as points in a multidimensional space of all of the attributes that define a category item (e.g., size/color/shape). Rather than defining instances in terms of what stimuli learners experience, our approach assumes that stored, motor instances reflect how they act, in terms of the velocity applied to the ball on each throw. An advantage of many motor learning tasks is the relative ease with which task execution variables can be directly measured (e.g., movement force, velocity, angle, posture) in addition to the decision and response time measures that typically exhaust the data generated from more classical cognitive tasks. Of course, whether learners actually are storing each individual motor instance is a fundamental question beyond the scope of the current work – though as described in the introduction there is some evidence in support of this idea (Chamberlin & Magill, 1992a; Crump & Logan, 2010; Hommel, 1998; Meigh et al., 2018; Poldrack et al., 1999). A particularly noteworthy instance-based model of sensory-motor behavior is the Knowledge II model of Rosenbaum and colleagues (R. G. Cohen & Rosenbaum, 2004; Rosenbaum et al., 1995). Knowledge II explicitly defines instances as postures (joint combinations), and is thus far more detailed than IGAS in regards to the contents of stored instances. Knowledge II also differs from IGAS in that learning is accounted for by both the retrieval of stored postures, and the generation of novel postures via the modification of retrieved postures. A promising avenue for future research would be to combine the adaptive similarity mechanism of IGAS with the novel instance generation mechanisms of Knowledge II.\nOur findings also have some conceptual overlap with an earlier study on the effects of varied training in a coincident timing task (Catalano & Kleiner, 1984). In this task, participants observe a series of lamps lighting up consecutively, and attempt to time a button press with the onset of the final lamp. The design consisted of four separate constant groups, each training from a single lighting velocity, and a single varied group training with all four of the lighting velocities used by the individual constant groups. Participants were then split into four separate testing conditions, each of which were tested from a single novel lighting velocity of varying distance from the training conditions. The result of primary interest was that all participants performed worse as the distance between training and testing velocity increased – a typical generalization decrement. However, varied participants showed less of a decrement than did constant participants. The authors take this result as evidence that varied training results in a less-steep generalization gradient than does constant training. Although the experimental conclusions of Catalano and Kleiner are similar to our own, our work is novel in that we account for our results with a cognitive model, and without assuming the formation of a schema. Additionally, the way in which Catalano and Kleiner collapse their separate constant groups together may result in similarity confounds between varied and constant conditions that leaves their study open to methodological criticisms, especially in light of related work which demonstrated that the extent to which varied training may be beneficial can depend on whether the constant group they are compared against trained from similar conditions to those later tested (Wrisberg et al., 1987). Our study alleviates such concerns by explicitly controlling for similarity.\n\nLimitations\nA limitation of this study concerns the ordering of the testing/transfer trials at the conclusion of both experiments. Participants were tested from each separate position (4 in Experiment 1, 6 in Experiment 2) in a random, intermixed order. Because the varied group was trained from two positions that were also randomly ordered, they may have benefited from experience with this type of sequencing, whereas the constant groups had no experience with switching between positions trial to trial. This concern is somewhat ameliorated by the fact that the testing phase performance of the constant groups from their trained position was not significantly worse than their level of performance at the end of the training phase, suggesting that they were not harmed by random ordering of positions during testing. It should also be noted that the computerized task utilized in the present work is relatively simple compared to many of the real-world tasks utilized in prior research. It is thus conceivable that the effect of variability in more complex tasks is distinct from the process put forward in the present work. An important challenge for future work will be to assess the extent to which IGAS can account for generalization in relatively complex tasks with far more degrees of freedom.\nIt is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g., the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.\n\n\nConclusion\nOur experiments demonstrate a reliable benefit of varied training in a simple projectile launching task. Such results were accounted for by an instance-based model that assumes that varied training results in the computation of a broader similarity-based generalization gradient. Instance-based models augmented with this assumption may be a valuable approach towards better understanding skill generalization and transfer.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#introduction-2",
    "href": "Sections/full.html#introduction-2",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Introduction",
    "text": "Introduction\nA longstanding issue across both science and instruction has been to understand how various aspects of an educational curriculum or training program influence learning acquisition and generalization. One such aspect, which has received a great deal of research attention, is the variability of examples experienced during training (Raviv et al., 2022). The influence of training variation has been studied in numerous domains, including category learning (A. L. Cohen et al., 2001; Posner & Keele, 1968), visuomotor learning (Berniker et al., 2014; Schmidt, 1975), language learning (Perry et al., 2010), and education (Braithwaite & Goldstone, 2015; Guo et al., 2014). The pattern of results is complex, with numerous studies finding both beneficial (Braun et al., 2009; Catalano & Kleiner, 1984; Gorman & Goldstone, 2022; Roller et al., 2001), as well as null or negative effects (Brekelmans et al., 2022; Hu & Nosofsky, 2024; Van Rossum, 1990). The present study seeks to contribute to the large body of existing research by examining the influence of variability in visuomotor function learning - a domain in which it has been relatively under-studied.\n\nFunction Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with an incomplete scatterplot or line graph and make predictions about regions of the plot that do not contain data (Ciccione & Dehaene, 2021; Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020). Studies of function learning often compare the difficulty of learning functions of different underlying forms (e.g. linear, bi-linear, power, sinusoidal), and the extent to which participants can accurately respond to novel inputs that fall in-between previously experienced inputs (interpolation testing), or that fall outside the range of previously experienced inputs (extrapolation).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a rule-based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974), testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out assuming a linear function, but given sufficient error will progressively test alternative hypotheses with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power law, as opposed to linear function. Koh and Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to accurately respond on interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performances are depicted in Figure 10.\nThe authors evaluated the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the under and over-estimation biases shown in the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance (see Figure 26). The EXAM model retains the same similarity-based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participants’ training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, by contrast, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate a sinusoidal function (Bott & Heit, 2004) or to scenarios where different functions apply to different regions of the input space (Kalish et al., 2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V-shaped input spaces (McDaniel et al., 2009).\n\n\n\n\n\n\n\n\nFigure 10: The generalization patterns of human particpiants observed in DeLosh et al. (1997) (reproduced from Figure 3 in their manuscript). Dots represent the average responses of human participants, and solid lines represent the true functions. The dashed vertical lines indicate the lower and upper bounds of the trained examples. Stimulii that fall within the dashed lines are interpolations of the training examples, while those that fall outside the dashed lines are extrapolations.\n\n\n\n\n\n\n\nVariability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.\n\n\nOverview Of Present Study\nThe present study investigates the influence of training variability on learning, generalization, and extrapolation in a uni-dimensional visuomotor function learning task. To the best of our knowledge, this research is the first to employ the classic constant vs. varied training manipulation, commonly used in the literature studying the benefits of variability, in the context of a uni-dimensional function learning task. Across three experiments, we compare constant and varied training conditions in terms of learning performance, extrapolation accuracy, and the ability to reliably discriminate between stimuli.\nTo account for the empirical results, we will apply a series of computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM). Notably, this study is the first to employ approximate Bayesian computation (ABC) to fit these models to individual subject data, enabling us to thoroughly investigate the full range of posterior predictions of each model, and to examine the ability of these influential models of function learning to account for both the group level and individual level data.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#experiment-1-1",
    "href": "Sections/full.html#experiment-1-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 1",
    "text": "Experiment 1\n\nMethods\nParticipants. A total of 183 participants were initially recruited from Indiana University Introductory Psychology Courses. Of these, 27 participants were excluded from further analysis due to meeting the exclusion criteria, resulting in a final sample of 156 participants. The exclusion criteria was defined as performance worse (i.e., larger deviations) than the condition average in either the training or testing stage of the experiment. The remaining participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in JavaScript, making use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at the target speed indicated at the top of the screen (see Figure 11). The target velocities were given as a range, or band, of acceptable velocity values (e.g., band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n\n\n\n\n\n\n\n\nFigure 11: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\n\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 12 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\n\n\n\n\n\n\n\n\n\ncluster\n\nTest Phase \n(Counterbalanced Order)\n\n\n\ndata1\n\n Varied Training \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1\n\nTest  \nNovel Bands \n100-300\n350-550\n600-800\n\n\n\ndata1-&gt;Test1\n\n\n\n\n\ndata2\n\n Constant Training \n800-1000\n\n\n\ndata2-&gt;Test1\n\n\n\n\n\nTest3\n\n    Final Test \n  Novel With Feedback  \n100-300\n350-550\n600-800\n\n\n\nTest2\n\n  Test \n  Varied Training Bands  \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1-&gt;Test2\n\n\n\n\n\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\nFigure 12: Experiment 1 Design. Constant and Varied participants complete different training conditions.\n\n\n\n\n\n\n\nDisplay code\n# pacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n#   brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n#   stringr, here,conflicted, patchwork, knitr)\n# #options(brms.backend=\"cmdstanr\",mc.cores=4)\n# options(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE1 &lt;- e1 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800)\ntrainE1_avg &lt;- trainE1_max |&gt; group_by(id,condit) |&gt; summarise(avg = mean(dist))\n\n\n\n\nAnalyses Strategy\nAll data processing and statistical analyses were performed in R version 4.32 (Team, 2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R (Bürkner, 2017), and descriptive stats and tables were extracted with the BayestestR package (Makowski et al., 2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoid convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\n\nTable 5: Statistical Model Specifications. The specifications for the Bayesian regression models used in the analyses of each of the 3 experiments. Comparisons of accuracy use absolute deviation as the dependent variable, while comparisons of discrimination use the raw velocities produced by participants as the dependent variable.\n\n\n\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\n\nEnd of Training Accuracy\nbrm(Abs. Deviation ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(Abs. Deviation ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was measured by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to have slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n\nResults\n\n\nDisplay code\np1 &lt;- trainE1 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e1_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 13: Experiment 1 - Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e1-train-dist\n##| tbl-cap: \"Experiment 1 - Learning curves. \"\n##| output: asis\n\nbmm_e1_train&lt;- trainE1_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e1_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\nmtr1 &lt;- as.data.frame(describe_posterior(bmm_e1_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# mtr1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#    kable(booktabs = TRUE)\n\ncdtr1 &lt;- get_coef_details(bmm_e1_train, \"conditVaried\")\n\n\n\n\n\nTable 6: Experiment 1 - End of training performance. Comparing final training block accuracy in the band common to both groups. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\n\nTraining. Figure 13 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation from the target band than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\n\nDisplay code\n##| label: tbl-e1-bmm-dist\n##| tbl-cap: \"E1. Training vs. Extrapolation\"\n#| \nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e1_dist_Cond_Type_RF_2\")\nbmtd &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE1, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted1 &lt;- as.data.frame(describe_posterior(bmtd, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n# \n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_id:bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n\n# mted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE)\ncdted1 &lt;- get_coef_details(bmtd, \"conditVaried\")\ncdted2 &lt;-get_coef_details(bmtd, \"bandTypeExtrapolation\")\ncdted3 &lt;-get_coef_details(bmtd, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\n\nTable 7: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation bands), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 7. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 14, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\n\nDisplay code\npe1td &lt;- testE1 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target Band\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Target Band\", x=\"Band Type\")\n\np2 &lt;- (pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 14: Experiment 1 Testing Accuracy. A) Empiricial Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e1-bmm-vx\n##| tbl-cap: \"Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e1_vxBMM) |&gt; kable(booktabs = TRUE)\n\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\n\n\nTable 8: Experiment 1 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condit and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 8 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, though this is not, in and of itself, useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The significant negative estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), indicates that the discrimination was modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition (see Figure 15 and Figure 16).\n\n\nDisplay code\ntestE1 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 15: Experiment 1. Empirical distribution of velocities produced in the testing stage. Translucent bands with dashed lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nDisplay code\npe1vce &lt;- e1_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE1$vb), \n                     limits = c(0, 1400)) + \n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e1_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e1_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e1_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e1_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e1Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\n\npid_den1 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes1 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n     theme(legend.title=element_blank(), \n           axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\n\np3 &lt;- (pe1vce + pid_den1 + pid_slopes1) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\n\n\nFigure 16: Experiment 1 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination between target bands. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\n\n\n\nExperiment 1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability to learn and extrapolate in a visuomotor task. Our findings that training with variable conditions resulted in lower final training performance are consistent with much of the prior research on the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and are particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#experiment-2-1",
    "href": "Sections/full.html#experiment-2-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 2",
    "text": "Experiment 2\n\n\nDisplay code\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne2 &lt;- readRDS(here(\"data/e2_08-04-23.rds\")) \ne2Sbjs &lt;- e2 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE2 &lt;- e2 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE2 &lt;-  e2 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE2_max &lt;- trainE2 |&gt; filter(Trial_Bin == nbins, bandInt==600)\n\n# e2 |&gt; group_by(condit, bandOrder) |&gt; summarise(n_distinct(id))\n\n\n\nMethods & Procedure\nInitially, 131 participants were recruited. After applying the same exclusion procedure as in Experiment 1, 21 participants were excluded, resulting in a final sample of 110 participants who completed the experiment (Varied: 55, Constant: 55). The task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 17). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands.\n\n\n\n\n\n\n\n\n\n\ncluster\n\nTest Phase \n(Counterbalanced Order)\n\n\n\ndata1\n\n Varied Training \n100-300\n350-550\n600-800\n\n\n\nTest1\n\nTest  \nNovel Bands  \n800-1000\n1000-1200\n1200-1400\n\n\n\ndata1-&gt;Test1\n\n\n\n\n\ndata2\n\n Constant Training \n600-800\n\n\n\ndata2-&gt;Test1\n\n\n\n\n\nTest3\n\n    Final Test \n  Novel With Feedback  \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest2\n\n  Test \n  Varied Training Bands  \n100-300\n350-550\n600-800\n\n\n\nTest1-&gt;Test2\n\n\n\n\n\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\nFigure 17: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.\n\n\n\n\n\n\n\nResults\n\n\nDisplay code\np1 &lt;- trainE2 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e2_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 18: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\n\nDisplay code\nbmm_e2_train &lt;- trainE2_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e2_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\nmtr2 &lt;- as.data.frame(describe_posterior(bmm_e2_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\ncdtr2 &lt;- get_coef_details(bmm_e2_train, \"conditVaried\")\n# mtr2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\n\n\n\n\nTable 9: Experiment 2 - End of training performance. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\n\nTraining. Figure 18 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\n\nDisplay code\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\ncd2ted1 &lt;- get_coef_details(bmtd2, \"conditVaried\")\ncd2ted2 &lt;-get_coef_details(bmtd2, \"bandTypeExtrapolation\")\ncd2ted3 &lt;-get_coef_details(bmtd2, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\n\nTable 10: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproportionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \n\nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 10, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 19).\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\npe2td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\npe2ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\np2 &lt;- (pe2td + pe2ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 19: Experiment 2 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e2-bmm-vx\n##| tbl-cap: \"Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc2 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\nTable 11: Experiment 2 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condition and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate)\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 11. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 20 and Figure 21).\n\n\nDisplay code\ntestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 20: Experiment 2. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n}\n\npe2vce &lt;- e2_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e2_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e2_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e2_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e2_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e2Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\npid_den2 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes2 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n      theme(legend.title=element_blank(), \n        axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe2vce + pid_den2 + pid_slopes2) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\n\n\nFigure 21: Experiment 2 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\n\n\n\nExperiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied and Constant groups did not show a significant difference in their discrimination between bands.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#experiment-3",
    "href": "Sections/full.html#experiment-3",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 3",
    "text": "Experiment 3\n\n\nDisplay code\ne3 &lt;- readRDS(here(\"data/e3_08-04-23.rds\")) |&gt; \n    mutate(trainCon=case_when(\n    bandOrder==\"Original\" ~ \"800\",\n    bandOrder==\"Reverse\" ~ \"600\",\n    TRUE ~ NA_character_\n    ), trainCon=as.numeric(trainCon)) \ne3Sbjs &lt;- e3 |&gt; group_by(id,condit,bandOrder) |&gt; summarise(n=n())\ntestE3 &lt;- e3 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE3 &lt;-  e3 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit,bandOrder, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE3_max &lt;- trainE3 |&gt; filter(Trial_Bin == nbins, bandInt==trainCon)\n\n\n\nMethods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. 227 participants were recruited initially. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.\n\n\nResults\n\n\nDisplay code\nbmm_e3_train &lt;- trainE3_max %&gt;% \n  brm(dist ~ condit*bandOrder, \n      file=here(\"data/model_cache/e3_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\n# mtr3 &lt;- as.data.frame(describe_posterior(bmm_e3_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mtr3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mtr3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\ncd3tr1 &lt;- get_coef_details(bmm_e3_train, \"conditVaried\")\ncd3tr2 &lt;-get_coef_details(bmm_e3_train, \"bandOrderReverse\")\ncd3tr3 &lt;-get_coef_details(bmm_e3_train, \"conditVaried:bandOrderReverse\")\n\n\n\n\n\nTable 12: Experiment 3 - End of training performance. The Intercept represents the average of the baseline condition (constant training & original band order), the conditVaried coefficient reflects the difference between the constant and varied groups, and the bandOrderReverse coefficient reflects the difference between original and reverse order. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. The negative value for the interaction between condit and bandOrder indicates that varied condition with reverse order had significantly lower deviations than the varied condition with the original band order\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 22 displays the average deviations from the target band across training blocks, and Table 12 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\n\nDisplay code\np1 &lt;- trainE3 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_nested_wrap(~bandOrder*vb,ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e3_train_deviation.png\"), p1, width = 9, height = 8,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 22: Experiment 3 training. Deviations from target band during training, shown separately for groups trained with the original order (used in E1) and reverse order (used in E2).\n\n\n\n\n\n\n\nDisplay code\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e3_dist_Cond_Type_RF_2\")\nbmtd3 &lt;- brm(dist ~ condit * bandType*bandOrder + (1|bandInt) + (1|id), \n    data=testE3, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted3 &lt;- as.data.frame(describe_posterior(bmtd3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\n#ce_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\n#wrap_plots(ce_bmtd3)\n\n#ggsave(here::here(\"Assets/figs\", \"e3_cond_effects_dist.png\"), wrap_plots(ce_bmtd3), width=11, height=11, bg=\"white\")\n\ncd3ted1 &lt;- get_coef_details(bmtd3, \"conditVaried\")\ncd3ted2 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation\")\ncd3ted3 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation\")\ncd3ted4 &lt;-get_coef_details(bmtd3, \"bandOrderReverse\")\ncd3ted5 &lt;-get_coef_details(bmtd3, \"conditVaried:bandOrderReverse\")\ncd3ted6 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation:bandOrderReverse\")\ncd3ted7 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation:bandOrderReverse\")\n\n\n\n\n\nTable 13: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training, trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrder\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 13 presents the results of the Bayesian mixed effects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3td &lt;- testE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~bandOrder,ncol=1) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\npe3ce &lt;- bmtd3 |&gt; emmeans( ~condit *bandOrder*bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\") + \n facet_wrap(~bandOrder,ncol=1)\n\np2 &lt;- pe3td + pe3ce + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e3_test-dev.png\"), p2, width=9, height=8, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 23: Experiment 3 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Shown separately for groups trained with the original order (used in E1) and reverse order (used in E2). Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e3-bmm-vx\n##| tbl-cap: \"Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# m1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\n# m2 &lt;- fixef(e3_vxBMM)\n# mp3 &lt;- m1[, c(1,2,4,5,6)]\n# colnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")                       \n# mp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T)\n\n#wrap_plots(plot(conditional_effects(e3_vxBMM),points=FALSE,plot=FALSE))\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\nintCoef2 &lt;- get_coef_details(e3_vxBMM, \"bandOrderReverse:bandInt\")\ncoef3 &lt;- get_coef_details(e3_vxBMM,\"conditVaried:bandOrderReverse:bandInt\")\n\n\n\n\n\nTable 14: Experiment 3 testing discrimination. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the slope for the baseline condition. The interaction terms which include condit and Band (e.g., conditVaried:Band & conditVaried:bandOrderReverse:band) respectively indicate how the slopes of the varied-original condition differed from the baseline condition, and how varied-reverse condition differed from the varied-original condition\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:band\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:band\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 13. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope coefficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating a greater slope for the varied condition trained with reverse order bands. This interaction is shown in Figure 24, where the steepness of the best fitting line for the varied-reversed condition is noticeably steeper than the other conditions.\n\n\nDisplay code\n##| column: screen-inset-right\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n   ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\n\n\n\n\n\n\nFigure 24: Experiment 3. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\n\nDisplay code\n# pe3tv &lt;- testE3 %&gt;% group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3vce &lt;- e3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  facet_wrap(~bandOrder,ncol=1) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e3_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e3_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction1 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandInt\"]\nfixed_effect_interaction2 &lt;- fixef(e3_vxBMM)[,1][\"bandOrderReverse:bandInt\"]\nfixed_effect_interaction3 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandOrderReverse:bandInt\"]\n\nre &lt;- data.frame(ranef(e3_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e3Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction1*(condit==\"Varied\") + \n           fixed_effect_interaction2*(bandOrder==\"Reverse\") + \n           fixed_effect_interaction3*(condit==\"Varied\" & bandOrder==\"Reverse\"),\n  slope = Estimate + adjust )\n\npid_den3 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\") +\n  facet_wrap(~bandOrder,ncol=1)\n\npid_slopes3 &lt;- re |&gt;  \n    mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n    geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n    theme(legend.title=element_blank(), \n      axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_nested_wrap(bandOrder~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe3vce + pid_den3 + pid_slopes3) + plot_annotation(tag_levels= 'A')\n\n#ggsave(here::here(\"Assets/figs\", \"e3_test-vx.png\"), p3,width=11,height=13, bg=\"white\",dpi=800)\np3\n\n\n\n\n\n\n\n\nFigure 25: Experiment 3 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.\n\n\n\n\n\n\n\nExperiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received only ordinal, not continuous valued, feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#computational-model-1",
    "href": "Sections/full.html#computational-model-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Computational Model",
    "text": "Computational Model\n\n\nDisplay code\n####| cache: false\n# pacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n#                stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n#                ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\n# #walk(c(\"dplyr\"), conflict_prefer_all, quiet = TRUE)\n# #options(brms.backend=\"cmdstanr\",mc.cores=4)\n# options(digits=3, scipen=999, dplyr.summarise.inform=FALSE)\n# walk(c(\"Display_Functions\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\", \"prep_model_data\",\"org_functions\"), ~source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\n\n\n\nDisplay code\n###| cache: true\ninvisible(list2env(load_sbj_data(), envir = .GlobalEnv))\ninvisible(list2env(load_e1(), envir = .GlobalEnv))\ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ne2_model &lt;- load_e2()\ne3_model &lt;- load_e3()\noptions(contrasts = initial_contrasts)\n\n\n\n\nDisplay code\nalm_plot()\n\n\n\n\n\n\n\n\nFigure 26: The Associative Learning Model (ALM). The diagram illustrates the basic structure of the ALM model used in the present work. Input nodes are activated as a function of their similarity to the lower-boundary of the target band. The generalization parameter, \\(c\\), determines the degree to which nearby input nodes are activated. The output nodes are activated as a function of the weighted sum of the input nodes. During training, when feedback is provided, network weights connecting the input layer to the output layer are updated via the delta rule.\n\n\n\n\n\nThe modeling goal is to implement a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. For this purpose, we will apply the associative learning model (ALM) and the EXAM model of function learning (DeLosh et al., 1997). ALM is a simple connectionist learning model which closely resembles Kruschke’s ALCOVE model (Kruschke, 1992), with modifications to allow for the generation of continuous responses.\n\nALM & Exam\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus ( a_i(X) = exp(-c(X - X_i)^2) ). So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g., 54 and 56; 53 and 57) may also activate to some degree. The units in the input layer activate as a function of their similarity to a presented stimulus. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nThe EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. When a novel test stimulus, \\(X\\), is presented, EXAM first identifies the two nearest training stimuli, \\(X_1\\) and \\(X_2\\), that bracket \\(X\\). This is done based on the Gaussian activation of input nodes, similar to ALM, but focuses on identifying the closest known points for extrapolation.\nSlope Calculation: EXAM calculates a local slope, \\(S\\), using the responses associated with \\(X_1\\) and \\(X_2\\). This is computed as:\n\\[\n   S = \\frac{m(X_{1}) - m(X_{2})}{X_{1} - X_{2}}\n   \\]\nwhere \\(m(X_1)\\) and \\(m(X_2)\\) are the output values from ALM corresponding to the \\(X_1\\) and \\(X_2\\) inputs.\nResponse Generation: The response for the novel stimulus \\(X\\) is then extrapolated using the slope \\(S\\):\n\\[\n   E[Y|X] = m(X_1) + S \\cdot |X - X_1|\n   \\]\nHere, \\(m(X_1)\\) is the ALM response value from the training data for the stimulus closest to \\(X\\), and \\((X - X_1)\\) represents the distance between the novel stimulus and the nearest training stimulus.\nAlthough this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix, 2017), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 15 for a full specification of the equations that define ALM and EXAM, and Figure 26 for a visual representation of the ALM model.\n\n\n\n\nTable 15: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\nSlope Computation\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).\n\n\n\n\n\n\n\n\n\nModel Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to McDaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data with three separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase. Models were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate Bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate Bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g., many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until the desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individual’s velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.\n\n\nModelling Results\n\n\nDisplay code\npost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; mutate(Fit_Method=rename_fm(Fit_Method)) \n\nif (primary) {\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\n} else {\n\n  e1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;% kable(escape=F,booktabs=T)\n\n  }\n\n\n\n\nTable 16: Model errors predicting empirical data from Experiment 1 - aggregated over the full posterior distribution for each participant. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n  \n    \n      Task Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n    \n      Constant\n      Varied\n      Constant\n      Varied\n    \n  \n  \n    Test\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n    Test\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n    Test\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n    Train\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n    Train\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n    Train\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n  \n  \n  \n\n\n\n\n\n\n\n\n\nDisplay code\nc_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\n\n\nFigure 27: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y-axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\n\n\nDisplay code\ntrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,bandType) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\n\n\nFigure 28: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y-axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 27, and model predictions are shown alongside the empirical data in Figure 29. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 28. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 28) show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that Y-axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 30), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 29, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\n\nDisplay code\npost_dat_l |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot(aes(x=Resp,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~rename_fm(Fit_Method)+condit, ncol=2,strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(10,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Vx\")\n\n\n\n\n\n\n\n\nFigure 29: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\n\n\nDisplay code\n###| eval: false\n\npacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n               ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n#p_ce_1 &lt;- (p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n# plot_custom_effects &lt;- function(model) {\n#   # Extract posterior samples for fixed effects\n#   post_samples &lt;- posterior_samples(model, pars = c(\"b_Intercept\", \"b_ModelEXAM\", \"b_conditVaried\", \"b_ModelEXAM:conditVaried\"))\n  \n#   # Calculate conditional effects\n#   post_samples &lt;- post_samples %&gt;%\n#     mutate(\n#       ALM_Constant = b_Intercept,\n#       EXAM_Constant = b_Intercept + b_ModelEXAM,\n#       ALM_Varied = b_Intercept + b_conditVaried,\n#       EXAM_Varied = b_Intercept + b_ModelEXAM + b_conditVaried + `b_ModelEXAM:conditVaried`\n#     )\n  \n#   # Reshape data for plotting\n#   plot_data &lt;- post_samples %&gt;%\n#     select(ALM_Constant, EXAM_Constant, ALM_Varied, EXAM_Varied) %&gt;%\n#     pivot_longer(everything(), names_to = \"Condition\", values_to = \"Estimate\") %&gt;%\n#     separate(Condition, into = c(\"Model\", \"Condit\"), sep = \"_\")\n  \n#   # Plot conditional effects\n#   ggplot(plot_data, aes(x = Model, y = Estimate, color = Condit)) +\n#     geom_boxplot() +\n#     theme_minimal() +\n#     labs(x = \"Model\", y = \"Estimate\", color = \"Condition\")\n# }\n# p_ce_1 &lt;- plot_custom_effects(e1_ee_brm_ae)\n\n\n\n\nbm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n# full set of Model x condit contrasts\n# ALM - EXAM\n# btw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n#   pluck(\"contrasts\") |&gt; \n#   gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n# Constant - Varied\n# emm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\n# btw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# # btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\np_em_1 &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye() + labs(x=\"Model Error Difference\",y=\"Contrast\") + theme(legend.position=\"none\") \n\n#p_ce_1 / p_em_1\n\n(p1 + p2+ p3) /p_em_1 + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\n\n\nFigure 30: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g., EXAM Constant) tended to have smaller model residuals.\n\n\n\n\n\nTo quantitatively assess the differences in performance between models, we fit a Bayesian regression model predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.88 (95% CrI [-46.02, -0.97], pd = 0.98).\n\n\nDisplay code\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\npost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt;\n mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; \n mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \n\nif (primary) {\ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\ngt_table\n} else {\n  e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  kable(escape=F,booktabs=T)\n}\n\n\n\n\nTable 17: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n  \n    \n      \n      \n        E2\n      \n      \n        E3\n      \n    \n    \n      Task Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n    \n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n  \n  \n    \n      Fit to Test Data\n    \n    Test\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n    Train\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n    \n      Fit to Test & Training Data\n    \n    Test\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n    Train\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n    \n      Fit to Training Data\n    \n    Test\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n    Train\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n  \n  \n  \n\n\n\n\n\n\n\n\n\nDisplay code\nrbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\n\n\n\n\n\nFigure 31: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\n\n\n\nDisplay code\ne2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nbct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) # %&gt;% kable(booktabs = TRUE)\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e3 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\nbm4_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried:bandOrderReverse\")\n\n\nbct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\nbct &lt;- rbind(bct_e1 |&gt; mutate(exp=\"Exp 1\"),bct_e2 |&gt; \n               mutate(exp= \"Exp 2\"),bct_e3 |&gt; mutate(exp=\"Exp 3\")) |&gt; \n  relocate(exp, .before=Term)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\nif (primary) {\nbct_table &lt;- bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;%\n  gt() %&gt;%\n  # tab_header(\n  #   title = \"Bayesian Model Results\",\n  #   subtitle = \"Estimates and Credible Intervals for Each Term Across Experiments\"\n  # ) %&gt;%\n  cols_label(\n    exp = \"Experiment\",\n    Term = \"Term\",\n    Estimate = \"Estimate\",\n    `95% CrI Lower` = \"95% CrI Lower\",\n    `95% CrI Upper` = \"95% CrI Upper\",\n    pd = \"pd\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Estimate, `95% CrI Lower`, `95% CrI Upper`),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = pd,\n    decimals = 2\n  ) %&gt;%\n  tab_spanner(\n    label = \"Credible Interval\",\n    columns = c(`95% CrI Lower`, `95% CrI Upper`)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      #cell_fill(color = \"lightgray\"),\n      cell_text(weight = \"bold\"), \n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = c(Estimate, pd),\n      rows = Term==\"ModelEXAM:conditVaried\"\n    )\n  ) %&gt;%\n   tab_row_group(\n    label = \"Experiment 3\",\n    rows = exp == \"Exp 3\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 2\",\n    rows = exp == \"Exp 2\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 1\",\n    rows = exp == \"Exp 1\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n    #row_group.background.color = \"gray95\"\n  )\nbct_table\n} else {\n  bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;% kable(booktabs = TRUE)\n\n}\n\n\n\n\nTable 18: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimated coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n  \n    \n      Experiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n    \n      95% CrI Lower\n      95% CrI Upper\n    \n  \n  \n    \n      Experiment 1\n    \n    Exp 1\nIntercept\n176.3\n156.9\n194.6\n1.00\n    Exp 1\nModelEXAM\n−88.4\n−104.5\n−71.8\n1.00\n    Exp 1\nconditVaried\n−37.5\n−60.4\n−14.2\n1.00\n    Exp 1\nModelEXAM:conditVaried\n60.4\n36.2\n83.8\n1.00\n    \n      Experiment 2\n    \n    Exp 2\nIntercept\n245.9\n226.2\n264.5\n1.00\n    Exp 2\nModelEXAM\n−137.7\n−160.2\n−115.5\n1.00\n    Exp 2\nconditVaried\n−86.4\n−113.5\n−59.3\n1.00\n    Exp 2\nModelEXAM:conditVaried\n56.9\n25.3\n88.0\n1.00\n    \n      Experiment 3\n    \n    Exp 3\nIntercept\n164.8\n140.1\n189.4\n1.00\n    Exp 3\nModelEXAM\n−65.7\n−86.0\n−46.0\n1.00\n    Exp 3\nconditVaried\n−40.6\n−75.9\n−3.0\n0.98\n    Exp 3\nbandOrderReverse\n25.5\n−9.3\n58.7\n0.93\n    Exp 3\nModelEXAM:conditVaried\n41.9\n11.2\n72.5\n0.99\n    Exp 3\nModelEXAM:bandOrderReverse\n−7.3\n−34.5\n21.1\n0.70\n    Exp 3\nconditVaried:bandOrderReverse\n30.8\n−19.6\n83.6\n0.88\n    Exp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.6\n−101.8\n−18.7\n1.00\n  \n  \n  \n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment 1. For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 18) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controlled for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 32).\n\n\nDisplay code\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n  p_e2 &lt;- (p1 + p2+ p3) \n# #wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\", fill=NULL, col=NULL) + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\np1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\n\n\nFigure 32: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiments 2 and 3 data. Experiment 3 also includes a condition for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\n\nComputational Model Summary. Across all three experiments, the model fits consistently favored the Extrapolation-Association Model (EXAM) over the Associative Learning Model (ALM). This preference for EXAM was particularly pronounced for participants in the constant training conditions (note the positive coefficients on ModelEXAM:conditVaried interaction terms Table 18). This pattern is clearly illustrated in Figure 33, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly larger for the constant group.\nThe superior performance of EXAM, especially for the constant training groups, may initially seem counterintuitive. One might assume that exposure to multiple, varied examples would be necessary to extract an abstract rule. However, EXAM is not a conventional rule-based model; it does not require the explicit abstraction of a rule. Instead, rule-based responses emerge during the retrieval process. The constant groups’ formation of a single, accurate input-output association, combined with the usefulness of the zero point, seem to have been sufficient for EXAM to capture their performance. A potential concern is that the assumption of participants utilizing the zero point essentially transforms the extrapolation problem into an interpolation problem. However, this concern is mitigated by the consistency of the results across both the original and reversed order conditions (the testing extrapolation bands fall in between the constant training band and the 0 point in experiment 1, but not in experiment 2).\nThe fits to the individual participants also reveal a number of interesting cases where the models struggle to capture the data (Figure 34). For example participant 68 exhibits a strong non-monotonicity in the highest velocity band, a pattern which ALM can mimic, but which EXAM cannot capture, given that it enforces a simple linear relationship between target velocity and response. Participant 70 (lower right corner of Figure 34) had a roughly parabolic response pattern in their observed data, a pattern which neither model can properly reproduce, but which causes EXAM to perform particularly poorly.\nModeling Limitations. The present work compared models based on their ability to predict the observed data, without employing conventional model fit indices such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). These indices, which penalize models based on their number of free parameters, would have been of limited utility in the current case, as both ALM and EXAM have two free parameters. However, despite having the same number of free parameters, EXAM could still be considered the more complex model, as it incorporates all the components of ALM plus an additional mechanism for rule-based responding. A more comprehensive model comparison approach might involve performing cross-validation with a held-out subset of the data (Mezzadri et al., 2022) or penalizing models based on the range of patterns they can produce (Dome & Wills, 2023), under the assumption that more constrained models are more impressive when they do adequately fit a given pattern of results.\n\n\n\nDisplay code\ntid1 &lt;- post_dat  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-pred_dist, -dist) |&gt;\n  rbind(e2_model$post_dat |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat |&gt; mutate(Exp=\"E3\")) |&gt;\n  filter(Fit_Method==\"Test_Train\") |&gt;\n  group_by(id,condit,Model,Fit_Method,x, Exp) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n    group_by(id,condit,Model,Fit_Method,Exp) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    arrange(id,condit,Fit_Method) |&gt;\n    round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; \n  mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method, Exp) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; \n  left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  #ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  ggh4x::facet_nested_wrap(~condit+Exp,scales=\"free\") + \n  theme(axis.text.y = element_text(size=8)) +\n  labs(fill=\"Best Model\",\n  x=\"Mean Model Error Difference (ALM - EXAM)\",\n  y=\"Participant\")\n\n\n\n\n\n\n\n\nFigure 33: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\n\n\nDisplay code\ncId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 68, 93, 74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ne1_sbjs &lt;- c(49,68,155, 175,74)\ne3_sbjs &lt;-  c(245, 280, 249)\ne2_sbjs &lt;- c(197, 157, 312, 334)\ncFinal &lt;- c(49, 128,202 )\nvFinal &lt;- c(68,70,245)\n\n\nindv_post_l &lt;- post_dat_l  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-signed_dist) |&gt;\n  rbind(e2_model$post_dat_l |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat_l |&gt; mutate(Exp=\"E3\") |&gt; select(-fb)) |&gt;\n  filter(Fit_Method==\"Test_Train\", id %in% c(cFinal,vFinal))\n\ntestIndv &lt;- indv_post_l |&gt; \n#filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar_sd + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\n\n\nFigure 34: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#project-2-discussion",
    "href": "Sections/full.html#project-2-discussion",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 Discussion",
    "text": "Project 2 Discussion\nAcross three experiments, we investigated the impact of training variability on learning and extrapolation in a visuomotor function learning task.\nIn Experiment 1, participants in the varied training condition, who experienced a wider range of velocity bands during training, showed lower accuracy at the end of training compared to those in the constant training condition. Crucially, during the testing phase, the varied group exhibited significantly larger deviations from the target velocity bands, particularly for the extrapolation bands that were not encountered during training. The varied group also showed less discrimination between velocity bands, as evidenced by shallower slopes when predicting response velocity from target velocity band.\nExperiment 2 extended these findings by reversing the order of the training and testing bands. Similar to Experiment 1, the varied group demonstrated poorer performance during both training and testing phases. However, unlike Experiment 1, the varied group did not show a significant difference in discrimination between bands compared to the constant group.\nIn Experiment 3, we provided only ordinal feedback during training, in contrast to the continuous feedback provided in the previous experiments. Participants were assigned to both an order condition (original or reverse) and a training condition (constant or varied). The varied condition showed larger deviations at the end of training, consistent with the previous experiments. Interestingly, there was a significant interaction between training condition and band order, with the varied condition showing greater accuracy in the reverse order condition. During testing, the varied group once again exhibited larger deviations, particularly for the extrapolation bands. The reverse order conditions showed smaller deviations compared to the original order conditions. Discrimination between velocity bands was poorer for the varied group in the original order condition, but not in the reverse order condition.\nAll three of our experiments yielded evidence that varied training conditions produced less learning by the end of training, a pattern consistent with much of the previous research on the influence of training variability (Catalano & Kleiner, 1984; Soderstrom & Bjork, 2015; Wrisberg et al., 1987). The sole exception to this pattern was the reverse order condition in Experiment 3, where the varied group was not significantly worse than the constant group. Neither the varied condition trained with the same reverse-order items in Experiment 2, nor the original-order varied condition trained with ordinal feedback in Experiment 3 were able to match the performance of their complementary constant groups by the end of training, suggesting that the relative success of the ordinal-reverse ordered varied group cannot be attributed to item or feedback effects alone.\nOur findings also diverge from the two previous studies that cleanly manipulated the variability of training items in a function learning task (DeLosh et al., 1997; van Dam & Ernst, 2015), although the varied training condition of van Dam & Ernst (2015) also exhibited less learning, neither of these previous studies observed any difference between training conditions in extrapolation to novel items. Like DeLosh et al. (1997) , our participants exhibited above chance extrapolation/discrimination of novel items, however they observed no difference between any of their three training conditions. A noteworthy difference difference between our studies is that DeLosh et al. (1997) trained participants with either 8, 20, or 50 unique items (all receiving the same total number of training trials). These larger sets of unique items, combined with the fact that participants achieved near ceiling level performance by the end of training, may have made it more difficult to observe any between-group differences of training variation in their study. van Dam & Ernst (2015) ’s variability manipulation was more similar to our own, as they trained participants with either 2 or 5 unique items. However, although the mapping between their input stimuli and motor responses was technically linear, the input dimension was more complex than our own, as it was defined by the degree of “spikiness” of the input shape. This entirely arbitrary mapping also would have precluded any sensible “0” point, which may partially explain why neither of their training conditions were able to extrapolate linearly in the manner observed in the current study or in DeLosh et al. (1997).\nLimitations\nWhile the present study provides valuable insights into the influence of training variability on visuomotor function learning and extrapolation, there are several limitations that should be flagged. First, although the constant training group never had experience from a velocity band closer to the extrapolation bands than the varied group, they always had a three times more trials with the nearest velocity band. Such a difference may be an unavoidable consequence of a varied vs. constant design which matches the total number of training trials between the two groups. However, in order to more carefully tease apart the influence of variability from the influence of frequency/repetition effects, future research could explore alternative designs that maintain the variability manipulation while equating the amount of training on the nearest examples across conditions, such as by increasing the total number of trials for the varied group. Another limitation is that the testing stage did not include any interpolation items, i.e. the participants were tested only from the training bands they experienced during training, or from extrapolation bands. The absence of interpolation testing makes it more difficult to distinguish between the effects of training variability on extrapolation specifically, as opposed to generalization more broadly. Of course, the nature of the constant training condition makes interpolation testing impossible to implement, however future studies might compare training regimes that each include at least 2 distinct items, but still differ in the total amount of variability experienced, which would then allow groups to be compared in terms of both interpolation and extrapolation testing. Finally, the task employed in the present study consisted of only a linear, positive function. Previous work in human function learning has repeatedly shown that such functions are among the easiest to learn, but that humans are nonetheless capable of learning negative, non-linear, or discontinuous functions (Busemeyer et al., 1997; DeLosh et al., 1997; Kalish, 2013; McDaniel et al., 2009). It thus remains an open question as to whether the influence of training variability might interact with various components of the to-be-learned function.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#empirical-and-modeling-summary",
    "href": "Sections/full.html#empirical-and-modeling-summary",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Empirical and Modeling Summary",
    "text": "Empirical and Modeling Summary\nAcross both projects, we investigated the influence of training variability on learning and generalization in computerized visuomotor skill learning, and function learning tasks. In project 1 (HTT), experiments 1 and 2 demonstrated that varied training led to superior testing performance compared to constant training. In Experiment 1, the varied group even outperformed the constant group even when testing from the constant groups trained position. In contrast, Project 2 (HTW) found the opposite pattern - the varied training groups exhibited poorer performance than the constant groups, both in terms of training accuracy, accuracy in extrapolation testing, and, in a subset of the experiments, the varied group showed a diminished ability to discriminate between bands. This detrimental effect of variability was observed across three experiments, with the exception of the reverse order condition in Experiment 3, where the varied group was able to match the constant group’s performance.\nBoth projects also included computational modeling componenents. In Project 1, the IGAS model was introduced as a means of addressing the lack of control for similarity between training and testing conditions common to previous work in the “benefits of variability” literature. The IGAS model provides a theoretically motivated method of quantifying the similarity between training experience and testing conditions. The resulting similarity metric (i.e. our 1c-similarity) is shown to be a significant predictor of testing performance on its own, and when added as a covariate to the statistical model used to compare the constant and varied training groups. We then showed the group-level effect of training variability on testing performance can be accounted for with the additional assumption that training variability influences the generalization gradient. The contribution of the IGAS model was thus twofold:  1) providing a theoretically justifiable method of quantifying/controlling for similarity between training and testing, and 2) demonstrating the viability of a flexible-similarity based generalization account for the empirically observed benefit of variability in our task. Although similar approaches have been employed in other domains, both contributions are novel additions to the large body of research assessing the effect of constant vs. varied training manipulations in visuomotor skill tasks.\nAlthough theoretically motivated, the IGAS model of Project 1 is best categorized as a descriptive measurement-model. Sufficient to account for group differences, but lacking the machinery necessary to provide a full process-level account of how the empirical quantities of interest are generated. In contrast, Project 2 (HTW) implemented a more robust computational modeling approach, implementing and comparing full process models (ALM & EXAM), capable of generating predictions for both the learning and testing stages of the experiment. ALM and EXAM have been used as models of function learning, cue judgement, and forecasting behavior in numerous studies over the past 25 years (Brown & Lacroix, 2017; DeLosh et al., 1997; Kane & Broomell, 2020; H. Kelley & Busemeyer, 2008; Kwantes et al., 2012; McDaniel et al., 2009; Von Helversen & Rieskamp, 2010). The present work presents the first application of these models to to the study of training variability in a visuomotor function learning task. We fit both models to individual participant data, using a form of simulation-based Bayesian parameter estimation that allowed us to generate and compare the full posterior predictive distributions of each model. EXAM provided the best overall account of the testing data, and the advantage of EXAM over ALM was significantly greater for the constant group. Notably, EXAM captured the constant groups’ ability to extrapolate linearly to novel velocity bands, despite receiving training from only a single input-output pair. This finding suggests that EXAM’s linear extrapolation mechanism, combined with the assumption of prior knowledge about the origin point (0, 0), was sufficient to account for the constant groups’ accurate extrapolation performance. Such findings may offer a preliminary suggestion that experience with a more variable set of training examples may be detrimental to performance in simple extrapolation tasks.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/full.html#differences-between-the-two-projects",
    "href": "Sections/full.html#differences-between-the-two-projects",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Differences between the two Projects",
    "text": "Differences between the two Projects\nThe HTT and HTW tasks differ across numerous dimensions that may be relevant to the opposing patterns observed in the two projects (see Table 19 provides for a detailed comparison of the two tasks).\nIn HTT, the salient perceptual elements of the task (i.e. the launching box, target and barrier) are subject to variation (i.e. different distances between the launching box and target), and the spatial layout of these perceptually variable elements are intrinsically linked to the task objective of striking the target. Conversely, the perceptual task elements in HTW are invariant across trials, and the task objective is specified by the target velocity value specified as a numeral at the top of the screen. If the benefits of training variation do arise from the formation and flexible retrieval of distinct memory traces, then the lack of perceptual salience between training instances in the HTW task may have limited any potential benefits of variability. Future work could investigate this possibility further employing a modified version of the HTW task wherein the correct velocity value is indicated by some perceptual feature of the task (e.g., the color of the wall, or size of the ball), rather than displaying the target velocity numerically.\nThe HTT and HTW tasks also differed in terms of general task complexity. The HTT task was designed to mimic projectile launching tasks commonly employed in visuomotor learning studies, and the parabolic trajectories necessary to strike the target in HTT were sensitive to both the x and y dimensions of the projectiles velocity (and to a lesser extent, the position within the launching box at which the ball was released). Conversely the HTW task was influenced to a greater extent by the tasks commonly utilized in the function learning literature, wherein the correct output respones are determined by a single input dimension. In HTW,the relationship between feedback and optimal behavioral adjustment is also almost perfectly smooth, if participants produce a throw that is 100 units too hard, they’ll be told that they were 100 units away from the target band. Whereas in HTT, the presence of the barrier in introduces irregularities in the task space. Even throws close to the solution space might result in failure, creating a less predictable learning environment.\n\n\n\nTable 19: Comparison of the tasks in Project 1 (HTT) and Project 2 (HTW).\n\n\n\n\n\n\n\n\n\n\nDimension\nHTT (Project 1)\nHTW (Project 2)\n\n\n\n\nTask Description\nProjectile launching to hit a target\nProjectile launching to hit wall at a specific velocity\n\n\nTask Complexity\nMore complex parabolic trajectory, both x and y velocities relevant to outcome\nSimpler 1D mapping of force to outcome. Only x velocity is relevant.\n\n\nTask Space\nMore complex: xy velocity combinations closer to the solution space may still result in worse feedback due to striking the barrier.\nSimpler: smooth, linear mapping between velocity and feedback.\n\n\nPerceptual salience of Varied Conditions\nVaried conditions (# of throwing distances) are perceptually distinct, i.e. salient differences in distance between launching box and target.\nVaried conditions (# of velocity bands) are less salient - only difference is the numeral displayed on screen.\n\n\nTesting Feedback\nTesting always included feedback\nPrimary testing stage had no feedback.\n\n\nPotential for Learning during Testing\nLimited potential for learning during testing due to feedback.\nSome potential for learning during no-feedback testing by observing ball trajectory.\n\n\nTraining Experience\nVaried group gets half as much experience on any one position as the constant group.\nVaried group gets 1/3 as much experience on any one velocity band as the constant group.\n\n\nTesting Structure\nRandom interleaving of trained/transfer testing distances.\nBlocked structure, separately testing trained vs extrapolation testing bands.\n\n\n\n\n\n\n\nConclusion\nIn summary, this dissertation provides a comprehensive examination of the effects of training variability on learning and generalization in visuomotor and function learning tasks. The contrasting results obtained from the Hit The Target (HTT) and Hit The Wall (HTW) tasks underscore the complexity inherent to the longstanding pedagogical and scientific goal of identifying training manipulations that consistently benefit learning and generalization. Moreover, through the development and application of computational models, we provide novel theoretical accounts for both the beneficial and detrimental effects of training variability observed in our experiments. These findings highlight the importance of considering task characteristics when designing experiments intended to assess the influence of training interventions, and demonstrate the value of combining empirical and computational modeling approaches to uncover the cognitive mechanisms that support learning and generalization. Future research should continue to investigate the complex interplay between task demands, training manipulations, and individual differences, with the ultimate goal of optimizing educational and training outcomes across a wide range of domains.",
    "crumbs": [
      "Full Dissertation w/Code"
    ]
  },
  {
    "objectID": "Sections/HTW_old.html",
    "href": "Sections/HTW_old.html",
    "title": "HTW Project",
    "section": "",
    "text": "Link to HTW project page\nWorking Draft of HTW Manuscript"
  },
  {
    "objectID": "Sections/HTW_old.html#function-learning-and-extrapolation",
    "href": "Sections/HTW_old.html#function-learning-and-extrapolation",
    "title": "HTW Project",
    "section": "Function Learning and Extrapolation",
    "text": "Function Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with a an incomplete scatterplot or line graph and make predictions about regions of the plot that don’t contain data (Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020; ciccioneCanHumansPerform2021a?).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a ruled based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974),testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out with an assumption of a linear function, but given sufficient error will progressively test alternative hypothesis with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power, as opposed to linear function. Koh & Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to the rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to correctly respond in interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performance are depicted in Figure 1.\nThe authors evaluated both of the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance. The EXAM model retains the same similarity based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participant training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, on the other hand, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate sinusoidal function Bott & Heit (2004) or to scenarios where different functions apply to different regions of the input space Kalish et al. (2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V shaped input spaces Mcdaniel et al. (2009).\n\n\n\n\n\n\n\n\nFigure 1: Generalization reproduced patterns from DeLosh et al. (1997) Figure 3. Stimulii that fall within the dashed lines are interpolations of the training examples."
  },
  {
    "objectID": "Sections/HTW_old.html#participants",
    "href": "Sections/HTW_old.html#participants",
    "title": "HTW Project",
    "section": "Participants",
    "text": "Participants\nData was collected from 647 participants (after exclusions). The results shown below consider data from subjects in our initial experiment, which consisted of 196 participants (106 constant, 90 varied). The follow-up experiments entailed minor manipulations: 1) reversing the velocity bands that were trained on vs. novel during testing; 2) providing ordinal rather than numerical feedback during training (e.g. correct, too low, too high). The data from these subsequent experiments are largely consistently with our initial results shown below."
  },
  {
    "objectID": "Sections/HTW_old.html#task",
    "href": "Sections/HTW_old.html#task",
    "title": "HTW Project",
    "section": "Task",
    "text": "Task\nWe developed a novel visuomotor extrapolation task, termed the Hit The Wall task, wherein participants learned to launch a projectile such that it hit a rectangle at the far end of the screen with an appropriate amount of force. Although the projectile had both x and y velocity components, only the x-dimension was relevant for the task.  Link to task demo"
  },
  {
    "objectID": "Sections/HTW_old.html#procedure",
    "href": "Sections/HTW_old.html#procedure",
    "title": "HTW Project",
    "section": "Procedure",
    "text": "Procedure\nThe HTW task involved launching projectiles to hit a target displayed on the computer screen. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). In contrast, participants in the constant training condition encountered only one velocity band (800-1000).\nDuring the training stage, participants in both conditions also completed “no feedback” trials, where they received no information about their performance. These trials were randomly interleaved with the regular training trials.\nFollowing the training stage, participants proceeded to the testing stage, which consisted of three phases. In the first phase, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials.\nIn the second phase of testing, participants completed “no-feedback” testing from the three velocity bands used during the training stage (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training.\nThe third and final phase of testing involved “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 10 trials. Participants received feedback on their performance during this phase.\nThroughout the experiment, participants’ performance was measured by calculating the distance between the produced x-velocity of the projectiles and the closest edge of the current velocity band. Lower distances indicated better performance.\nAfter completing the experiment, participants were debriefed and provided with an opportunity to ask questions about the study.\n\n\n\n\n\n\n\n\n\n\ncluster\n\nTest Phase \n(Counterbalanced Order)\n\n\n\ndata1\n\n Varied Training \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1\n\nTest  \nNovel Bands \n100-300\n350-550\n600-800\n\n\n\ndata1-&gt;Test1\n\n\n\n\n\ndata2\n\n Constant Training \n800-1000\n\n\n\ndata2-&gt;Test1\n\n\n\n\n\nTest3\n\n    Final Test \n  Novel With Feedback  \n100-300\n350-550\n600-800\n\n\n\nTest2\n\n  Test \n  Varied Training Bands  \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1-&gt;Test2\n\n\n\n\n\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\nFigure 2: Experiment 1 Design. Constant and Varied participants complete different training conditions."
  },
  {
    "objectID": "Sections/HTW_old.html#analyses-strategy",
    "href": "Sections/HTW_old.html#analyses-strategy",
    "title": "HTW Project",
    "section": "Analyses Strategy",
    "text": "Analyses Strategy\nAll data processing and statistical analyses were performed in R version 4.31 Team (2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R Bürkner (2017), and descriptive stats and tables were extracted with the BayestestR package Makowski et al. (2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as circumventing convergence issues common to the frequentist analogues of our mixed models. For each model, we report the median values of the posterior distribution, and 95% credible intervals.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 of which were discarded as warmup chains. Rhat values were generally within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects.\nWe compared varied and constant performance across two measures, deviation and discrimination. Deviation was quantified as the absolute deviation from the nearest boundary of the velocity band, or set to 0 if the throw velocity fell anywhere inside the target band. Thus, when the target band was 600-800, throws of 400, 650, and 1100 would result in deviation values of 200, 0, and 300, respectively. Discrimination was measured by fitting a linear model to the testing throws of each subjects, with the lower end of the target velocity band as the predicted variable, and the x velocity produced by the participants as the predictor variable. Participants who reliably discriminated between velocity bands tended to have positive slopes with values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.\n\n\nCode\nresult &lt;- test_summary_table(test, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\n\n\nresult$constant |&gt;kable(booktabs = TRUE,\n                        linesep = \"\\\\addlinespace[0.5em]\")\n                        #caption = paste(\"Summary of Deviation- Constant\"))\n# |&gt;\n#   kable_styling(font_size = ifelse(fmt_out == \"latex\", 8.5, NA))\n\nresult$varied |&gt; kable(booktabs = TRUE,\n                        linesep = \"\\\\addlinespace[0.5em]\")\n                        #caption = paste(\"Summary of Deviation- Varied\"))\n\n\n\n\nTable 1: Testing Deviation - Empirical Summary\n\n\n\n\n\n\n(a) Full datasets\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n254\n148\n298\n\n\n350-550\nExtrapolation\n191\n110\n229\n\n\n600-800\nExtrapolation\n150\n84\n184\n\n\n800-1000\nTrained\n184\n106\n242\n\n\n1000-1200\nExtrapolation\n233\n157\n282\n\n\n1200-1400\nExtrapolation\n287\n214\n290\n\n\n\n\n\n\n\n\n\n\n\n(b) Intersection of samples with all labels available\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n386\n233\n426\n\n\n350-550\nExtrapolation\n285\n149\n340\n\n\n600-800\nExtrapolation\n234\n144\n270\n\n\n800-1000\nTrained\n221\n149\n248\n\n\n1000-1200\nTrained\n208\n142\n226\n\n\n1200-1400\nTrained\n242\n182\n235"
  },
  {
    "objectID": "Sections/HTW_old.html#results",
    "href": "Sections/HTW_old.html#results",
    "title": "HTW Project",
    "section": "Results",
    "text": "Results\n\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\n\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 1 and Figure 3. To model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\\[\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\n\nCode\ntest |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\n\n\n\n\n\nFigure 3: E1. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\n\nCode\nmodelName &lt;- \"e1_testDistBand_RF_5K\"\ne1_distBMM &lt;- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nGetModelStats(e1_distBMM) |&gt; kable(booktabs = TRUE,caption = paste(\"Coefficients\"))\n\n\ne1_distBMM |&gt; \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |&gt; \n  pairs() |&gt; gather_emmeans_draws()  |&gt; \n   summarize(median_qi(.value),pd=sum(.value&gt;0)/n()) |&gt;\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value&lt;0,1-pd,pd)) |&gt; kable(booktabs = TRUE)\n# |&gt; \n#   kbl(caption=\"Contrasts\")\n\ncoef_details &lt;- get_coef_details(e1_distBMM, \"conditVaried\")\n\n\n\n\nTable 2: Experiment 1. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n(a) Constant Testing1 - Deviation\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n205.09\n136.86\n274.06\n1.00\n\n\nconditVaried\n157.44\n60.53\n254.90\n1.00\n\n\nBand\n0.01\n-0.07\n0.08\n0.57\n\n\ncondit*Band\n-0.16\n-0.26\n-0.06\n1.00\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\n\n\ncontrast\nBand\nvalue\nlower\nupper\npd\n\n\n\n\nConstant - Varied\n100\n-141.49\n-229.2\n-53.83\n1.00\n\n\nConstant - Varied\n350\n-101.79\n-165.6\n-36.32\n1.00\n\n\nConstant - Varied\n600\n-62.02\n-106.2\n-14.77\n1.00\n\n\nConstant - Varied\n800\n-30.11\n-65.1\n6.98\n0.94\n\n\nConstant - Varied\n1000\n2.05\n-33.5\n38.41\n0.54\n\n\nConstant - Varied\n1200\n33.96\n-11.9\n81.01\n0.92\n\n\n\n\n\n\n\n\n\n\n\nThe model predicting absolute deviation (dist) showed clear effects of both training condition and target velocity band (Table X). Overall, the varied training group showed a larger deviation relative to the constant training group (β = 157.44, 95% CI [60.53, 254.9]). Deviation also depended on target velocity band, with lower bands showing less deviation. See Table 2 for full model output.\n\n\nDiscrimination between bands\nIn addition to accuracy/deviation, we also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 3 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants on each testing trial.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\n\nCode\ntest %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 4: E1 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nCode\n\nresult &lt;- test_summary_table(test, \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable(booktabs = TRUE)\nresult$varied |&gt; kable(booktabs = TRUE)\n\n\n\n\nTable 3: Testing vx - Empirical Summary\n\n\n\n\n\n\n(a) Constant\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n524\n448\n327\n\n\n350-550\nExtrapolation\n659\n624\n303\n\n\n600-800\nExtrapolation\n770\n724\n300\n\n\n800-1000\nTrained\n1001\n940\n357\n\n\n1000-1200\nExtrapolation\n1167\n1104\n430\n\n\n1200-1400\nExtrapolation\n1283\n1225\n483\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n664\n533\n448\n\n\n350-550\nExtrapolation\n768\n677\n402\n\n\n600-800\nExtrapolation\n876\n813\n390\n\n\n800-1000\nTrained\n1064\n1029\n370\n\n\n1000-1200\nTrained\n1180\n1179\n372\n\n\n1200-1400\nTrained\n1265\n1249\n412\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nGetModelStats(e1_vxBMM ) |&gt; kable(booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\nmodelName &lt;- \"e1_extrap_testVxBand\"\ne1_extrap_VxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                  data=test |&gt;\n                    filter(expMode==\"test-Nf\"),file=paste0(here::here(\"data/model_cache\",modelName)),\n                  iter=5000,chains=4)\nGetModelStats(e1_extrap_VxBMM ) |&gt; kable(booktabs=T, caption=\"Fit to 3 extrapolation bands\")\n\n\nsc2 &lt;- get_coef_details(e1_extrap_VxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e1_extrap_VxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 4: Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n(a) Model fit to all 6 bands\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\n\n\n\n\n\n(b) Model fit to 3 extrapolation bands\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n478.47\n404.00\n551.45\n1.00\n\n\nconditVaried\n142.04\n37.17\n247.59\n1.00\n\n\nBand\n0.50\n0.42\n0.57\n1.00\n\n\ncondit*Band\n-0.07\n-0.17\n0.04\n0.89\n\n\n\n\n\n\n\n\n\n\n\nSee Table 4 for the full model results. The estimated coefficient for training condition (\\(B\\) = 164.05, 95% CrI [45.5, 278.85]) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the slope on Velocity Band (\\(B\\) = 0.71, 95% CrI [0.62, 0.8]). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition (\\(B\\) = -0.14, 95% CrI [-0.26, -0.01]), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less senitivity between vands than the constant condition. This difference is depicted visually in Figure 5.@tbl-e1-slope-quartile shows the average slope coefficients for varied and constant participants separately for each quartile. The constant participant participants appear to have larger slopes across quartiles, but the difference between conditions may be less pronounced for the top quartiles of subjects who show the strongest discrimination. Figure Figure 6 shows the distributions of slope values for each participant, and the compares the probability density of slope coefficients between training conditions. Figure 7\nThe second model, which focused solely on extrapolation bands, revealed similar patterns. The Velocity Band term (\\(B\\) = 0.5, 95% CrI [0.42, 0.57]) still demonstrates a high degree of discrimination ability. However, the posterior distribution for interaction term (\\(B\\) = -0.07, 95% CrI [-0.17, 0.04] ) does across over 0, suggesting that the evidence for decreased discrimination ability for the varied participants is not as strong when considering only the three extrapolation bands.\n\nCode\ne1_vxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(test$vb), \n                     limits = c(0, 1400))\n\ne1_extrap_VxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600))) |&gt;\n  gather_emmeans_draws() |&gt;\n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600), \n                     labels = levels(test$vb)[1:3], \n                     limits = c(0, 1000)) \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Model fit to all 6 bands\n\n\n\n\n\n\n\n\n\n\n\n(b) Model fit to only 3 extrapolation bands\n\n\n\n\n\n\n\nFigure 5: Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n\n\n\n\n\nCode\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")]))) |&gt; \n  dplyr::arrange(id,bandInt) |&gt; \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs &lt;- as_tibble(coef(e1_vxBMM)$id, rownames=\"id\")|&gt; \n  select(id, starts_with(\"Est\")) |&gt;\n  left_join(e1Sbjs, by=join_by(id) ) \n\n\nfixed_effects &lt;- e1_vxBMM |&gt; \n  spread_draws(`^b_.*`,regex=TRUE) |&gt; arrange(.chain,.draw,.iteration)\n\n\nrandom_effects &lt;- e1_vxBMM |&gt; \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1500) |&gt; \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |&gt; \n  mutate(id = factor(id,levels=levels(test$id))) |&gt; \n  pivot_wider(names_from = term, values_from = .value) |&gt; arrange(id,.chain,.draw,.iteration)\n\n\n indvDraws &lt;- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |&gt; \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |&gt;\n  right_join(new_data_grid, by = join_by(\"id\")) |&gt; \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)) + (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt = Slope + (`b_conditVaried:bandInt`*condit_dummy)\n  ) \n\n  indvSlopes &lt;- indvDraws |&gt; group_by(id) |&gt; median_qi(Slope,SlopeInt, Intercept,b_Intercept,b_bandInt) |&gt;\n  left_join(e1Sbjs, by=join_by(id)) |&gt; group_by(condit) |&gt;\n    select(id,condit,Intercept,b_Intercept,starts_with(\"Slope\"),b_bandInt, n) |&gt;\n  mutate(rankSlope=rank(Slope)) |&gt; arrange(rankSlope)   |&gt; ungroup()\n \n  \n  indvSlopes |&gt; mutate(Condition=condit) |&gt;  group_by(Condition) |&gt; \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |&gt; \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |&gt;\n  group_by(Condition) |&gt;\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |&gt; kable()\n\n\n\n\nTable 5: Slope coefficients by quartile, per condition\n\n\n\n\n\n\nCondition\nQ_0%_mean\nQ_25%_mean\nQ_50%_mean\nQ_75%_mean\nQ_100%_mean\n\n\n\n\nConstant\n-0.109\n0.483\n0.690\n0.932\n1.4\n\n\nVaried\n-0.202\n0.270\n0.586\n0.901\n1.3\n\n\n\n\n\n\n\n\n\nCode\n  indvSlopes |&gt; ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + facet_wrap(~condit)\n\n   ggplot(indvSlopes, aes(x = SlopeInt, color = condit)) + \n  geom_density() + labs(x=\"Slope Coefficient\",y=\"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Slope estimates by participant - ordered from lowest to highest within each condition.\n\n\n\n\n\n\n\n\n\n\n\n(b) Destiny of slope coefficients by training group\n\n\n\n\n\n\n\nFigure 6: Slope distributions between condition\n\n\n\ntest\n\n\nCode\n\nnSbj &lt;- 3\nindvDraws  |&gt; indv_model_plot(indvSlopes, testAvg, SlopeInt,rank_variable=Slope,n_sbj=nSbj,\"max\")\nindvDraws |&gt; indv_model_plot(indvSlopes, testAvg,SlopeInt, rank_variable=Slope,n_sbj=nSbj,\"min\")\n\n\n\n\n\n\n\n\n\n\n\n(a) subset with largest slopes\n\n\n\n\n\n\n\n\n\n\n\n(b) subset with smallest slopes\n\n\n\n\n\n\nFigure 7: Subset of Varied and Constant Participants with the smallest and largest estimated slope values. Red lines represent the best fitting line for each participant, gray lines are 200 random samples from the posterior distribution. Colored points and intervals at each band represent the empirical median and 95% HDI."
  },
  {
    "objectID": "Sections/HTW_old.html#e2-results",
    "href": "Sections/HTW_old.html#e2-results",
    "title": "HTW Project",
    "section": "E2 Results",
    "text": "E2 Results\n\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\n\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 6 and Figure 9. To model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\\[\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\n\nCode\nresult &lt;- test_summary_table(testE2, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable()\nresult$varied |&gt; kable()\n# make kable table with smaller font size\n# result$constant |&gt; kbl(caption=\"Constant Testing - Deviation\",booktabs=T,escape=F) |&gt; kable_styling(font_size = 7)\n\n\n\n\nTable 6: Testing Deviation - Empirical Summary\n\n\n\n\n\n\n(a) Constant Testing - Deviation\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n206\n48\n317\n\n\n350-550\nExtrapolation\n194\n86\n268\n\n\n600-800\nTrained\n182\n112\n240\n\n\n800-1000\nExtrapolation\n200\n129\n233\n\n\n1000-1200\nExtrapolation\n238\n190\n234\n\n\n1200-1400\nExtrapolation\n311\n254\n288\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nTrained\n153\n25\n266\n\n\n350-550\nTrained\n138\n53\n233\n\n\n600-800\nTrained\n160\n120\n183\n\n\n800-1000\nExtrapolation\n261\n207\n257\n\n\n1000-1200\nExtrapolation\n305\n258\n273\n\n\n1200-1400\nExtrapolation\n363\n314\n297\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntestE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\n\n\n\n\n\nFigure 9: E2. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\n\nCode\nmodelName &lt;- \"e2_testDistBand_RF_5K\"\ne2_distBMM &lt;- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=testE2,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp2 &lt;- GetModelStats(e2_distBMM) |&gt; kable(booktabs=T)\nmp2\n\ne2_distBMM |&gt; \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |&gt; \n  pairs() |&gt; gather_emmeans_draws()  |&gt; \n   summarize(median_qi(.value),pd=sum(.value&gt;0)/n()) |&gt;\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value&lt;0,1-pd,pd)) |&gt;\n   kable()\n\ncoef_details &lt;- get_coef_details(e2_distBMM, \"conditVaried\")\n\n\n\n\nTable 7: Experiment 2. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n(a) Model fits\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n151.71\n90.51\n215.86\n1.00\n\n\nconditVaried\n-70.33\n-156.87\n16.66\n0.94\n\n\nBand\n0.10\n0.02\n0.18\n1.00\n\n\ncondit*Band\n0.12\n0.02\n0.23\n0.99\n\n\n\n\n\n\n\n\n\n\n\n(b) Contrasts\n\n\n\n\n\ncontrast\nBand\nvalue\nlower\nupper\npd\n\n\n\n\nConstant - Varied\n100\n57.6\n-20.5\n135.32\n0.93\n\n\nConstant - Varied\n350\n26.6\n-30.9\n83.84\n0.83\n\n\nConstant - Varied\n600\n-4.3\n-46.7\n38.52\n0.58\n\n\nConstant - Varied\n800\n-29.3\n-69.4\n11.29\n0.92\n\n\nConstant - Varied\n1000\n-54.6\n-101.1\n-5.32\n0.98\n\n\nConstant - Varied\n1200\n-79.6\n-139.5\n-15.45\n0.99\n\n\n\n\n\n\n\n\n\n\n\nThe model predicting absolute deviation showed a modest tendency for the varied training group to have lower deviation compared to the constant training group (β = -70.33, 95% CI [-156.87, 16.66]),with 94% of the posterior distribution being less than 0. This suggests a potential benefit of training with variation, though the evidence is not definitive.\n\n\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 8 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\n\nCode\ntestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 10: E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\nCode\nresult &lt;- test_summary_table(testE2, \"vx\",\"X Velocity\" ,mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |&gt; kable()\nresult$varied |&gt; kable()\n\n\n\n\nTable 8: Testing vx - Empirical Summary\n\n\n\n\n\n\n\n(a) Constant Testing - vx\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n457\n346\n354\n\n\n350-550\nExtrapolation\n597\n485\n368\n\n\n600-800\nTrained\n728\n673\n367\n\n\n800-1000\nExtrapolation\n953\n913\n375\n\n\n1000-1200\nExtrapolation\n1064\n1012\n408\n\n\n1200-1400\nExtrapolation\n1213\n1139\n493\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - vx\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nTrained\n410\n323\n297\n\n\n350-550\nTrained\n582\n530\n303\n\n\n600-800\nTrained\n696\n641\n316\n\n\n800-1000\nExtrapolation\n910\n848\n443\n\n\n1000-1200\nExtrapolation\n1028\n962\n482\n\n\n1200-1400\nExtrapolation\n1095\n1051\n510\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=testE2,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nmt3 &lt;-GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T)\nmt3\n\ncd1 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 9: Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\n\n\nSee Table 9 for the full model results.\nWhen examining discrimination ability using the model predicting raw x-velocity, the results were less clear than those of the absolute deviation analysis. The slope on Velocity Band (β = 0.71, 95% CrI [0.58, 0.84]) indicates that participants showed good discrimination between bands overall. However, the interaction term suggested this effect was not modulated by training condition (β = -0.06, 95% CrI [-0.24, 0.13]) Thus, while varied training may provide some advantage for accuracy, both training conditions seem to have similar abilities to discriminate between velocity bands.\n\n\nCode\ne2_vxBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  ylab(\"Predicted X Velocity\") + xlab(\"Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\n\n\n\n\n\n\nFigure 11: Conditional effect of training condition and Band. Ribbons indicate 95% HDI."
  },
  {
    "objectID": "Sections/HTW_old.html#results-1",
    "href": "Sections/HTW_old.html#results-1",
    "title": "HTW Project",
    "section": "Results",
    "text": "Results\n\nTesting Phase - No feedback.\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw. Note that these no-feedback testing trials are identical to those of Experiment 1 and 2, as the ordinal feedback only occurs during the training phase, and final testing phase, of Experiment 3.\n\nDeviation From Target Band\nDescriptive summaries testing deviation data are provided in Table 10 and Figure 12. To model differences in accuracy between groups, we fit Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\n\nCode\nresultOrig &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Original\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |&gt; kable()\nresultOrig$varied |&gt; kable()\n\nresultRev &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Reverse\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |&gt; kable()\nresultRev$varied |&gt; kable()\n\n\n\n\nTable 10: Testing Deviation - Empirical Summary\n\n\n\n\n\n\n(a) Constant Testing - Deviation\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n396\n325\n350\n\n\n350-550\nExtrapolation\n278\n176\n299\n\n\n600-800\nExtrapolation\n173\n102\n215\n\n\n800-1000\nTrained\n225\n126\n284\n\n\n1000-1200\nExtrapolation\n253\n192\n271\n\n\n1200-1400\nExtrapolation\n277\n210\n262\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - Deviation\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n383\n254\n385\n\n\n350-550\nExtrapolation\n287\n154\n318\n\n\n600-800\nExtrapolation\n213\n140\n244\n\n\n800-1000\nTrained\n199\n142\n209\n\n\n1000-1200\nTrained\n222\n163\n221\n\n\n1200-1400\nTrained\n281\n227\n246\n\n\n\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n403\n334\n383\n\n\n350-550\nExtrapolation\n246\n149\n287\n\n\n600-800\nTrained\n155\n82\n209\n\n\n800-1000\nExtrapolation\n207\n151\n241\n\n\n1000-1200\nExtrapolation\n248\n220\n222\n\n\n1200-1400\nExtrapolation\n322\n281\n264\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nTrained\n153\n0\n307\n\n\n350-550\nTrained\n147\n55\n258\n\n\n600-800\nTrained\n159\n107\n192\n\n\n800-1000\nExtrapolation\n221\n160\n235\n\n\n1000-1200\nExtrapolation\n244\n185\n235\n\n\n1200-1400\nExtrapolation\n324\n264\n291\n\n\n\n\n\n\n\n\n\n\nCode\ntestE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\") + facet_wrap(~bandOrder)\n\n\n\n\n\n\n\n\nFigure 12: e3. Deviations from target band during testing without feedback stage.\n\n\n\n\n\n\n\nCode\nmodelName &lt;- \"e3_testDistBand_RF_5K\"\ne3_distBMM &lt;- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=testE3,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp3 &lt;- GetModelStats(e3_distBMM) |&gt; kable(booktabs=T)\nmp3\n\n\ncd1 &lt;- get_coef_details(e3_distBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_distBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_distBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 11: Experiment 3. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n306.47\n243.89\n368.75\n1.00\n\n\nconditVaried\n-90.65\n-182.79\n3.75\n0.97\n\n\nBand\n-0.07\n-0.13\n0.00\n0.97\n\n\ncondit*Band\n0.09\n-0.01\n0.19\n0.96\n\n\n\n\n\n\n\n\nThe effect of training condition in Experiment 3 showed a similar pattern to Experiment 2, with the varied group tending to have lower deviation than the constant group (β = -90.65, 95% CrI [-182.79, 3.75]), with 97% of the posterior distribution falling under 0.\n\n\nCode\n\ne3_distBMM |&gt; emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt;\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n    ylab(\"Predicted Deviation\") + xlab(\"Velocity Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n\n\n\n\n\n\n\n\nFigure 13: e3. Conditioinal Effect of Training Condition and Band. Ribbon indicated 95% Credible Intervals.\n\n\n\n\n\n\n\nDiscrimination between Velocity Bands\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). Table 12 shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants.\n\\[\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\\]\n\n\nCode\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n  facet_wrap(bandOrder~condit,scale=\"free_x\") \n\n# column: screen-inset-right\n\n\n\n\n\n\n\n\nFigure 14: e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nCode\nresultOrig &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Original\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |&gt; kable()\nresultOrig$varied |&gt; kable()\n\nresultRev &lt;- test_summary_table(testE3 |&gt; filter(bandOrder==\"Reverse\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |&gt; kable()\nresultRev$varied |&gt; kable()\n\n\n\n\nTable 12: Testing vx - Empirical Summary\n\n\n\n\n\n\n(a) Constant Testing - vx\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n680\n625\n370\n\n\n350-550\nExtrapolation\n771\n716\n357\n\n\n600-800\nExtrapolation\n832\n786\n318\n\n\n800-1000\nTrained\n1006\n916\n417\n\n\n1000-1200\nExtrapolation\n1149\n1105\n441\n\n\n1200-1400\nExtrapolation\n1180\n1112\n443\n\n\n\n\n\n\n\n\n\n\n\n(b) Varied Testing - vx\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n667\n554\n403\n\n\n350-550\nExtrapolation\n770\n688\n383\n\n\n600-800\nExtrapolation\n869\n814\n358\n\n\n800-1000\nTrained\n953\n928\n359\n\n\n1000-1200\nTrained\n1072\n1066\n388\n\n\n1200-1400\nTrained\n1144\n1093\n426\n\n\n\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nExtrapolation\n684\n634\n406\n\n\n350-550\nExtrapolation\n729\n679\n350\n\n\n600-800\nTrained\n776\n721\n318\n\n\n800-1000\nExtrapolation\n941\n883\n387\n\n\n1000-1200\nExtrapolation\n1014\n956\n403\n\n\n1200-1400\nExtrapolation\n1072\n1014\n442\n\n\n\n\n\n\n\n\nBand\nBand Type\nMean\nMedian\nSd\n\n\n\n\n100-300\nTrained\n392\n270\n343\n\n\n350-550\nTrained\n540\n442\n343\n\n\n600-800\nTrained\n642\n588\n315\n\n\n800-1000\nExtrapolation\n943\n899\n394\n\n\n1000-1200\nExtrapolation\n1081\n1048\n415\n\n\n1200-1400\nExtrapolation\n1185\n1129\n500\n\n\n\n\n\n\n\n\n\n\nCode\ne3_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=testE3,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nmt4 &lt;-GetModelStats(e3_vxBMM ) |&gt; kable(booktabs=T)\nmt4\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\nTable 13: Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\ncondit*Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:Band\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:Band\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\n\n\nSee Table 13 for the full model results.\nSlope estimates for experiment 3 suggest that participants were capable of distinguishing between velocity bands even when provided only ordinal feedback during training (β = 0.49, 95% CrI [0.36, 0.62]). Unlike the previous two experiments, the posterior distribution for the interaction between condition and band was consistently positive, suggestive of superior discrimination for the varied participants β = -0.04, 95% CrI [-0.23, 0.15].\n\n\n\nComputational Modelling"
  },
  {
    "objectID": "Sections/htw_exam.html",
    "href": "Sections/htw_exam.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\nCode\n# load and view data\npacman::p_load(tidyverse,patchwork,here, pander, latex2exp, flextable)\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\", \"Functions/alm_core.R\",\"Functions/misc_model_funs.R\")),source)\n\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\")),source)\n\nselect &lt;- dplyr::select; mutate &lt;- dplyr::mutate \n\nds &lt;- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))\ndsAvg &lt;- ds |&gt; group_by(condit,expMode2,tr, x) |&gt; \n  summarise(y=mean(y),.groups=\"keep\") \n\nvAvg &lt;- dsAvg |&gt; filter(condit==\"Varied\")\ncAvg &lt;- dsAvg |&gt; filter(condit==\"Constant\")\n\n#i1 &lt;- ds |&gt; filter(id==\"3\")\n\ninput.layer &lt;- c(100,350,600,800,1000,1200)\noutput.layer &lt;- c(100,350,600,800,1000,1200)\n\n\npurrr::walk(c(\"con_group_exam_fits\", \"var_group_exam_fits\", \"hybrid_group_exam_fits\"), \n            ~ list2env(readRDS(here::here(paste0(\"data/model_cache/\", .x, \".rds\"))), \n            envir = .GlobalEnv))\n\n# pluck(ex_te_v, \"Fit\") |&gt; mutate(w= ifelse(exists(\"w\"), round(w,2),NA))\n# pluck(hybrid_te_v, \"Fit\") |&gt; mutate(w= ifelse(exists(\"w\"), round(w,2), NA))\nCode\n alm_plot()"
  },
  {
    "objectID": "Sections/htw_exam.html#alm-exam-description",
    "href": "Sections/htw_exam.html#alm-exam-description",
    "title": "",
    "section": "ALM & Exam Description",
    "text": "ALM & Exam Description\nDeLosh et al. (1997) introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke’s influential ALCOVE model of categorization (Kruschke, 1992).\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nSee Table 1 for a full specification of the equations that define ALM and EXAM."
  },
  {
    "objectID": "Sections/htw_exam.html#model-fitting-and-comparison",
    "href": "Sections/htw_exam.html#model-fitting-and-comparison",
    "title": "",
    "section": "Model Fitting and Comparison",
    "text": "Model Fitting and Comparison\nFollowing the procedure used by Mcdaniel et al. (2009), we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models were fit to the aggregated participant data by minimizing the root-mean squared deviation (RMSE). Because ALM has been shown to do poorly at accounting for human patterns extrapolation (DeLosh et al., 1997), we will also generate predictions from the EXAM model for the testing stage. EXAM which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing.\nFor the hybrid model, predictions are computed by first generating separate predictions from ALM and EXAM, and then combining them using the following equation: \\(\\hat{y} = (1 - w) \\cdot alm.pred + w \\cdot exam.pred\\). For the grid search, the weight parameter is varied from 0 to 1, and the resulting RMSE is recorded.\nEach model was fit to the data in 3 different ways. 1) To just the testing data, 2) Both the training and testing data, 3) Only the training data. In all cases, the model only updates its weights during the training phase, and the weights are frozen during the testing phase. In all cases, only the ALM model generates predictions during the training phase. For the testing phase, all 3 models are used to generate predictions.\n\n\n\n\nTable 2: Fit Parameters and Model RMSE. The Test_RMSE column is the main performance indicator of interest, and represents the RMSE for just the testing data. The Fit_Method column indicates the data used to fit the model. The \\(w\\) parameter determines the balance between the ALM and EXAM response generation processes, and is only included for the hybrid model. A weight of .5 would indicate equal contribution from both models. \\(w\\) values approaching 1 indicate stronger weight for EXAM.\n\n\n\nCode\n##| column: body-outset-right\n\n\nreshaped_df &lt;- all_combined_params %&gt;%\n  select(-Value,-Test_RMSE) |&gt;\n  rename(\"Fit Method\" = Fit_Method) |&gt;\n  pivot_longer(cols=c(c,lr,w),names_to=\"Parameter\") %&gt;%\n  unite(Group, Group, Parameter) %&gt;%\n  pivot_wider(names_from = Group, values_from = value)\n\nheader_df &lt;- data.frame(\n  col_keys = c(\"Model\", \"Fit Method\",\"Constant_c\", \"Constant_lr\", \"Constant_w\", \"Varied_c\", \"Varied_lr\", \"Varied_w\"),\n  line1 = c(\"\", \"\", \"Constant\", \"\", \"\", \"Varied\", \"\",\"\"),\n  line2 = c(\"Model\", \"Fit Method\", \"c\", \"lr\", \"w\", \"c\", \"lr\", \"w\")\n)\n\nft &lt;- flextable(reshaped_df) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% add_header_lines(values = \" \") %&gt;%\n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;% \n  hline(part = \"header\", i = 2, j=3:5) %&gt;% \n  hline(part = \"header\", i = 2, j=6:8)\n\nft\n\n\n\n\n\nTesting Observations vs. Predictions\n\n\nCode\ntvte&lt;- pluck(a_te_v, \"test\") |&gt; \n  mutate(Fit_Method=\"Test Only\") |&gt;\n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |&gt; pull(pred))\n\ntvtetr&lt;-pluck(a_tetr_v, \"test\") |&gt; \n  mutate(Fit_Method=\"Test & Train\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |&gt; pull(pred))\n\ntvtr&lt;- pluck(a_tr_v, \"test\")|&gt; \n  mutate(Fit_Method=\"Train Only\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |&gt; pull(pred))\n\ntcte&lt;- pluck(a_te_c, \"test\") |&gt; \n  mutate(Fit_Method=\"Test Only\") |&gt; \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |&gt; pull(pred))\n\ntctetr&lt;-pluck(a_tetr_c, \"test\") |&gt; \n  mutate(Fit_Method=\"Test & Train\") |&gt;  \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |&gt; pull(pred))\n\ntctr&lt;- pluck(a_tr_c, \"test\")|&gt; \n  mutate(Fit_Method=\"Train Only\") |&gt;  \n  rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |&gt; pull(pred))\n\nvPreds &lt;- rbind(tvte,tvtetr, tvtr) |&gt; relocate(Fit_Method,.before=x) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\ncPreds &lt;- rbind(tcte,tctetr, tctr) |&gt; relocate(Fit_Method,.before=x) |&gt; \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\nallPreds &lt;- rbind(vPreds |&gt; mutate(Group=\"Varied\"), cPreds |&gt; mutate(Group=\"Constant\")) |&gt;\n  pivot_longer(cols=c(\"ALM\",\"EXAM\",\"Hybrid\"), names_to=\"Model\",values_to = \"Prediction\") |&gt; \n  mutate(Error=Observed-Prediction, Abs_Error=((Error)^2)) |&gt; \n  group_by(Group,Fit_Method, Model) #|&gt; summarise(Mean_Error=mean(Error), Abs_Error=mean(Abs_Error))\n\n\n\n\n\nTable 3: Model Perforamnce - averaged over all X values/Bands. ME=Mean Average Error, RMSE = Root mean squared error.\n\n\n\nCode\nallPreds |&gt; summarise(Error=mean(Error), Abs_Error=sqrt(mean(Abs_Error))) |&gt; \n  mutate(Fit_Method=factor(Fit_Method, levels=c(\"Test Only\", \"Test & Train\", \"Train Only\"))) |&gt;\n  tabulator(rows=c(\"Fit_Method\", \"Model\"), columns=c(\"Group\"), \n             `ME` = as_paragraph(Error), \n            `RMSE` = as_paragraph(Abs_Error)) |&gt; as_flextable()"
  },
  {
    "objectID": "Sections/htw_exam.html#varied-testing-predictions",
    "href": "Sections/htw_exam.html#varied-testing-predictions",
    "title": "",
    "section": "Varied Testing Predictions",
    "text": "Varied Testing Predictions\n\n\nCode\n##| column: screen-inset-right\n\n####\n\nvte &lt;-  pluck(a_te_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nvtetr &lt;-  pluck(a_tetr_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nvtr &lt;-  pluck(a_tr_v, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n\n vte/vtetr/vtr\n\n\n\n\n\nTable 4: Varied group - mean model predictions vs. observations. Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).\n\n\n\nCode\n##| column: screen-inset-right\n\n\n# Create a custom header dataframe\nheader_df &lt;- data.frame(\n  col_keys = c(\"Fit_Method\", \"x\",\"Observed\" ,\"ALM_Predicted\", \"ALM_Residual\", \"EXAM_Predicted\",\"EXAM_Residual\", \"Hybrid_Predicted\",\"Hybrid_Residual\"),\n  line1 = c(\"\",\"\",\"\", \"ALM\", \"\", \"EXAM\", \"\", \"Hybrid\",\"\"),\n  line2 = c(\"Fit Method\", \"X\", \"Observed\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\")\n)\n\n\nbest_vPreds &lt;- vPreds %&gt;%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |&gt;\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |&gt; group_by(Fit_Method,x) |&gt;\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |&gt; select(-abs_res)\n\nlong_vPreds &lt;- best_vPreds |&gt; select(-best) |&gt;\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |&gt;\n  relocate(Model, .after=Fit_Method) |&gt; \n  unite(Model,Model,Model_Perf) |&gt;\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wide &lt;- best_vPreds |&gt; select(-Residual,-Predicted,-Observed) |&gt; ungroup() |&gt;\n  pivot_wider(names_from=Model,values_from=best) |&gt; select(ALM,EXAM,Hybrid)\n\nbest_indexV &lt;- row_indices &lt;- apply(best_wide, 1, function(row) {\n which(row == 1)\n})\n\n\napply_best_formatting &lt;- function(ft, best_index) {\n  for (i in 1:length(best_index)) {\n      #ft &lt;- ft %&gt;% surround(i=i,j=best_index[i],border=fp_border_default(color=\"red\",width=1))\n      ind = best_index[[i]]\n      ind &lt;- ind  %&gt;% map_dbl(~ .x*2+3)\n      ft &lt;- ft %&gt;% highlight(i=i,j=ind,color=\"wheat\")\n      }\n  return(ft)\n}\n\nft &lt;- flextable(long_vPreds) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% \n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;%\n  hline(part = \"header\", i = 1, j=4:9) %&gt;%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %&gt;%\n  hline(part = \"body\", i=c(6,12)) |&gt; \n  bold(i=long_vPreds$x %in% c(100,350,600), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\nft &lt;- apply_best_formatting(ft, best_indexV)\nft\n\n\n\n\n\n\nCode\npander(tvte, caption=\"Varied fit to test only\")\npander(tvtetr,caption=\"Varied fit to train and test\")\npander(tvtr,caption=\"Varied fit to train only\")"
  },
  {
    "objectID": "Sections/htw_exam.html#constant-testing-predictions",
    "href": "Sections/htw_exam.html#constant-testing-predictions",
    "title": "",
    "section": "Constant Testing Predictions",
    "text": "Constant Testing Predictions\n\n\nCode\n##| column: screen-inset-right\n\n####\n\ncte &lt;-  pluck(a_te_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nctetr &lt;-  pluck(a_tetr_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nctr &lt;-  pluck(a_tr_c, \"test\") |&gt; rename(ALM=pred,Observed=y) %&gt;% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |&gt; pull(pred)) %&gt;%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |&gt; pull(pred)) |&gt;  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |&gt; \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n  \ncte/ctetr/ctr\n\n\n\n\n\nTable 5: Constant group - mean model predictions vs. observations. The X values of Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).\n\n\n\nCode\n##| column: screen-inset-right\n\n\n\nbest_cPreds &lt;- cPreds %&gt;%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |&gt;\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |&gt; group_by(Fit_Method,x) |&gt;\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |&gt; select(-abs_res)\n\nlong_cPreds &lt;- best_cPreds |&gt; select(-best) |&gt;\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |&gt;\n  relocate(Model, .after=Fit_Method) |&gt; \n  unite(Model,Model,Model_Perf) |&gt;\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wideC &lt;- best_cPreds |&gt; select(-Residual,-Predicted,-Observed) |&gt; ungroup() |&gt;\n  pivot_wider(names_from=Model,values_from=best) |&gt; select(ALM,EXAM,Hybrid)\n\nbest_indexC &lt;- row_indices &lt;- apply(best_wideC, 1, function(row) {\n which(row == 1)\n})\n\n\nft &lt;- flextable(long_cPreds) %&gt;% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %&gt;% \n  theme_booktabs() %&gt;% \n  merge_v(part = \"header\") %&gt;% \n  merge_h(part = \"header\") %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;% \n  #autofit() %&gt;% \n  empty_blanks() %&gt;% \n  fix_border_issues() %&gt;%\n  hline(part = \"header\", i = 1, j=4:9) %&gt;%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %&gt;%\n  hline(part = \"body\", i=c(6,12)) |&gt; \n  bold(i=long_cPreds$x %in% c(100,350,600, 1000,1200), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\n\nft &lt;- apply_best_formatting(ft, best_indexC)\nft"
  },
  {
    "objectID": "Sections/Appendix/E1_Appendix.html",
    "href": "Sections/Appendix/E1_Appendix.html",
    "title": "Dissertation",
    "section": "",
    "text": "# print(getwd())\n# here::set_here(path='..')\n# print(getwd())\n#source(here::here(\"Functions\", \"packages.R\"))\npacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  stringr, here,conflicted, patchwork, knitr, cowplot, RColorBrewer,data.table,gt,\n  ggh4x,htmltools,future,furrr,ez)\nwalk(c(\"dplyr\",\"here\"), conflict_prefer_all, quiet = TRUE)\n\nwalk(c(\"Display_Functions\", \"org_functions\", \"Table_Functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\n\ntest &lt;- readRDS(here::here(\"data/e1_08-21-23.rds\")) |&gt; filter(expMode2 == \"Test\")  |&gt;\n  select(id,condit,bandInt,vb,vx,dist,sdist,bandType,tOrder)\n\n\nPosterior Predictive Distributions\n\ndist_pred &lt;- \n  posterior_predict(e1_distBMM, ndraws = 500) |&gt; \n  array_branch(margin=1) |&gt; \n   map_dfr( \n    function(yrep_iter) {\n      test  |&gt;\n        mutate(dist_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) |&gt;\n  mutate(iter = as.numeric(iter))\n\n\n\ndist_pred  |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(dist_pred, group = iter)) +\n  geom_line(alpha = .03, stat = 'density', color = 'blue') +\n  geom_density(data = test,\n               aes(dist,col=vb),\n               inherit.aes = FALSE,\n               size = 0.7) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('Deviation')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 1: Posterior Predictive distributions for Absolute Deviance. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\nvx_pred &lt;- \n  posterior_predict(e1_vxBMM, ndraws = 500) |&gt; \n  array_branch(margin=1) |&gt; \n   map_dfr( \n    function(yrep_iter) {\n      test  |&gt;\n        mutate(vx_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) |&gt;\n  mutate(iter = as.numeric(iter))\n\n\n\nvx_pred  |&gt;\n  filter(iter &lt; 100) %&gt;%\n  ggplot(aes(vx_pred, group = iter)) +\n  geom_line(alpha = .03, stat = 'density', color = 'blue') +\n  geom_density(data = test,\n               aes(vx,col=vb),\n               inherit.aes = FALSE,\n               size = 0.7) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('Vx')\n\n\n\n\n\n\n\nFigure 2: Posterior Predictive distributions for Vx. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\n\nEmpirical vs. Predicted\n\n{\nvx_pred  |&gt;\n  filter(iter &lt; 100)  |&gt; group_by(id,condit,vb,iter) |&gt;\n  summarise(vx_pred=mean(vx_pred)) %&gt;%\n  ggplot(aes(x=vb,y=vx_pred,fill=condit)) + \n  geom_flat_violin( position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,\n                   trim = FALSE, alpha = .5, colour = NA) +\n  # geom_point(aes(x = as.numeric(vb) - 0.15, y = vx_pred, colour = vb),\n  #            position = position_jitter(width = 0.05, height = 0),\n  #            size = 1, shape = 20) +\n  geom_boxplot(aes(x = vb, y = vx_pred, fill = condit),\n               outlier.shape = NA,\n               alpha = 0.5,\n               width = 0.1,\n               colour = \"black\") +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Predicted Vx\")  \n} / {\nvx_pred  |&gt;\n  filter(iter &lt; 2)  |&gt; group_by(id,condit,vb) |&gt;\n  summarise(vx=mean(vx)) %&gt;%\n  ggplot(aes(x=vb,y=vx,fill=condit)) + \n  geom_flat_violin( position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,\n                   trim = FALSE,\n                   alpha = .5,\n                   colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15,col=condit),\n             # position = position_jitter(width = 0.05),\n             position = position_jitter(width = 0.05, height = 0),\n             size = 1,\n             shape = 20) +\n  geom_boxplot(\n               outlier.shape = NA,\n               alpha = 0.5,\n               width = 0.1,\n               colour = \"black\") +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Vx\") \n}\n\nWarning: Using the `size` aesthetic with geom_polygon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\nFigure 3: Bayesian Mixed Model predictions vs. Empirical Predictions - X velocity\n\n\n\n\n\n\n\nDifferent Aggregations\n\nepId &lt;- dist_pred  |&gt;\n  filter(iter &lt; 2)  |&gt; group_by(id,condit,vb) |&gt;\n  summarise(dist=median(dist)) |&gt;\n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  geom_flat_violin(aes(fill=condit), position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,trim = FALSE, alpha = .5, colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15, col=condit),\n             position = position_jitter(width = 0.05, height = 0),\n             size = 1, shape = 20, alpha=.7) +\n  geom_boxplot(aes(x=vb,y=dist,fill=condit),\n               outlier.shape = NA,\n               alpha = 0.5, width = 0.1) +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Deviation - Subject level averaging\") \n\n\n\nepId \n\n\n\n\n\n\n\nFigure 4: E1. Distribution of Vx at Participant level\n\n\n\n\n\n\nepTrial &lt;- dist_pred  |&gt;\n  filter(iter &lt; 2)  |&gt; group_by(id,condit,vb) |&gt;\n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  geom_flat_violin(aes(fill=condit), position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,trim = FALSE, alpha = .5, colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15, col=condit),\n             position = position_jitter(width = 0.05, height = 0),\n             size = .5, shape = 20, alpha=.7) +\n  geom_boxplot(aes(x=vb,y=dist,fill=condit),\n               outlier.shape = NA,\n               alpha = 0.5, width = 0.1) +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Deviation - Raw Trial\") +\n   theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())\n\nepTrial\n\n\n\n\n\n\n\nFigure 5: E1. Distribution of Vx at Trial level\n\n\n\n\n\n\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")])))\n\ncSamp &lt;- e1_distBMM  |&gt; \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |&gt; \n  pairs() |&gt; gather_emmeans_draws()  |&gt;\n  group_by(contrast, .draw,bandInt) |&gt; summarise(value=mean(.value), n=n())\n\n\n ameBand &lt;- cSamp |&gt; ggplot(aes(x=value,y=\"\")) + \n  stat_halfeye() + \n  geom_vline(xintercept=0,alpha=.4)+\n  facet_wrap(~bandInt,ncol=1) + labs(x=\"Marginal Effect (Constant - Varied)\", y= NULL)+\n  ggtitle(\"Average Marginal Effect\")\n\nbothConditGM &lt;- e1_distBMM %&gt;%\n  epred_draws(newdata = new_data_grid,ndraws = 2000, re_formula = NA) |&gt;\n  ggplot(aes(x=.epred,y=\"Mean\",fill=condit)) + \n  stat_halfeye() +facet_wrap(~bandInt, ncol = 1) +\n  labs(x=\"Predicted Deviation\", y=NULL)+\n  ggtitle(\"Grand Means\") +theme(legend.position = \"bottom\")\n\n(bothConditGM | ameBand) + plot_layout(widths=c(2,1.0))\n\n\n\n\n\n\n\nFigure 6: E1. Predicted Means Per Condition and Band, and Average Marginal Effect (Constant - Varied)"
  },
  {
    "objectID": "Presentation/slides.html",
    "href": "Presentation/slides.html",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "",
    "text": "Click here to open the dissertation manuscript.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#slides",
    "href": "Presentation/slides.html#slides",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Slides ",
    "text": "Slides \nPresentation slides are available below. You can navigate these sides using the ← and → keys. Press m to display all commands (e.g., press o to switch to slide overview)\n\n Open slides in new window",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#variability",
    "href": "Presentation/slides.html#variability",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Variability",
    "text": "Variability\n\nVariation during training linked to improved transfer in numerous domains\n\nCategory learning, perceptual learning, education, visuomotor learning\nsometimes alternatively termed diversity, heterogenity, numerosity etc.\n\n\n\n\nWhat does variability mean in the context of learning interventions?\n\nHow spread out examples are in the task space\nThe number of unique items/problems experienced\nexposure to wider array of contexts/background conditions\n\n\n\n\n\nThe influence of variability has been studied across many different domains\nalthough sometimes referred\n\nvariability can refer to many different",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#common-empirical-patterns",
    "href": "Presentation/slides.html#common-empirical-patterns",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Common Empirical Patterns",
    "text": "Common Empirical Patterns\n\n\n\nTraining\n\nBoth training conditions complete the same number of training trials.\nVaried group has worse training performance.\n\n\n\n\n\n\nTesting\n\nTested from novel conditions.\nVaried group has better test performance\n\n\n\n\n\n\n\nhigher or lower variability can of course vary greatly between domains\nOne common manipulation in visuomotor learning is to have a group train with the lowest possible variability - constant\nThis is a fairly common pattern, at least to my reading. But….,",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#but-also-plenty-of-contradictory-results-and-complications",
    "href": "Presentation/slides.html#but-also-plenty-of-contradictory-results-and-complications",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "But also plenty of contradictory results and complications",
    "text": "But also plenty of contradictory results and complications\n\nCases where varied training doesn’t benefit generalization\nCases where more training variation results in worse outcomes\nCases where the influence of variation interacts with some other factor\n\ndifficulty\nprior knowledge\nFrequency effects, or amount of training/learning before testing\n\n\n\nPlenty of discrepancy in results, sometimes even for very similar tasks. - and lots of work remains to unravel when exactly variability is or isn’t helpful",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#overview",
    "href": "Presentation/slides.html#overview",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Overview",
    "text": "Overview\n\nProject 1\n\nVisuomotor projectile launching task\ntwo experiments\nBeneficial effect of variability\nInstance-based similarity model\n\n\n\nProject 2\n\nVisuomotor extrapolation task\nThree experiments\nEffect of variability is null or negative\nConnectionist model (ALM) and hybrid associative & rule model (EXAM)\n\n\n\nDissertations consists of 2 primary projects.\nEach within their own subdomain of visuomotor learning\nand each employing a distinct type of computational modeling",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-1-1",
    "href": "Presentation/slides.html#project-1-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1",
    "text": "Project 1\n\n\nAn instance-based model account of the benefits of varied practice in visuomotor skill1",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#theoretical-frameworks",
    "href": "Presentation/slides.html#theoretical-frameworks",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Theoretical Frameworks",
    "text": "Theoretical Frameworks\n\n\n\nDesirable Difficulties Framework (Bjork & Bjork, 2011) \n\nChallenge Point Framework (Guadagnoli & Lee, 2004)\n\n\n\nSchema Theory (Schmidt, 1975)\n\n\nSchmidt (1975) Bjork & Bjork (2011) Guadagnoli & Lee (2004)\n\n\n\nVariety of theoretical explanations have been proposed for the commonly observed effects\nIn visuomotor learning and most relevant for my work, Schema theory in paticular has been extremely influential, and seems to have inspired hundreds of studies\nUnlike the models I’ll be presenting today, at least to my knowledge none of these have ever been formally specified and actually fit to human data",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#issues-with-previous-research",
    "href": "Presentation/slides.html#issues-with-previous-research",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Issues with previous research",
    "text": "Issues with previous research\n\nAssumptions about what is encoded\nAssumptions about the formation of abstractions\nAggregation issues and similarity confounds\n\n\n\nSchema theory assumes that learners encode abstractions, or parameters, that enable generalization, and that variation is parituiclarly helpful\nfailures to consider nonlinear effects of learning\nfailure to consider that, through lieu of being exposed to broader coverage of the task space, varied training manipulations can often",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#designs-that-avoid-similarity-issue",
    "href": "Presentation/slides.html#designs-that-avoid-similarity-issue",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Designs that avoid similarity issue",
    "text": "Designs that avoid similarity issue\n\nKerr & Booth 1978\n\n\n\n\n\nBean bag tossing task\nConstant and varied conditions train from distinct positions\nBoth groups are tested from the position where the constant group trained\nImpressive demonstration of varied training outperforming constant training\n\n\n\n\nKerr & Booth (1978)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#experiment-1",
    "href": "Presentation/slides.html#experiment-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 1",
    "text": "Experiment 1\n\nConceptual replication of Kerr & Booth design\nAlso test positions novel to both groups\nValidate computerized visuomotor learning task",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#experiment-1-design",
    "href": "Presentation/slides.html#experiment-1-design",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 1 Design",
    "text": "Experiment 1 Design\n\n\n\n\n\n\nConstant trains from one position (760)\n\n200 trials\n\nVaried trains from two positions (610 and 910)\n\n100 trials per position\n\nBoth groups are tested from all three training positions, and a new position novel to both groups (835)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#hit-the-target-task",
    "href": "Presentation/slides.html#hit-the-target-task",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Hit The Target Task",
    "text": "Hit The Target Task\n\n\n\nVideo\n\n\nTraining Stage - 200 training trials with feedback. Constant groups trains from single position. Varied group practices from two positions.\nTransfer Stage - All subjects tested from both positions they were trained, and the positions trained by other group\nData recorded - For every throw, recorded the X velocity and Y velocity of ball at release\n\n\n\n\n\n\nUse mouse to flick the ball towards the target\nMust release from orange square\nSubjects have flexibility in terms of trajectory and release point inside the square\nNicety of computerized task - we record the precise details easily",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-1---experiment-1-results",
    "href": "Presentation/slides.html#project-1---experiment-1-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1 - Experiment 1 Results",
    "text": "Project 1 - Experiment 1 Results\n\n\n\n\nTraining\n\n\n\n\n\n\nTesting\n\n\n\n\n\n\nVaried training significantly better at the position that was novel for all subjects. Also better even at the position that the constant group trained at.\n4 different positions, the varied group trained from the easiest and the hardest position, they also interpolated the constant group.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#experiment-2-design",
    "href": "Presentation/slides.html#experiment-2-design",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 2 Design",
    "text": "Experiment 2 Design\n\n\ntask mostly the same - minor adjustments to barrier 6 different constant groups varied group doesn’t train on either the easiest or the hardest position\nlarger design to give us more data to work with for modelling",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-1---experiment-2-results",
    "href": "Presentation/slides.html#project-1---experiment-2-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1 - Experiment 2 Results",
    "text": "Project 1 - Experiment 2 Results\n\n\n\n\nTraining\n\n\n\n\n\n\nTesting\n\n\n\n\n\n\n\nThis time we can compare varied and constant training - by just comparing the varied group to the 2 constant groups that trained at the same locations\nHere we have all 6 of the constant groups collapsed together\nalso did more fine grained grained comparisons",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-1-computational-model",
    "href": "Presentation/slides.html#project-1-computational-model",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1 Computational Model",
    "text": "Project 1 Computational Model\n\nInstance encoding, and similarity-based generalization assumptions\n\nInstances refers to individual throws (x and y velocity)\nSimilarity refers to the distance between training throws and the solution space of each of the eventual testing positions.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-1-computational-model-1",
    "href": "Presentation/slides.html#project-1-computational-model-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 1 Computational Model",
    "text": "Project 1 Computational Model\n\n\n \n\n\nComputing Similarity\n\nEuclidean distance between each training throw, and each solution space\nSimilarity computed as a Gaussian decay function of distance, i.e. larger distances result in lower similarity\nEach participant gets their own similarity score for each of the 6 testing positions\n\nsimilarity score for a given testing position is the sum of the similarities between each training throw and the entire empirical solution space\n\n\n\n\nModel Definition\n\n\\(d_{i,j} = \\sqrt{(x_{Train_i}-x_{Solution_j})^2 + (y_{Train_i}-y_{Solution_j})^2 }\\)\n\\(Similarity_{I,J} = \\sum_{i=I}\\sum_{j=J} (e^{-c^\\cdot d^{p}_{i,j}})\\)\n\n\n\n\n\neach participant has all of their training throws compared to each of the 6 testing positions",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#non-linear-similarity",
    "href": "Presentation/slides.html#non-linear-similarity",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Non-linear similarity",
    "text": "Non-linear similarity",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#does-this-similarity-metric-work",
    "href": "Presentation/slides.html#does-this-similarity-metric-work",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Does this similarity metric work?",
    "text": "Does this similarity metric work?\n\nOur similarity measure does is a significant predictor of testing performance.\n\nRemains significant when controlling for training accuracy, and training condition.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#accounting-for-group-level-effect",
    "href": "Presentation/slides.html#accounting-for-group-level-effect",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Accounting for group-level effect",
    "text": "Accounting for group-level effect\n\n\nThe base version of the model fit a single \\(c\\) parameter\n\nthus assuming that everyone generalizes to the same degree\n\n\n\n\n\nWe next fit a 2-c version, separately optimizing for varied and constant groups, resulting in new 2-c similarity scores for each participant.\n\nThe optimal c value for the varied group is smaller than that of the constant group, indicative of broader generalization.\n\n\n\n\n\nWhen 2-c similarity is added to the linear model predicting testing performance as a function of condition, the effect of training condition is no longer significant.\n\nnot the case with the original 1-c similarities.\n\n\n\n\n\nThus the influence of varied training can be explained by an instance-based similarity model, IF one assumes flexibility in the generalization gradient.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#conclusions",
    "href": "Presentation/slides.html#conclusions",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nNew empirical support for a benefit of variability in visuomotor skill tasks\n\n\n\n\nSome support for the less common pattern observed by Kerr & Booth (1978)\n\n\n\n\nDemonstrated the utility of a similarity based approach in this domain",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2-2",
    "href": "Presentation/slides.html#project-2-2",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2",
    "text": "Project 2\n\nImpact of Training Variability on Visuomotor Function Learning and Extrapolation\n\nInfluence of varied practice in a function learning task\nExperiments 1, 2, and 3:\n\nTraining regimes and testing conditions\nLearning, discrimination, and extrapolation performance metrics",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---questions-and-goals",
    "href": "Presentation/slides.html#project-2---questions-and-goals",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Questions and Goals",
    "text": "Project 2 - Questions and Goals\nEmpirical - Design a task-space large enough to assess multiple degrees of extrapolation - Compare varied and constant generalization from several distinct distances from their nearest training condition\nModel-based - If variation does influence extrapolation, can an associative learning model with similarity-based activation provide a good account? - Can our modelling framework simultaneously account for both training and testing data? - Accounting for the full distribution of responses",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#hit-the-wall-task",
    "href": "Presentation/slides.html#hit-the-wall-task",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Hit The Wall Task",
    "text": "Hit The Wall Task\n\nVideo\n\nTarget velocity presented at top of screen\nParticipants attempt to “hit the wall” at the correct velocity\nFeedback during training - how many units above or below the target-band\nOnly the “x velocity” component of the throw is relevant to the task",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-1-design",
    "href": "Presentation/slides.html#project-2---experiment-1-design",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 1 Design",
    "text": "Project 2 - Experiment 1 Design\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n156 participants included in final analysis\nVaried group trains from 3 “velocity bands”, constant group from 1\nBoth groups complete same total number of training trials",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-1-results",
    "href": "Presentation/slides.html#project-2---experiment-1-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 1 Results",
    "text": "Project 2 - Experiment 1 Results\n\n\n \n\n \n\n\n\n\nTo compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 3. The main effect of training condition was not significant ( The extrapolation testing items had a significantly greater deviation than the training bands ( Most importantly, the interaction between training condition and band type was significant ( As shown in Figure 5, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant.\non. Most relevant to the issue of discrimination is the coefficient on the Band predictor ( Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The significant negative estimate for the interaction between slope and condition ( suggests that the discrimination was modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-2-design",
    "href": "Presentation/slides.html#project-2---experiment-2-design",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 2 Design",
    "text": "Project 2 - Experiment 2 Design\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining and Testing bands are in reversed order, relative to Experiment 1\n110 participants included in final analysis",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-2-results",
    "href": "Presentation/slides.html#project-2---experiment-2-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 2 Results",
    "text": "Project 2 - Experiment 2 Results\n\n\n \n\n \n\n\n\nThe analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition ( However, the interaction between training condition and band type was significant with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-3",
    "href": "Presentation/slides.html#project-2---experiment-3",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 3",
    "text": "Project 2 - Experiment 3\n\nOrdinal Feedback\n\ndirectional feedback indicating too high, too low, or correct\n\nBoth Original (Experiment 1), and reverse (Experiment 2) orders included\n195 participants included in final analysis",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---experiment-3-results",
    "href": "Presentation/slides.html#project-2---experiment-3-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Experiment 3 Results",
    "text": "Project 2 - Experiment 3 Results",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#experiment-3---accuracy",
    "href": "Presentation/slides.html#experiment-3---accuracy",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 3 - Accuracy",
    "text": "Experiment 3 - Accuracy",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#experiment-3---discrimination",
    "href": "Presentation/slides.html#experiment-3---discrimination",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Experiment 3 - Discrimination",
    "text": "Experiment 3 - Discrimination",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#the-associative-learning-model-alm",
    "href": "Presentation/slides.html#the-associative-learning-model-alm",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "The Associative Learning Model ALM",
    "text": "The Associative Learning Model ALM\n\n\n\n\n\nTwo layer network - adapted from ALCOVE (Kruschke (1992))\nInput layer node for each stimulus, output node for each response\nInput nodes activate as a function of their Gaussian similarity to the stimulus\nWeights udpated via delta rule - error driven learning\nProvides good account of human learning data, and interpolation performance, but struggles with extrapolation (DeLosh et al., 1997)",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#extrapolation---association-model-exam",
    "href": "Presentation/slides.html#extrapolation---association-model-exam",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Extrapolation - Association Model (EXAM)",
    "text": "Extrapolation - Association Model (EXAM)\n\nExtension to ALM to account for human extrapolation behavior\nWhen a novel stimulus is presented, EXAM assumes the nearest 2-3 prior examples are retrieved, and used to compute a slope\nThe slope is used to adjust the ALM response\n\n\n\n\n\n\n\n\n\n\nEXAM Response Generation\n\n\n\n\n\nSlope Computation\n\\(S = \\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---model-fitting-procedure",
    "href": "Presentation/slides.html#project-2---model-fitting-procedure",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Model Fitting Procedure",
    "text": "Project 2 - Model Fitting Procedure\n\nApproximate Bayesian Computation (ABC)\n\n\n\n\n\nsimulation based parameter estimation (Kangasrääsiö et al., 2019; Turner & Van Zandt, 2012)\nUseful for models with unknown likelihood functions (e.g. many neural network and drift diffusion models)\nfull distribution of plausible model predictions for each participant",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-2---model-fitting-procedure-1",
    "href": "Presentation/slides.html#project-2---model-fitting-procedure-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project 2 - Model Fitting Procedure",
    "text": "Project 2 - Model Fitting Procedure\n\nModel Fitting Approach\n\nTwo parameters for both ALM and EXAM\n\ngeneralization parameter: \\(c\\)\nlearning rate parameter: \\(lr\\)\n\nALM and EXAM fit separately to each individual participant\nRejection based ABC used to obtain 200 posterior samples, per participant, per model\n\ni.e. 200 plausible values of \\(c\\) and \\(lr\\)\n\nAll models fit with three different approaches\n\nFit to only the training data\nFit to both training and testing data\nFit to only testing data",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#modelling-results",
    "href": "Presentation/slides.html#modelling-results",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Modelling Results",
    "text": "Modelling Results\n\nPosterior Predictive Distribution\n\n\nEmpirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#modelling-results-1",
    "href": "Presentation/slides.html#modelling-results-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Modelling Results",
    "text": "Modelling Results\n\nBest fitting models per participant\n\n\nDifference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#model-comparison---experiment-1",
    "href": "Presentation/slides.html#model-comparison---experiment-1",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Model Comparison - Experiment 1",
    "text": "Model Comparison - Experiment 1\n\n\nFigure 21: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g., EXAM Constant) tended to have smaller model residuals.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#model-comparison---experiment-2-and-3",
    "href": "Presentation/slides.html#model-comparison---experiment-2-and-3",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Model Comparison - Experiment 2 and 3",
    "text": "Model Comparison - Experiment 2 and 3\n\n\nConditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiment 2 and 3 data. Experiment 3 also includes a control for the order of training vs. testing bands (original order vs. reverse order).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#general-discussion",
    "href": "Presentation/slides.html#general-discussion",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "General Discussion",
    "text": "General Discussion",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#summary",
    "href": "Presentation/slides.html#summary",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Summary",
    "text": "Summary\n\nProject Comparison:\n\nHTT (Project 1): Varied training led to superior testing performance.\nHTW (Project 2): Varied training led to poorer performance.\n\nKey Findings:\n\nHTT: Varied group outperformed constant group in both training and testing.\nHTW: Varied group exhibited poorer performance across training and testing stages.\n\nModeling Approaches:\n\nHTT: IGAS model quantifies similarity between training and testing conditions.\nHTW: ALM and EXAM models fit to individual participant data.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#project-comparison",
    "href": "Presentation/slides.html#project-comparison",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Project Comparison",
    "text": "Project Comparison\n\nTask Differences:\n\nHTT:\n\nComplex parabolic trajectory.\nBoth x and y velocities relevant.\nPerceptually salient varied conditions.\n\nHTW:\n\nSimple 1D force mapping.\nOnly x velocity relevant.\nLess perceptually salient varied conditions.\n\n\nTask Complexity:\n\nHTT: More complex task space with irregularities.\nHTW: Smooth, linear mapping between velocity and feedback.\n\n\n\nthe task spaces were different. HTT had a more complex task space with irregularities introduced by the barrier, making the learning environment less predictable. HTW had a smoother, linear mapping between velocity and feedback, providing a more predictable learning environment.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#conclusion",
    "href": "Presentation/slides.html#conclusion",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Conclusion",
    "text": "Conclusion\n\nTask-specific characteristics are crucial in determining the benefits of varied training.\nCombining empirical research with computational modeling enhances understanding of learning and generalization.\n\n\n\nThe contrasting results highlight the importance of considering task characteristics when designing experiments to assess the influence of training interventions.\nCombining empirical and computational modeling approaches can help uncover the cognitive mechanisms supporting learning and generalization.\nFuture research should continue to investigate the interplay between task demands, training manipulations, and individual differences to optimize educational and training outcomes.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#references",
    "href": "Presentation/slides.html#references",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "References",
    "text": "References\n\n\nBjork, E. L., & Bjork, R. A. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society, 2, 59–68.\n\n\nDeLosh, E. L., McDaniel, M. A., & Busemeyer, J. R. (1997). Extrapolation: The Sine Qua Non for Abstraction in Function Learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(4), 19. https://doi.org/10.1037/0278-7393.23.4.968\n\n\nGorman, T. E., & Goldstone, R. L. (2022). An instance-based model account of the benefits of varied practice in visuomotor skill. Cognitive Psychology, 137, 101491. https://doi.org/10.1016/j.cogpsych.2022.101491\n\n\nGuadagnoli, M. A., & Lee, T. D. (2004). Challenge Point: A Framework for Conceptualizing the Effects of Various Practice Conditions in Motor Learning. Journal of Motor Behavior, 36(2), 212–224. https://doi.org/10.3200/JMBR.36.2.212-224\n\n\nKangasrääsiö, A., Jokinen, J. P. P., Oulasvirta, A., Howes, A., & Kaski, S. (2019). Parameter Inference for Computational Cognitive Models with Approximate Bayesian Computation. Cognitive Science, 43(6), e12738. https://doi.org/10.1111/cogs.12738\n\n\nKerr, R., & Booth, B. (1978). Specific and varied practice of motor skill. Perceptual and Motor Skills, 46(2), 395–401. https://doi.org/10.1177/003151257804600201\n\n\nKruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of Category Learning. Psychological Review, 99(1). https://doi.org/10.1037/0033-295X.99.1.22\n\n\nSchmidt, R. A. (1975). A schema theory of discrete motor skill learning. Psychological Review, 82(4), 225–260. https://doi.org/10.1037/h0076770\n\n\nTurner, B. M., & Van Zandt, T. (2012). A tutorial on approximate Bayesian computation. Journal of Mathematical Psychology, 56(2), 69–85. https://doi.org/10.1016/j.jmp.2012.02.005",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#extras",
    "href": "Presentation/slides.html#extras",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Extras",
    "text": "Extras",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#manuscript",
    "href": "Presentation/slides.html#manuscript",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Manuscript",
    "text": "Manuscript",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#alm",
    "href": "Presentation/slides.html#alm",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "ALM",
    "text": "ALM\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#exam",
    "href": "Presentation/slides.html#exam",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "EXAM",
    "text": "EXAM\n\n\n\n\n\n\n\n\n\nEXAM Response Generation\n\n\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\nSlope Computation\n\\(S = \\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#comparison-.scrollable-.unnumbered-.unlisted-.smaller",
    "href": "Presentation/slides.html#comparison-.scrollable-.unnumbered-.unlisted-.smaller",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Comparison {.scrollable .unnumbered .unlisted, .smaller}",
    "text": "Comparison {.scrollable .unnumbered .unlisted, .smaller}\n\n\n\n\n\n\n\n\nDimension\nHTT (Project 1)\nHTW (Project 2)\n\n\n\n\nTask Description\nProjectile launching to hit a target\nProjectile launching to hit wall at a specific velocity\n\n\nTask Complexity\nMore complex parabolic trajectory, both x and y velocities relevant to outcome\nSimpler 1D mapping of force to outcome. Only x velocity is relevant.\n\n\nTask Space\nMore complex: xy velocity combinations closer to the solution space may still result in worse feedback due to striking the barrier.\nSimpler: smooth, linear mapping between velocity and feedback.\n\n\nPerceptual salience of Varied Conditions\nVaried conditions (# of throwing distances) are perceptually distinct, i.e. salient differences in distance between launching box and target.\nVaried conditions (# of velocity bands) are less salient - only difference is the numeral displayed on screen.\n\n\nTesting Feedback\nTesting always included feedback\nPrimary testing stage had no feedback.\n\n\nPotential for Learning during Testing\nLimited potential for learning during testing due to feedback.\nSome potential for learning during no-feedback testing by observing ball trajectory.\n\n\nTraining Experience\nVaried group gets half as much experience on any one position as the constant group.\nVaried group gets 1/3 as much experience on any one velocity band as the constant group.\n\n\nTesting Structure\nRandom interleaving of trained/transfer testing distances.\nBlocked structure, separately testing trained vs extrapolation testing bands.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides.html#footnotes",
    "href": "Presentation/slides.html#footnotes",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNow published in Cognitive Psychology - (Gorman & Goldstone, 2022)↩︎",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "Presentation/slides_revealjs.html#references",
    "href": "Presentation/slides_revealjs.html#references",
    "title": "The Role of Variability in Learning Generalization: A Computational Modeling Approach",
    "section": "References",
    "text": "References\n\n\nBjork, E. L., & Bjork, R. A. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. Psychology and the Real World: Essays Illustrating Fundamental Contributions to Society, 2, 59–68.\n\n\nDeLosh, E. L., McDaniel, M. A., & Busemeyer, J. R. (1997). Extrapolation: The Sine Qua Non for Abstraction in Function Learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(4), 19. https://doi.org/10.1037/0278-7393.23.4.968\n\n\nGorman, T. E., & Goldstone, R. L. (2022). An instance-based model account of the benefits of varied practice in visuomotor skill. Cognitive Psychology, 137, 101491. https://doi.org/10.1016/j.cogpsych.2022.101491\n\n\nGuadagnoli, M. A., & Lee, T. D. (2004). Challenge Point: A Framework for Conceptualizing the Effects of Various Practice Conditions in Motor Learning. Journal of Motor Behavior, 36(2), 212–224. https://doi.org/10.3200/JMBR.36.2.212-224\n\n\nKangasrääsiö, A., Jokinen, J. P. P., Oulasvirta, A., Howes, A., & Kaski, S. (2019). Parameter Inference for Computational Cognitive Models with Approximate Bayesian Computation. Cognitive Science, 43(6), e12738. https://doi.org/10.1111/cogs.12738\n\n\nKerr, R., & Booth, B. (1978). Specific and varied practice of motor skill. Perceptual and Motor Skills, 46(2), 395–401. https://doi.org/10.1177/003151257804600201\n\n\nKruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of Category Learning. Psychological Review, 99(1). https://doi.org/10.1037/0033-295X.99.1.22\n\n\nSchmidt, R. A. (1975). A schema theory of discrete motor skill learning. Psychological Review, 82(4), 225–260. https://doi.org/10.1037/h0076770\n\n\nTurner, B. M., & Van Zandt, T. (2012). A tutorial on approximate Bayesian computation. Journal of Mathematical Psychology, 56(2), 69–85. https://doi.org/10.1016/j.jmp.2012.02.005"
  },
  {
    "objectID": "Presentation/embed_slides.html",
    "href": "Presentation/embed_slides.html",
    "title": "Dissertation",
    "section": "",
    "text": "Click here to open the dissertation manuscript."
  },
  {
    "objectID": "Presentation/embed_slides.html#slides",
    "href": "Presentation/embed_slides.html#slides",
    "title": "Dissertation",
    "section": "Slides ",
    "text": "Slides \nPresentation slides are available below. You can navigate these sides using the ← and → keys. Press m to display all commands (e.g., press o to switch to slide overview)\n\n Open slides in new window"
  },
  {
    "objectID": "Sections/Introduction.html",
    "href": "Sections/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Varied training has been shown to influence learning in a wide array of different tasks and domains, including categorization (Hahn et al., 2005; Maddox & Filoteo, 2011; Morgenstern et al., 2019; Nosofsky et al., 2019; Plebanek & James, 2021; Posner & Keele, 1968), language learning (Brekelmans et al., 2022; Jones & Brandt, 2020; Perry et al., 2010; Twomey et al., 2018; Wonnacott et al., 2012), anagram completion (Goode et al., 2008), perceptual learning (Lovibond et al., 2020; Manenti et al., 2023; Robson et al., 2022; Zaman et al., 2021), trajectory extrapolation (Fulvio et al., 2014), cognitive control tasks (Moshon-Cohen et al., 2024; Sabah et al., 2019), associative learning (Fan et al., 2022; Lee et al., 2019; Livesey & McLaren, 2019; Prada & Garcia-Marques, 2020; Reichmann et al., 2023), visual search (George & Egner, 2021; Gonzalez & Madhavan, 2011; Kelley & Yantis, 2009), voice identity learning (Lavan et al., 2019), face recognition (Burton et al., 2016; Honig et al., 2022; Menon et al., 2015), the perception of social group heterogeneity (Gershman & Cikara, 2023; Konovalova & Le Mens, 2020; Linville & Fischer, 1993; Park & Hastie, 1987) , simple motor learning (Braun et al., 2009; Kerr & Booth, 1978; Roller et al., 2001; Willey & Liu, 2018a), sports training (Breslin et al., 2012; Green et al., 1995; North et al., 2019), and complex skill learning (Hacques et al., 2022; Huet et al., 2011; Seow et al., 2019). See Czyż (2021) or Raviv et al. (2022) for more detailed reviews.\nResearch on the effects of varied training typically manipulates variability in one of two ways. In the first approach, a high variability group is exposed to a greater number of unique instances during training, while a low variability group receives fewer unique instances with more repetitions. Alternatively, both groups may receive the same number of unique instances, but the high variability group’s instances are more widely distributed or spread out in the relevant psychological space, while the low variability group’s instances are clustered more tightly together. Researchers then compare the training groups in terms of their performance during the training phase, as well as their generalization performance during a testing phase. Researchers usually compare the performance of the two groups during both the training phase and a subsequent testing phase. The primary theoretical interest is often to assess the influence of training variability on generalization to novel testing items or conditions. However, the test may also include some or all of the items that were used during the training stage, allowing for an assessment of whether the variability manipulation influenced the learning of the trained items themselves, or to easily measure how much performance degrades as a function of how far away testing items are from the training items.\nThe influence of training variability has received a large amount of attention in the domain of sensorimotor skill learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes that learners possess general motor programs for a class of movements (e.g., an underhand throw). When called up for use motor programs are parameterized by schema rules which determine how the motor program is parameterized or scaled to the particular demands of the current task. Schema theory predicts that variable training facilitates the formation of more robust schemas, which will result in improved generalization or transfer. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, studies using a projectile throwing task might assign participants to either constant training that practice throwing from a single location, or to a varied group that throws from multiple locations. Following training, both groups are then tested from novel throwing locations (Pacheco & Newell, 2018; Pigott & Shapiro, 1984; Willey & Liu, 2018a; Wulf, 1991).\nOne of the earliest and still often cited investigations of Schmidt’s benefits of variability hypothesis was the work of Kerr & Booth (1978). Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blindfolded for testing and did not receive trial by trial feedback in this stage. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. This result provides particularly strong evidence for the benefits of varied practice, as the varied group outperformed the constant group even when tested at the “home-turf” distance that the constant group had exclusively practiced. A similar pattern of results was observed in another study wherein varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket (Green et al., 1995). One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. However, as is the case with many of the patterns commonly observed in the “benefits of variability” literature, the pattern wherein the varied group outperfroms the constant group even from the constants group’s home turf has not been consistently replicated. One recent study attempted a near replication of the Kerr & Booth study (Willey & Liu, 2018b), having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). This study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups. However, this study diverged from the original in that the participants were adults; and the amount of training was much greater (20 sessions with 60 practice trials each, spread out over 5-7 weeks).\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters (Catalano & Kleiner, 1984; Chua et al., 2019; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976), or for a single constant group to train from just one of the conditions experienced by the varied participants (Pigott & Shapiro, 1984; Roller et al., 2001; Wrisberg & McLean, 1984; Wrisberg & Mead, 1983). A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition (Wrisberg et al., 1987; Wulf & Schmidt, 1997). Of particular relevance to the current work is the early study of Catalano & Kleiner (1984), as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions – 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\nBenefits of varied training have also been observed in many studies outside of the sensorimotor domain. Goode et al. (2008) trained participants to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g., LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had three prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups – solving two more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution – further suggesting that there may not have been a strong identity advantage in this task.\nIn the domain of category learning, the constant vs. varied comparison is much less suitable. Instead, researchers will typically employ designs where all training groups encounter numerous stimuli, but one group experiences a greater number of unique exemplars (Brunstein & Gonzalez, 2011; Doyle & Hourihan, 2016; Hosch et al., 2023; Nosofsky et al., 2019; Wahlheim et al., 2012), or designs where the number of unique training exemplars is held constant, but one group trains with items that are more dispersed, or spread out across the category space (Bowman & Zeithamova, 2020; Homa & Vosburgh, 1976; Hu & Nosofsky, 2024; Maddox & Filoteo, 2011; Posner & Keele, 1968).\nMuch of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns (Homa & Vosburgh, 1976; Posner & Keele, 1968). A seminal study by Posner & Keele (1968) trained participants to categorize artificial dot patterns, manipulating whether learners were trained with low variability examples clustered close to the category prototypes (i.e. low distortion training patterns), or higher-variability patterns spread further away from the prototype (i.e. high-distortion patterns). Participants that received training on more highly-distorted items showed superior generalization to novel high distortion patterns in the subsequent testing phase. It should be noted that unlike the sensorimotor studies discussed earlier, the Posner & Keele (1968) study did not present low-varied and high-varied participants with an equal number of training rathers, but instead had participants remain in the training stage of the experiment until they reached a criterion level of performance. This train-until-criterion procedure led to the high-variability condition participants tending to complete a larger number of training trials before switching to the testing stage. More recent work (Hu & Nosofsky, 2024), also used dot pattern categories, but matched the number of training trials across conditions. Under this procedure, higher-variability participants tended to reach lower levels of performance by the end of the training stage. The results in the testing phase were the opposite of Posner & Keele (1968), with the low-variability training group showing superior generalization to novel high-distortion patterns (as well as generalization to novel patterns of low or medium distortion levels). However, whether this discrepancy is solely a result of the different training procedures is unclear, as the studies also differed in the nature of the prototype patterns used. Posner & Keele (1968) utilized simpler, recognizable prototypes (e.g., a triangle, the letter M, the letter F), while Hu & Nosofsky (2024) employed random prototype patterns.\nRecent studies have also begun utilizing more complex or realistic sitmuli when assessing the influence of variability on category learning. Wahlheim et al. (2012) conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address an unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization (Nosofsky et al., 2019). For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions.\nA distinct sub-literature within the category learning domain has examined how the variability or dispersion of the categories themselves influences generalization to ambiguous regions of the category space (e.g., the region between the two categories). The general approach is to train participants with examples from a high variability category and a low variability category. Participants are then tested with novel items located within ambiguous regions of the category space which allow the experimenters to assess whether the difference in category variability influenced how far participants generalize the category boundaries. Cohen et al. (2001) conducted two experiments with this basic paradigm. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into the category with greater variation.\nFurther observations of widened generalization following varied training have since been observed in numerous investigations (Hahn et al., 2005; Hosch et al., 2023; Hsu & Griffiths, 2010; Perlman et al., 2012; Sakamoto et al., 2008; but see Stewart & Chater, 2002; Yang & Wu, 2014; and Seitz et al., 2023). The results of Sakamoto et al. (2008) are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\nThe work of Hahn et al. (2005), is also of particular interest to the present work. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with “neither”, in addition to responses to the two training categories. The “neither” response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The author compared a variety of similarity-based models based around the Generalized Context Model (Nosofsky, 1986) to account for their results, manipulating whether a response-bias or similarity-scaling parameter was fit separately between variability conditions. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improve significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items. This model accounted for both the extended generalization gradients of the varied participants, and also for their poorer performance in a recognition condition.\nVariability has also been examined in the learning of higher-order linguistic categories (Perry et al., 2010). In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the “tight” condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. In the testing phase, the varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category. More interesting was the superior performance of the varied group on a higher order generalization task – such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.\nOf course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation (DeLosh et al., 1997; Sinkeviciute et al., 2019; Van Rossum, 1990; Wrisberg et al., 1987), and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials, or the type of transfer being measured (Berniker et al., 2014; Braithwaite & Goldstone, 2015; Hahn et al., 2005; Lavan et al., 2019; North et al., 2019; Sadakata & McQueen, 2014; Zaman et al., 2021).\nIn an example of a more complex influence of training variation, (Braithwaite & Goldstone, 2015) trained participants on example problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g., all involving people selecting objects) or in which the surface features were varied between examples (e.g., people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants – such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples. Interactions between prior experience and the influence of varied training have also been observed in sensorimotor learning (Del Rey et al., 1982; Guadagnoli et al., 1999). Del Rey et al. (1982) recruited participants who self-reported either extensive, or very little experience with athletic activities, and then trained participants on a coincident timing task under with either a single constant training velocity, with one of several varied training procedures. Unsurprisingly, athlete participants had superior performance during training, regardless of condition, and training performance was superior for all subjects in the constant group. Of greater interest is the pattern of testing results from novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received variable training. Non-athletes showed the opposite pattern, with superior performance for those who had constant training."
  },
  {
    "objectID": "Sections/Introduction.html#varied-training-and-generalization",
    "href": "Sections/Introduction.html#varied-training-and-generalization",
    "title": "Introduction",
    "section": "",
    "text": "Varied training has been shown to influence learning in a wide array of different tasks and domains, including categorization (Hahn et al., 2005; Maddox & Filoteo, 2011; Morgenstern et al., 2019; Nosofsky et al., 2019; Plebanek & James, 2021; Posner & Keele, 1968), language learning (Brekelmans et al., 2022; Jones & Brandt, 2020; Perry et al., 2010; Twomey et al., 2018; Wonnacott et al., 2012), anagram completion (Goode et al., 2008), perceptual learning (Lovibond et al., 2020; Manenti et al., 2023; Robson et al., 2022; Zaman et al., 2021), trajectory extrapolation (Fulvio et al., 2014), cognitive control tasks (Moshon-Cohen et al., 2024; Sabah et al., 2019), associative learning (Fan et al., 2022; Lee et al., 2019; Livesey & McLaren, 2019; Prada & Garcia-Marques, 2020; Reichmann et al., 2023), visual search (George & Egner, 2021; Gonzalez & Madhavan, 2011; Kelley & Yantis, 2009), voice identity learning (Lavan et al., 2019), face recognition (Burton et al., 2016; Honig et al., 2022; Menon et al., 2015), the perception of social group heterogeneity (Gershman & Cikara, 2023; Konovalova & Le Mens, 2020; Linville & Fischer, 1993; Park & Hastie, 1987) , simple motor learning (Braun et al., 2009; Kerr & Booth, 1978; Roller et al., 2001; Willey & Liu, 2018a), sports training (Breslin et al., 2012; Green et al., 1995; North et al., 2019), and complex skill learning (Hacques et al., 2022; Huet et al., 2011; Seow et al., 2019). See Czyż (2021) or Raviv et al. (2022) for more detailed reviews.\nResearch on the effects of varied training typically manipulates variability in one of two ways. In the first approach, a high variability group is exposed to a greater number of unique instances during training, while a low variability group receives fewer unique instances with more repetitions. Alternatively, both groups may receive the same number of unique instances, but the high variability group’s instances are more widely distributed or spread out in the relevant psychological space, while the low variability group’s instances are clustered more tightly together. Researchers then compare the training groups in terms of their performance during the training phase, as well as their generalization performance during a testing phase. Researchers usually compare the performance of the two groups during both the training phase and a subsequent testing phase. The primary theoretical interest is often to assess the influence of training variability on generalization to novel testing items or conditions. However, the test may also include some or all of the items that were used during the training stage, allowing for an assessment of whether the variability manipulation influenced the learning of the trained items themselves, or to easily measure how much performance degrades as a function of how far away testing items are from the training items.\nThe influence of training variability has received a large amount of attention in the domain of sensorimotor skill learning. Much of this research has been influenced by the work of Schmidt (1975), who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes that learners possess general motor programs for a class of movements (e.g., an underhand throw). When called up for use motor programs are parameterized by schema rules which determine how the motor program is parameterized or scaled to the particular demands of the current task. Schema theory predicts that variable training facilitates the formation of more robust schemas, which will result in improved generalization or transfer. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, studies using a projectile throwing task might assign participants to either constant training that practice throwing from a single location, or to a varied group that throws from multiple locations. Following training, both groups are then tested from novel throwing locations (Pacheco & Newell, 2018; Pigott & Shapiro, 1984; Willey & Liu, 2018a; Wulf, 1991).\nOne of the earliest and still often cited investigations of Schmidt’s benefits of variability hypothesis was the work of Kerr & Booth (1978). Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blindfolded for testing and did not receive trial by trial feedback in this stage. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. This result provides particularly strong evidence for the benefits of varied practice, as the varied group outperformed the constant group even when tested at the “home-turf” distance that the constant group had exclusively practiced. A similar pattern of results was observed in another study wherein varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket (Green et al., 1995). One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. However, as is the case with many of the patterns commonly observed in the “benefits of variability” literature, the pattern wherein the varied group outperfroms the constant group even from the constants group’s home turf has not been consistently replicated. One recent study attempted a near replication of the Kerr & Booth study (Willey & Liu, 2018b), having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). This study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups. However, this study diverged from the original in that the participants were adults; and the amount of training was much greater (20 sessions with 60 practice trials each, spread out over 5-7 weeks).\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters (Catalano & Kleiner, 1984; Chua et al., 2019; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976), or for a single constant group to train from just one of the conditions experienced by the varied participants (Pigott & Shapiro, 1984; Roller et al., 2001; Wrisberg & McLean, 1984; Wrisberg & Mead, 1983). A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition (Wrisberg et al., 1987; Wulf & Schmidt, 1997). Of particular relevance to the current work is the early study of Catalano & Kleiner (1984), as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions – 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\nBenefits of varied training have also been observed in many studies outside of the sensorimotor domain. Goode et al. (2008) trained participants to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g., LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had three prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups – solving two more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution – further suggesting that there may not have been a strong identity advantage in this task.\nIn the domain of category learning, the constant vs. varied comparison is much less suitable. Instead, researchers will typically employ designs where all training groups encounter numerous stimuli, but one group experiences a greater number of unique exemplars (Brunstein & Gonzalez, 2011; Doyle & Hourihan, 2016; Hosch et al., 2023; Nosofsky et al., 2019; Wahlheim et al., 2012), or designs where the number of unique training exemplars is held constant, but one group trains with items that are more dispersed, or spread out across the category space (Bowman & Zeithamova, 2020; Homa & Vosburgh, 1976; Hu & Nosofsky, 2024; Maddox & Filoteo, 2011; Posner & Keele, 1968).\nMuch of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns (Homa & Vosburgh, 1976; Posner & Keele, 1968). A seminal study by Posner & Keele (1968) trained participants to categorize artificial dot patterns, manipulating whether learners were trained with low variability examples clustered close to the category prototypes (i.e. low distortion training patterns), or higher-variability patterns spread further away from the prototype (i.e. high-distortion patterns). Participants that received training on more highly-distorted items showed superior generalization to novel high distortion patterns in the subsequent testing phase. It should be noted that unlike the sensorimotor studies discussed earlier, the Posner & Keele (1968) study did not present low-varied and high-varied participants with an equal number of training rathers, but instead had participants remain in the training stage of the experiment until they reached a criterion level of performance. This train-until-criterion procedure led to the high-variability condition participants tending to complete a larger number of training trials before switching to the testing stage. More recent work (Hu & Nosofsky, 2024), also used dot pattern categories, but matched the number of training trials across conditions. Under this procedure, higher-variability participants tended to reach lower levels of performance by the end of the training stage. The results in the testing phase were the opposite of Posner & Keele (1968), with the low-variability training group showing superior generalization to novel high-distortion patterns (as well as generalization to novel patterns of low or medium distortion levels). However, whether this discrepancy is solely a result of the different training procedures is unclear, as the studies also differed in the nature of the prototype patterns used. Posner & Keele (1968) utilized simpler, recognizable prototypes (e.g., a triangle, the letter M, the letter F), while Hu & Nosofsky (2024) employed random prototype patterns.\nRecent studies have also begun utilizing more complex or realistic sitmuli when assessing the influence of variability on category learning. Wahlheim et al. (2012) conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address an unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization (Nosofsky et al., 2019). For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions.\nA distinct sub-literature within the category learning domain has examined how the variability or dispersion of the categories themselves influences generalization to ambiguous regions of the category space (e.g., the region between the two categories). The general approach is to train participants with examples from a high variability category and a low variability category. Participants are then tested with novel items located within ambiguous regions of the category space which allow the experimenters to assess whether the difference in category variability influenced how far participants generalize the category boundaries. Cohen et al. (2001) conducted two experiments with this basic paradigm. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into the category with greater variation.\nFurther observations of widened generalization following varied training have since been observed in numerous investigations (Hahn et al., 2005; Hosch et al., 2023; Hsu & Griffiths, 2010; Perlman et al., 2012; Sakamoto et al., 2008; but see Stewart & Chater, 2002; Yang & Wu, 2014; and Seitz et al., 2023). The results of Sakamoto et al. (2008) are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\nThe work of Hahn et al. (2005), is also of particular interest to the present work. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with “neither”, in addition to responses to the two training categories. The “neither” response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The author compared a variety of similarity-based models based around the Generalized Context Model (Nosofsky, 1986) to account for their results, manipulating whether a response-bias or similarity-scaling parameter was fit separately between variability conditions. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improve significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items. This model accounted for both the extended generalization gradients of the varied participants, and also for their poorer performance in a recognition condition.\nVariability has also been examined in the learning of higher-order linguistic categories (Perry et al., 2010). In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the “tight” condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. In the testing phase, the varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category. More interesting was the superior performance of the varied group on a higher order generalization task – such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.\nOf course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation (DeLosh et al., 1997; Sinkeviciute et al., 2019; Van Rossum, 1990; Wrisberg et al., 1987), and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials, or the type of transfer being measured (Berniker et al., 2014; Braithwaite & Goldstone, 2015; Hahn et al., 2005; Lavan et al., 2019; North et al., 2019; Sadakata & McQueen, 2014; Zaman et al., 2021).\nIn an example of a more complex influence of training variation, (Braithwaite & Goldstone, 2015) trained participants on example problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g., all involving people selecting objects) or in which the surface features were varied between examples (e.g., people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants – such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples. Interactions between prior experience and the influence of varied training have also been observed in sensorimotor learning (Del Rey et al., 1982; Guadagnoli et al., 1999). Del Rey et al. (1982) recruited participants who self-reported either extensive, or very little experience with athletic activities, and then trained participants on a coincident timing task under with either a single constant training velocity, with one of several varied training procedures. Unsurprisingly, athlete participants had superior performance during training, regardless of condition, and training performance was superior for all subjects in the constant group. Of greater interest is the pattern of testing results from novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received variable training. Non-athletes showed the opposite pattern, with superior performance for those who had constant training."
  },
  {
    "objectID": "Sections/Introduction.html#existing-theoretical-frameworks",
    "href": "Sections/Introduction.html#existing-theoretical-frameworks",
    "title": "Introduction",
    "section": "Existing Theoretical Frameworks",
    "text": "Existing Theoretical Frameworks\nA number of theoretical frameworks have been proposed to conceptually explain the effects of varied training on learning and generalization. Schema theory (described in more detail above), posts that varied practice leads to the formation of more flexible motor schemas, which then facilitate generalization (Schmidt, 1975). The desirable difficulties framework (Bjork & Bjork, 2011; Soderstrom & Bjork, 2015) proposes that variable practice conditions may impair initial performance but then enhance longer-term retention and transfer. Similarly, the challenge point framework (Guadagnoli & Lee, 2004) contends that training variation induces optimal learning occurs insofar as it causes the difficulty of practice tasks to be appropriately matched to the learner’s capabilities, but may also be detrimental if the amount of variation causes the task to be too difficult.\nWhile these frameworks offer valuable conceptual accounts, there has been a limited application of computational modeling efforts aimed at quantitatively assessing and comparing the learning and generalization mechanisms which may be underlying the influence of variability in visuomotor skill learning. In contrast, the effects of variability have received more formal computational treatment in other domains, such as category learning Hu & Nosofsky (2024), language learning (Jones & Brandt, 2020), and function learning (DeLosh et al., 1997). A primary goal of the current dissertation is to to address this gap by adapting and applying modeling approaches from these other domains to investigate the effects of training variability in visuomotor skill learning and function learning tasks."
  },
  {
    "objectID": "Sections/Introduction.html#the-current-work",
    "href": "Sections/Introduction.html#the-current-work",
    "title": "Introduction",
    "section": "The current work",
    "text": "The current work\nThe overarching purpose of this dissertation is to investigate the effects of training variability on learning and generalization within visuomotor skill learning and function learning. Our investigation is structured into two main projects, each employing distinct experimental paradigms and computational modeling frameworks to elucidate how and when variability in training enhances or impedes subsequent generalization.\nIn Project 1, we investigated the influence of varied practice in a simple visuomotor projectile launching task. Experiments 1 and 2 compared the performance of constant and varied training groups to assess potential benefits of variability on transfer to novel testing conditions. To account for the observed empirical effects, we introduced the Instance-based Generalization with Adaptive Similarity (IGAS) model. IGAS provides a novel computational approach for quantifying the similarity between training experiences and transfer conditions, while also allowing for variability to influence the generalization gradient itself.\nProject 2 shifted focus to the domain of function learning by employing a visuomotor extrapolation task. Across three experiments, we examined how constant and varied training regimes affected learning, discrimination between stimuli, and the ability to extrapolate to novel regions of the function’s input space. To model human performance in this task, we fit the influential Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM) to individual participant data using advanced Bayesian parameter estimation techniques."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dissertation",
    "section": "",
    "text": "Project 1 page\n pdf of the journal article\n\n Link to online version of journal article\n repo"
  },
  {
    "objectID": "index.html#project-1",
    "href": "index.html#project-1",
    "title": "Dissertation",
    "section": "",
    "text": "Project 1 page\n pdf of the journal article\n\n Link to online version of journal article\n repo"
  },
  {
    "objectID": "index.html#project-2",
    "href": "index.html#project-2",
    "title": "Dissertation",
    "section": "Project 2",
    "text": "Project 2\n Link to project 2 page\n\n Working Draft of Manuscript\n repo"
  },
  {
    "objectID": "index.html#full-manuscript",
    "href": "index.html#full-manuscript",
    "title": "Dissertation",
    "section": "Full Manuscript",
    "text": "Full Manuscript\n Web page version\n Version with code\n pdf version\n\n repo"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Dissertation",
    "section": "Slides",
    "text": "Slides\n Slides overview\n Defense slides"
  },
  {
    "objectID": "Sections/Discussion.html",
    "href": "Sections/Discussion.html",
    "title": "General Discussion",
    "section": "",
    "text": "To facilitate ease of comparison between the two projects and their respective tasks, we’ll now refer to project 1 as Hit The Target (HTT) and project 2 as Hit The Wall (HTW).\n\n\nAcross both projects, we investigated the influence of training variability on learning and generalization in computerized visuomotor skill learning, and function learning tasks. In project 1 (HTT), experiments 1 and 2 demonstrated that varied training led to superior testing performance compared to constant training. In Experiment 1, the varied group even outperformed the constant group even when testing from the constant groups trained position. In contrast, Project 2 (HTW) found the opposite pattern - the varied training groups exhibited poorer performance than the constant groups, both in terms of training accuracy, accuracy in extrapolation testing, and, in a subset of the experiments, the varied group showed a diminished ability to discriminate between bands. This detrimental effect of variability was observed across three experiments, with the exception of the reverse order condition in Experiment 3, where the varied group was able to match the constant group’s performance.\nBoth projects also included computational modeling componenents. In Project 1, the IGAS model was introduced as a means of addressing the lack of control for similarity between training and testing conditions common to previous work in the “benefits of variability” literature. The IGAS model provides a theoretically motivated method of quantifying the similarity between training experience and testing conditions. The resulting similarity metric (i.e. our 1c-similarity) is shown to be a significant predictor of testing performance on its own, and when added as a covariate to the statistical model used to compare the constant and varied training groups. We then showed the group-level effect of training variability on testing performance can be accounted for with the additional assumption that training variability influences the generalization gradient. The contribution of the IGAS model was thus twofold:  1) providing a theoretically justifiable method of quantifying/controlling for similarity between training and testing, and 2) demonstrating the viability of a flexible-similarity based generalization account for the empirically observed benefit of variability in our task. Although similar approaches have been employed in other domains, both contributions are novel additions to the large body of research assessing the effect of constant vs. varied training manipulations in visuomotor skill tasks.\nAlthough theoretically motivated, the IGAS model of Project 1 is best categorized as a descriptive measurement-model. Sufficient to account for group differences, but lacking the machinery necessary to provide a full process-level account of how the empirical quantities of interest are generated. In contrast, Project 2 (HTW) implemented a more robust computational modeling approach, implementing and comparing full process models (ALM & EXAM), capable of generating predictions for both the learning and testing stages of the experiment. ALM and EXAM have been used as models of function learning, cue judgement, and forecasting behavior in numerous studies over the past 25 years (Brown & Lacroix, 2017; DeLosh et al., 1997; Kane & Broomell, 2020; Kelley & Busemeyer, 2008; Kwantes et al., 2012; Mcdaniel et al., 2009; Von Helversen & Rieskamp, 2010). The present work presents the first application of these models to to the study of training variability in a visuomotor function learning task. We fit both models to individual participant data, using a form of simulation-based Bayesian parameter estimation that allowed us to generate and compare the full posterior predictive distributions of each model. EXAM provided the best overall account of the testing data, and the advantage of EXAM over ALM was significantly greater for the constant group. Notably, EXAM captured the constant groups’ ability to extrapolate linearly to novel velocity bands, despite receiving training from only a single input-output pair. This finding suggests that EXAM’s linear extrapolation mechanism, combined with the assumption of prior knowledge about the origin point (0, 0), was sufficient to account for the constant groups’ accurate extrapolation performance. Such findings may offer a preliminary suggestion that experience with a more variable set of training examples may be detrimental to performance in simple extrapolation tasks.\n\n\n\nThe HTT and HTW tasks differ across numerous dimensions that may be relevant to the opposing patterns observed in the two projects (see Table 1 provides for a detailed comparison of the two tasks).\nIn HTT, the salient perceptual elements of the task (i.e. the launching box, target and barrier) are subject to variation (i.e. different distances between the launching box and target), and the spatial layout of these perceptually variable elements are intrinsically linked to the task objective of striking the target. Conversely, the perceptual task elements in HTW are invariant across trials, and the task objective is specified by the target velocity value specified as a numeral at the top of the screen. If the benefits of training variation do arise from the formation and flexible retrieval of distinct memory traces, then the lack of perceptual salience between training instances in the HTW task may have limited any potential benefits of variability. Future work could investigate this possibility further employing a modified version of the HTW task wherein the correct velocity value is indicated by some perceptual feature of the task (e.g., the color of the wall, or size of the ball), rather than displaying the target velocity numerically.\nThe HTT and HTW tasks also differed in terms of general task complexity. The HTT task was designed to mimic projectile launching tasks commonly employed in visuomotor learning studies, and the parabolic trajectories necessary to strike the target in HTT were sensitive to both the x and y dimensions of the projectiles velocity (and to a lesser extent, the position within the launching box at which the ball was released). Conversely the HTW task was influenced to a greater extent by the tasks commonly utilized in the function learning literature, wherein the correct output respones are determined by a single input dimension. In HTW,the relationship between feedback and optimal behavioral adjustment is also almost perfectly smooth, if participants produce a throw that is 100 units too hard, they’ll be told that they were 100 units away from the target band. Whereas in HTT, the presence of the barrier in introduces irregularities in the task space. Even throws close to the solution space might result in failure, creating a less predictable learning environment.\n\n\n\nTable 1: Comparison of the tasks in Project 1 (HTT) and Project 2 (HTW).\n\n\n\n\n\n\n\n\n\n\nDimension\nHTT (Project 1)\nHTW (Project 2)\n\n\n\n\nTask Description\nProjectile launching to hit a target\nProjectile launching to hit wall at a specific velocity\n\n\nTask Complexity\nMore complex parabolic trajectory, both x and y velocities relevant to outcome\nSimpler 1D mapping of force to outcome. Only x velocity is relevant.\n\n\nTask Space\nMore complex: xy velocity combinations closer to the solution space may still result in worse feedback due to striking the barrier.\nSimpler: smooth, linear mapping between velocity and feedback.\n\n\nPerceptual salience of Varied Conditions\nVaried conditions (# of throwing distances) are perceptually distinct, i.e. salient differences in distance between launching box and target.\nVaried conditions (# of velocity bands) are less salient - only difference is the numeral displayed on screen.\n\n\nTesting Feedback\nTesting always included feedback\nPrimary testing stage had no feedback.\n\n\nPotential for Learning during Testing\nLimited potential for learning during testing due to feedback.\nSome potential for learning during no-feedback testing by observing ball trajectory.\n\n\nTraining Experience\nVaried group gets half as much experience on any one position as the constant group.\nVaried group gets 1/3 as much experience on any one velocity band as the constant group.\n\n\nTesting Structure\nRandom interleaving of trained/transfer testing distances.\nBlocked structure, separately testing trained vs extrapolation testing bands.\n\n\n\n\n\n\n\n\nIn summary, this dissertation provides a comprehensive examination of the effects of training variability on learning and generalization in visuomotor and function learning tasks. The contrasting results obtained from the Hit The Target (HTT) and Hit The Wall (HTW) tasks underscore the complexity inherent to the longstanding pedagogical and scientific goal of identifying training manipulations that consistently benefit learning and generalization. Moreover, through the development and application of computational models, we provide novel theoretical accounts for both the beneficial and detrimental effects of training variability observed in our experiments. These findings highlight the importance of considering task characteristics when designing experiments intended to assess the influence of training interventions, and demonstrate the value of combining empirical and computational modeling approaches to uncover the cognitive mechanisms that support learning and generalization. Future research should continue to investigate the complex interplay between task demands, training manipulations, and individual differences, with the ultimate goal of optimizing educational and training outcomes across a wide range of domains."
  },
  {
    "objectID": "Sections/Discussion.html#empirical-and-modeling-summary",
    "href": "Sections/Discussion.html#empirical-and-modeling-summary",
    "title": "General Discussion",
    "section": "",
    "text": "Across both projects, we investigated the influence of training variability on learning and generalization in computerized visuomotor skill learning, and function learning tasks. In project 1 (HTT), experiments 1 and 2 demonstrated that varied training led to superior testing performance compared to constant training. In Experiment 1, the varied group even outperformed the constant group even when testing from the constant groups trained position. In contrast, Project 2 (HTW) found the opposite pattern - the varied training groups exhibited poorer performance than the constant groups, both in terms of training accuracy, accuracy in extrapolation testing, and, in a subset of the experiments, the varied group showed a diminished ability to discriminate between bands. This detrimental effect of variability was observed across three experiments, with the exception of the reverse order condition in Experiment 3, where the varied group was able to match the constant group’s performance.\nBoth projects also included computational modeling componenents. In Project 1, the IGAS model was introduced as a means of addressing the lack of control for similarity between training and testing conditions common to previous work in the “benefits of variability” literature. The IGAS model provides a theoretically motivated method of quantifying the similarity between training experience and testing conditions. The resulting similarity metric (i.e. our 1c-similarity) is shown to be a significant predictor of testing performance on its own, and when added as a covariate to the statistical model used to compare the constant and varied training groups. We then showed the group-level effect of training variability on testing performance can be accounted for with the additional assumption that training variability influences the generalization gradient. The contribution of the IGAS model was thus twofold:  1) providing a theoretically justifiable method of quantifying/controlling for similarity between training and testing, and 2) demonstrating the viability of a flexible-similarity based generalization account for the empirically observed benefit of variability in our task. Although similar approaches have been employed in other domains, both contributions are novel additions to the large body of research assessing the effect of constant vs. varied training manipulations in visuomotor skill tasks.\nAlthough theoretically motivated, the IGAS model of Project 1 is best categorized as a descriptive measurement-model. Sufficient to account for group differences, but lacking the machinery necessary to provide a full process-level account of how the empirical quantities of interest are generated. In contrast, Project 2 (HTW) implemented a more robust computational modeling approach, implementing and comparing full process models (ALM & EXAM), capable of generating predictions for both the learning and testing stages of the experiment. ALM and EXAM have been used as models of function learning, cue judgement, and forecasting behavior in numerous studies over the past 25 years (Brown & Lacroix, 2017; DeLosh et al., 1997; Kane & Broomell, 2020; Kelley & Busemeyer, 2008; Kwantes et al., 2012; Mcdaniel et al., 2009; Von Helversen & Rieskamp, 2010). The present work presents the first application of these models to to the study of training variability in a visuomotor function learning task. We fit both models to individual participant data, using a form of simulation-based Bayesian parameter estimation that allowed us to generate and compare the full posterior predictive distributions of each model. EXAM provided the best overall account of the testing data, and the advantage of EXAM over ALM was significantly greater for the constant group. Notably, EXAM captured the constant groups’ ability to extrapolate linearly to novel velocity bands, despite receiving training from only a single input-output pair. This finding suggests that EXAM’s linear extrapolation mechanism, combined with the assumption of prior knowledge about the origin point (0, 0), was sufficient to account for the constant groups’ accurate extrapolation performance. Such findings may offer a preliminary suggestion that experience with a more variable set of training examples may be detrimental to performance in simple extrapolation tasks."
  },
  {
    "objectID": "Sections/Discussion.html#differences-between-the-two-projects",
    "href": "Sections/Discussion.html#differences-between-the-two-projects",
    "title": "General Discussion",
    "section": "",
    "text": "The HTT and HTW tasks differ across numerous dimensions that may be relevant to the opposing patterns observed in the two projects (see Table 1 provides for a detailed comparison of the two tasks).\nIn HTT, the salient perceptual elements of the task (i.e. the launching box, target and barrier) are subject to variation (i.e. different distances between the launching box and target), and the spatial layout of these perceptually variable elements are intrinsically linked to the task objective of striking the target. Conversely, the perceptual task elements in HTW are invariant across trials, and the task objective is specified by the target velocity value specified as a numeral at the top of the screen. If the benefits of training variation do arise from the formation and flexible retrieval of distinct memory traces, then the lack of perceptual salience between training instances in the HTW task may have limited any potential benefits of variability. Future work could investigate this possibility further employing a modified version of the HTW task wherein the correct velocity value is indicated by some perceptual feature of the task (e.g., the color of the wall, or size of the ball), rather than displaying the target velocity numerically.\nThe HTT and HTW tasks also differed in terms of general task complexity. The HTT task was designed to mimic projectile launching tasks commonly employed in visuomotor learning studies, and the parabolic trajectories necessary to strike the target in HTT were sensitive to both the x and y dimensions of the projectiles velocity (and to a lesser extent, the position within the launching box at which the ball was released). Conversely the HTW task was influenced to a greater extent by the tasks commonly utilized in the function learning literature, wherein the correct output respones are determined by a single input dimension. In HTW,the relationship between feedback and optimal behavioral adjustment is also almost perfectly smooth, if participants produce a throw that is 100 units too hard, they’ll be told that they were 100 units away from the target band. Whereas in HTT, the presence of the barrier in introduces irregularities in the task space. Even throws close to the solution space might result in failure, creating a less predictable learning environment.\n\n\n\nTable 1: Comparison of the tasks in Project 1 (HTT) and Project 2 (HTW).\n\n\n\n\n\n\n\n\n\n\nDimension\nHTT (Project 1)\nHTW (Project 2)\n\n\n\n\nTask Description\nProjectile launching to hit a target\nProjectile launching to hit wall at a specific velocity\n\n\nTask Complexity\nMore complex parabolic trajectory, both x and y velocities relevant to outcome\nSimpler 1D mapping of force to outcome. Only x velocity is relevant.\n\n\nTask Space\nMore complex: xy velocity combinations closer to the solution space may still result in worse feedback due to striking the barrier.\nSimpler: smooth, linear mapping between velocity and feedback.\n\n\nPerceptual salience of Varied Conditions\nVaried conditions (# of throwing distances) are perceptually distinct, i.e. salient differences in distance between launching box and target.\nVaried conditions (# of velocity bands) are less salient - only difference is the numeral displayed on screen.\n\n\nTesting Feedback\nTesting always included feedback\nPrimary testing stage had no feedback.\n\n\nPotential for Learning during Testing\nLimited potential for learning during testing due to feedback.\nSome potential for learning during no-feedback testing by observing ball trajectory.\n\n\nTraining Experience\nVaried group gets half as much experience on any one position as the constant group.\nVaried group gets 1/3 as much experience on any one velocity band as the constant group.\n\n\nTesting Structure\nRandom interleaving of trained/transfer testing distances.\nBlocked structure, separately testing trained vs extrapolation testing bands.\n\n\n\n\n\n\n\n\nIn summary, this dissertation provides a comprehensive examination of the effects of training variability on learning and generalization in visuomotor and function learning tasks. The contrasting results obtained from the Hit The Target (HTT) and Hit The Wall (HTW) tasks underscore the complexity inherent to the longstanding pedagogical and scientific goal of identifying training manipulations that consistently benefit learning and generalization. Moreover, through the development and application of computational models, we provide novel theoretical accounts for both the beneficial and detrimental effects of training variability observed in our experiments. These findings highlight the importance of considering task characteristics when designing experiments intended to assess the influence of training interventions, and demonstrate the value of combining empirical and computational modeling approaches to uncover the cognitive mechanisms that support learning and generalization. Future research should continue to investigate the complex interplay between task demands, training manipulations, and individual differences, with the ultimate goal of optimizing educational and training outcomes across a wide range of domains."
  },
  {
    "objectID": "Sections/project_transition.html",
    "href": "Sections/project_transition.html",
    "title": "Dissertation",
    "section": "",
    "text": "In project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model.\nOptional: - emphasize that both tasks involve extrapolation - it’s just emphasized more in Project 2, and project 2 is designed to more closely mirror tasks used in the function learning literature.\nThe HTT and HTW tasks also differed in terms of general task complexity. The HTT task was designed to mimic projectile launching tasks commonly employed in visuomotor learning studies, and the parabolic trajectories necessary to strike the target in HTT were sensitive to both the x and y dimensions of the projectile’s velocity (and to a lesser extent, the position within the launching box at which the ball was released). Conversely, the HTW task was influenced to a greater extent by the tasks commonly utilized in the function learning literature, wherein the correct output responses are determined by a single input dimension. In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects; if they throw 100 units too hard, they’ll be told that they were 100 units too hard. In contrast, in HTT, it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws but resulted in worse feedback due to striking the barrier.\nIn Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\nIn project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, we will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. Project 2 also places a greater emphasis on extrapolation performance following training - as varied training has often been purported to be particularly beneficial in such situations. Extrapolation has long been a focus of the literature on function learning (Brehmer 1974; Carroll 1963). Central questions of the function learning literature have included the relative difficulties of learning various functional forms (e.g. linear vs.bilinear vs. quadratic), and the relative effectiveness of rule-based vs. association-based exemplar models vs. various hybrid models (Bott and Heit 2004; DeLosh, McDaniel, and Busemeyer 1997; Jones et al. 2018; Kalish, Lewandowsky, and Kruschke 2004; M. A. Mcdaniel and Busemeyer 2005; M. Mcdaniel et al. 2009). However the issue of training variation has received surprisingly little attention in this area."
  },
  {
    "objectID": "Sections/pre_sections/sigs.html",
    "href": "Sections/pre_sections/sigs.html",
    "title": "Dissertation",
    "section": "",
    "text": "Accepted by the Graduate Faculty, Indiana University, in partial fulfillment of the requirements for the degree of Doctor of Philosophy.\n\n\n\n\n\n\nDoctoral Committee    ___________________________ Robert L. Goldstone, Ph.D., Co-Chair  \n___________________________ Robert Nosofsky, Ph.D., Co-Chair  \n___________________________ Peter Todd, Ph.D. ___________________________ Mike Jones, Ph.D.\n              \n10/20/2023"
  },
  {
    "objectID": "Sections/pre_sections/pre_html.html",
    "href": "Sections/pre_sections/pre_html.html",
    "title": "Dissertation",
    "section": "",
    "text": "The Role of Variability in Learning Transfer: A Similarity-Based Computational Approach\n\n\nThomas E. Gorman"
  },
  {
    "objectID": "Outline.html",
    "href": "Outline.html",
    "title": "Dissertation Outline",
    "section": "",
    "text": "In Project 1, I programmed a simple projectile launching task to serve as a conceptual replication of an influential paradigm in the visuomotor skill learning literature. Several of the canonical empirical patterns are replicated, with the varied trained participants tending to perform better during testing in both experiments. A major issue with previous research in the cross-disciplinary “benefits of variability” literature is that many previous works do not adequately control for the similarity between training and testing conditions. Such issues arise when both from failures to consider the possibility of non-linear generalization, and from often the unquestioned assumption that participants are acquiring, and then generalizing from prototype or schema-based representations. I introduce a theoretically motivated method of explicitly quantifying the similarity between training experience and testing condition. The resulting similarity quantity can then be used to explicitly control for similarity (by adding it as a covariate to the statistical model). The effect of variability remains significant while controlling for similarity, which I argue is a more rigorous demonstration of the effect of variability on testing performance than what is typically provided with standard methods. I conclude by introducing an extended version of the model that assumes training variation influences the steepness of the generalization gradient. With this flexible similarity mechanism, the group-level effect of variability can then be accounted for within the similarity-based generalization framework.\n\n\n\n\nIn Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\n\nhttps://tegorman13.github.io/DP/\n\n\n\n\n\n\nProject 1\n\nAbstract\nIntroduction\n\nSimilarity and instance-based approaches to transfer of learning\nThe effect of training variability on transfer\nIssues with Previous Research\n\nExperiment 1\n\nMethods\n\nSample Size Estimation\nParticipants\nTask\n\nResults\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\nDiscussion\n\nExperiment 2\n\nMethods\n\nParticipants\nTask and Procedure\n\nResults\n\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\n\nDiscussion\n\nComputational Model\n\nFitting model parameters separately by group\n\nGeneral Discussion\n\nLimitations\nConclusion\n\nReferences\n\nProject 2\n\nIntroduction\nMethods\n\nParticipants\nTask\nDesign\n\nResults\n\nTraining\nTesting\n\nModeling\n\nALM & Exam Description\nModel Equations\nModel Fitting and Comparison\n\nReferences\n\nProject 3\n\nOverview\nMethods\n\nDataset and Game Description\nSplit-Test Data\n\nTrial-by-trial influence of variability\n\nRandomization\nMeasuring Trial-by-trial variability\n\nComputational Modelling\n\nSimilarity Between Trials\nMeasurement model of learning and performance\n\nReferences"
  },
  {
    "objectID": "Outline.html#dissertation-outline",
    "href": "Outline.html#dissertation-outline",
    "title": "Dissertation Outline",
    "section": "",
    "text": "In Project 1, I programmed a simple projectile launching task to serve as a conceptual replication of an influential paradigm in the visuomotor skill learning literature. Several of the canonical empirical patterns are replicated, with the varied trained participants tending to perform better during testing in both experiments. A major issue with previous research in the cross-disciplinary “benefits of variability” literature is that many previous works do not adequately control for the similarity between training and testing conditions. Such issues arise when both from failures to consider the possibility of non-linear generalization, and from often the unquestioned assumption that participants are acquiring, and then generalizing from prototype or schema-based representations. I introduce a theoretically motivated method of explicitly quantifying the similarity between training experience and testing condition. The resulting similarity quantity can then be used to explicitly control for similarity (by adding it as a covariate to the statistical model). The effect of variability remains significant while controlling for similarity, which I argue is a more rigorous demonstration of the effect of variability on testing performance than what is typically provided with standard methods. I conclude by introducing an extended version of the model that assumes training variation influences the steepness of the generalization gradient. With this flexible similarity mechanism, the group-level effect of variability can then be accounted for within the similarity-based generalization framework.\n\n\n\n\nIn Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.\n\nhttps://tegorman13.github.io/DP/\n\n\n\n\n\n\nProject 1\n\nAbstract\nIntroduction\n\nSimilarity and instance-based approaches to transfer of learning\nThe effect of training variability on transfer\nIssues with Previous Research\n\nExperiment 1\n\nMethods\n\nSample Size Estimation\nParticipants\nTask\n\nResults\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\nDiscussion\n\nExperiment 2\n\nMethods\n\nParticipants\nTask and Procedure\n\nResults\n\nData Processing and Statistical Packages\nTraining Phase\nTesting Phase\n\nDiscussion\n\nComputational Model\n\nFitting model parameters separately by group\n\nGeneral Discussion\n\nLimitations\nConclusion\n\nReferences\n\nProject 2\n\nIntroduction\nMethods\n\nParticipants\nTask\nDesign\n\nResults\n\nTraining\nTesting\n\nModeling\n\nALM & Exam Description\nModel Equations\nModel Fitting and Comparison\n\nReferences\n\nProject 3\n\nOverview\nMethods\n\nDataset and Game Description\nSplit-Test Data\n\nTrial-by-trial influence of variability\n\nRandomization\nMeasuring Trial-by-trial variability\n\nComputational Modelling\n\nSimilarity Between Trials\nMeasurement model of learning and performance\n\nReferences"
  },
  {
    "objectID": "Sections/pre_sections/cover.html",
    "href": "Sections/pre_sections/cover.html",
    "title": "Dissertation",
    "section": "",
    "text": "Thomas Gorman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmitted to the faculty of the University Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the Department of Psychology and Brain Sciences and the Cognitive Science Program, Indiana University Indiana University Fall 2023"
  },
  {
    "objectID": "plan.html",
    "href": "plan.html",
    "title": "plan",
    "section": "",
    "text": "Writing Plan\n\n\nCurrent Progress\n\n\n\nSection\nPercentage\n\n\n\n\nVariability Intro\n30%\n\n\nIGAS Study\n90%\n\n\nHTW Study\n40%\n\n\n* Function Learning Review\n0%\n\n\n* Function Learning + Variability connection\n0%\n\n\n* Experimental Methods\n40%\n\n\n* Results\n10%\n\n\n* Modelling\n10%\n\n\n* Discussion\n0%\n\n\nSynthesis Model\n0%\n\n\nGeneral Discussion\n0%"
  },
  {
    "objectID": "Sections/pre_sections/toc_html.html",
    "href": "Sections/pre_sections/toc_html.html",
    "title": "Table of contents",
    "section": "",
    "text": "Table of contents"
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Dissertation Manuscript",
    "section": "",
    "text": "HTML\n   \n  \n  \n    HTML (new window)\n  \n\n    \n    HTML (code included)\n  \n\n  \n    PDF\n   \n  \n\n    PDF (download)\n  \n\n\n\n    docx (download)\n  \n    \n  \n  \n  \n  \n  \n  \n  \n  Click buttons above to select format or download",
    "crumbs": [
      "Dissertation Manuscript"
    ]
  },
  {
    "objectID": "Sections/Appendix/Full_Appendix.html",
    "href": "Sections/Appendix/Full_Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Reviewer 3 Absolute versus relative distance: From a methodological standpoint, I understand the need to differentiate these two types of distance. However, from a theoretical perspective there may be some issue in differentiating these two concepts. Schema theory relies on relative (or invariant) information to inform the motor program. However, both distances would be important to an instance or exemplar representation. You may want to consider commenting on this issue.\nReviewer 2 For the same reason, the plots showing improvement during training could be due to participants learning the task, rather than fine motor skills. Although task learning and motor learning are impossible to separate cleanly, the common practice in the field is indeed to offer practice trials to reduce the task learning aspects. The authors should address this.\nIn addition to absolute errors (which is related to variance), the authors should also provide other measures of performance, e.g., the mean of the signed errors, so that readers have a better idea whether there was any meaningful over- or undershooting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=========================================================================\nconditType devianceDirection      610            760            910      \n-------------------------------------------------------------------------\nconstant       Overshoot                    311.84(307.92)               \nconstant      Undershoot                    188.05(163.62)               \nvaried         Overshoot     211.69(234.97)                360.14(322.01)\nvaried        Undershoot     107.35(81.21)                 244.85(196.47)\n-------------------------------------------------------------------------\n\n\n\n======================================================\nconditType      610           760            910      \n------------------------------------------------------\nconstant                 121.03(269.17)               \nvaried     39.91(178.12)                150.53(290.04)\n------------------------------------------------------\n\n\n\n====================================================================\nconditType     610           760            835            910      \n--------------------------------------------------------------------\nconstant   7.13(124.02) 107.02(218.49) 142.42(252.34) 122.92(282.58)\nvaried     3.19(96.67)   92.1(173.9)   103.84(214.4)  108.12(234.59)\n--------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n====================================================================================================================================\nconditType2         msdu_610       msdu_760       msdu_835       msdu_910      msds_610      msds_760      msds_835      msds_910   \n------------------------------------------------------------------------------------------------------------------------------------\nConstant Training 136.27(84.29) 191.65(112.65) 219.46(139.91) 276.75(153.09) 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   105.12(51.39)  149.37(93.4)  180.54(129.52) 198.64(137.84) 13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n=========================================================================\nCondition              610           760           835           910     \n-------------------------------------------------------------------------\nConstant Training 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n-------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n======================================================================================================\nCondition 610_First Half 760_First Half 910_First Half 610_Second Half 760_Second Half 910_Second Half\n------------------------------------------------------------------------------------------------------\nconstant  206.64(82.08)  286.51(121.07) 406.93(145.2)   187.2(55.24)    238.21(95.16)  313.27(114.86) \nvaried    195.68(78.58)  278.9(105.37)  318.53(134.81)  177.79(70.82)  224.98(108.04)   276.86(110.5) \n------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.04 0.083       0.015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.38 0.067       0.016",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "Sections/Appendix/Full_Appendix.html#appendix---project-1",
    "href": "Sections/Appendix/Full_Appendix.html#appendix---project-1",
    "title": "Appendix",
    "section": "",
    "text": "Reviewer 3 Absolute versus relative distance: From a methodological standpoint, I understand the need to differentiate these two types of distance. However, from a theoretical perspective there may be some issue in differentiating these two concepts. Schema theory relies on relative (or invariant) information to inform the motor program. However, both distances would be important to an instance or exemplar representation. You may want to consider commenting on this issue.\nReviewer 2 For the same reason, the plots showing improvement during training could be due to participants learning the task, rather than fine motor skills. Although task learning and motor learning are impossible to separate cleanly, the common practice in the field is indeed to offer practice trials to reduce the task learning aspects. The authors should address this.\nIn addition to absolute errors (which is related to variance), the authors should also provide other measures of performance, e.g., the mean of the signed errors, so that readers have a better idea whether there was any meaningful over- or undershooting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=========================================================================\nconditType devianceDirection      610            760            910      \n-------------------------------------------------------------------------\nconstant       Overshoot                    311.84(307.92)               \nconstant      Undershoot                    188.05(163.62)               \nvaried         Overshoot     211.69(234.97)                360.14(322.01)\nvaried        Undershoot     107.35(81.21)                 244.85(196.47)\n-------------------------------------------------------------------------\n\n\n\n======================================================\nconditType      610           760            910      \n------------------------------------------------------\nconstant                 121.03(269.17)               \nvaried     39.91(178.12)                150.53(290.04)\n------------------------------------------------------\n\n\n\n====================================================================\nconditType     610           760            835            910      \n--------------------------------------------------------------------\nconstant   7.13(124.02) 107.02(218.49) 142.42(252.34) 122.92(282.58)\nvaried     3.19(96.67)   92.1(173.9)   103.84(214.4)  108.12(234.59)\n--------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n====================================================================================================================================\nconditType2         msdu_610       msdu_760       msdu_835       msdu_910      msds_610      msds_760      msds_835      msds_910   \n------------------------------------------------------------------------------------------------------------------------------------\nConstant Training 136.27(84.29) 191.65(112.65) 219.46(139.91) 276.75(153.09) 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   105.12(51.39)  149.37(93.4)  180.54(129.52) 198.64(137.84) 13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n=========================================================================\nCondition              610           760           835           910     \n-------------------------------------------------------------------------\nConstant Training 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n-------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n======================================================================================================\nCondition 610_First Half 760_First Half 910_First Half 610_Second Half 760_Second Half 910_Second Half\n------------------------------------------------------------------------------------------------------\nconstant  206.64(82.08)  286.51(121.07) 406.93(145.2)   187.2(55.24)    238.21(95.16)  313.27(114.86) \nvaried    195.68(78.58)  278.9(105.37)  318.53(134.81)  177.79(70.82)  224.98(108.04)   276.86(110.5) \n------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.04 0.083       0.015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.38 0.067       0.016",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "Sections/Appendix/Full_Appendix.html#appendix---project-2---experiment-1",
    "href": "Sections/Appendix/Full_Appendix.html#appendix---project-2---experiment-1",
    "title": "Appendix",
    "section": "Appendix - Project 2 - Experiment 1",
    "text": "Appendix - Project 2 - Experiment 1\n\nPosterior Predictive Distributions\n\n\n\n\n\n\n\n\nFigure 1: Posterior Predictive distributions for Absolute Deviance. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Posterior Predictive distributions for Vx. Posterior Draws in Blue, colored lines are empirical data.\n\n\n\n\n\n\n\nEmpirical vs. Predicted\n\n\n\n\n\n\n\n\nFigure 3: Bayesian Mixed Model predictions vs. Empirical Predictions - X velocity\n\n\n\n\n\n\n\nDifferent Aggregations\n\n\n\n\n\n\n\n\nFigure 4: E1. Distribution of Vx at Participant level\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: E1. Distribution of Vx at Trial level\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: E1. Predicted Means Per Condition and Band, and Average Marginal Effect (Constant - Varied)\n\n\n\n\n\n:::",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "Sections/Appendix/IGAS-Supplemental.html",
    "href": "Sections/Appendix/IGAS-Supplemental.html",
    "title": "Dissertation",
    "section": "",
    "text": "exponential learning models fit to individual subjects\n\n\nGroup comparison of learning rate fits\n\n\n\n\n\n\n\n\n\n\n\nFirst vs. second half of testing stage\n\n\n\n\n\n\n\n\n\n\n\nGroup Comparison for asymptote-starting performance\n\n\n\n\n\n\n\n\n\n\n\nRelative distance and under/overshooting\nReviewer 3 Absolute versus relative distance: From a methodological standpoint, I understand the need to differentiate these two types of distance. However, from a theoretical perspective there may be some issue in differentiating these two concepts. Schema theory relies on relative (or invariant) information to inform the motor program. However, both distances would be important to an instance or exemplar representation. You may want to consider commenting on this issue.\nReviewer 2 For the same reason, the plots showing improvement during training could be due to participants learning the task, rather than fine motor skills. Although task learning and motor learning are impossible to separate cleanly, the common practice in the field is indeed to offer practice trials to reduce the task learning aspects. The authors should address this.\nIn addition to absolute errors (which is related to variance), the authors should also provide other measures of performance, e.g., the mean of the signed errors, so that readers have a better idea whether there was any meaningful over- or undershooting.\n\nexperiment 1 training - relative distances\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=========================================================================\nconditType devianceDirection      610            760            910      \n-------------------------------------------------------------------------\nconstant       Overshoot                    311.84(307.92)               \nconstant      Undershoot                    188.05(163.62)               \nvaried         Overshoot     211.69(234.97)                360.14(322.01)\nvaried        Undershoot     107.35(81.21)                 244.85(196.47)\n-------------------------------------------------------------------------\n\n\n\n======================================================\nconditType      610           760            910      \n------------------------------------------------------\nconstant                 121.03(269.17)               \nvaried     39.91(178.12)                150.53(290.04)\n------------------------------------------------------\n\n\n\n====================================================================\nconditType     610           760            835            910      \n--------------------------------------------------------------------\nconstant   7.13(124.02) 107.02(218.49) 142.42(252.34) 122.92(282.58)\nvaried     3.19(96.67)   92.1(173.9)   103.84(214.4)  108.12(234.59)\n--------------------------------------------------------------------\n\n\n\n\nexperiment 2 training - relative distances\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 1 Testing - relative distances\n\n\n\n\n\n\n\n\n\n\n====================================================================================================================================\nconditType2         msdu_610       msdu_760       msdu_835       msdu_910      msds_610      msds_760      msds_835      msds_910   \n------------------------------------------------------------------------------------------------------------------------------------\nConstant Training 136.27(84.29) 191.65(112.65) 219.46(139.91) 276.75(153.09) 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   105.12(51.39)  149.37(93.4)  180.54(129.52) 198.64(137.84) 13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n=========================================================================\nCondition              610           760           835           910     \n-------------------------------------------------------------------------\nConstant Training 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n-------------------------------------------------------------------------\n\n\n\n\nExperiment 2 Testing - relative distances\n\n\n\n\n\n\n\n\n\n\n\nExperimenet 1 - intermittent testing\n\n\n\n\n\n\n\n\n\n\n======================================================================================================\nCondition 610_First Half 760_First Half 910_First Half 610_Second Half 760_Second Half 910_Second Half\n------------------------------------------------------------------------------------------------------\nconstant  206.64(82.08)  286.51(121.07) 406.93(145.2)   187.2(55.24)    238.21(95.16)  313.27(114.86) \nvaried    195.68(78.58)  278.9(105.37)  318.53(134.81)  177.79(70.82)  224.98(108.04)   276.86(110.5) \n------------------------------------------------------------------------------------------------------\n\n\n\n\n\nTraining plots - Experiment 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot in manuscript\n\n\nfit to testing performance averaged across positions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical tests for starting performance\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.04 0.083       0.015\n\n\n\n\n\n\n\n\n\n\n\nstatistical tests for asymptote\n\n\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p&lt;.05   ges\n1 conditType   1 206 3.38 0.067       0.016"
  },
  {
    "objectID": "Sections/IGAS.html",
    "href": "Sections/IGAS.html",
    "title": "IGAS Project",
    "section": "",
    "text": "pdf of the journal article\nLink to online version of journal article\nrepo\nDisplay code\npacman::p_load(tidyr,papaja, knitr, tinytex, RColorBrewer, kableExtra, cowplot, patchwork,here)\nsource(here::here('Functions/IGAS_ProcessFunctions.R'))\n\ntheme_set(theme_classic())\n# load the processed data from experiment 1 and 2\ne1 &lt;- readRDS(here::here(\"data/igas_e1_cleanedData-final.rds\")) %&gt;% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity,stageInt=as.numeric(as.character(experimentStage)))\ne2&lt;- readRDS(here::here('data/igas_e2_cleanedData-final.rds')) %&gt;% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity)\n# load subject similarity data - computed with the IGAS model in 'IGAS-SimModel.R'\ne2_sim &lt;- readRDS(here::here('data/IGAS_Similarity-Performance.rds'))\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\ndefaultContrasts = options()$contrasts\ntheme_set(theme_classic())\n\ndodge &lt;- position_dodge(width = 0.9)\ne2GrpPos &lt;- c(\"400\",\"500\",\"625\",\"675\",\"800\",\"900\")\ne2Grp &lt;- paste(\"Constant\",\"Constant\", \"Constant\",\"Constant\",\"Constant\",\"Constant\", \"Varied\")\ne2Labels &lt;- paste(c(\"400\\n Constant\",\"500\\n Constant\",\"625\\n Constant\",\"675\\n Constant\",\n                   \"800\\n Constant\",\"900\\n Constant\",\"500-800\\n Varied\"),sep=\"\")\n\ne1Pos &lt;- c(\"610\",\"760\",\"835\",\"910\")\ne1Var &lt;- paste(\"Varied Train Position\",\"Constant Train Position\", \"Novel Position\", \"Varied Training Position\")\ne1Labels&lt;- paste(c(\"610\\n Varied Trained\",\"760\\n Constant Trained\",\"835\\n Novel Location\",\"910\\n Varied Trained\"),sep=\"\")",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#similarity-and-instance-based-approaches-to-transfer-of-learning",
    "href": "Sections/IGAS.html#similarity-and-instance-based-approaches-to-transfer-of-learning",
    "title": "IGAS Project",
    "section": "Similarity and instance-based approaches to transfer of learning",
    "text": "Similarity and instance-based approaches to transfer of learning\nNotions of similarity have long played a central role in many prominent models of generalization of learning, as well as in the longstanding theoretical issue of whether learners abstract an aggregate, summary representation, or if they simply store individual instances. Early models of learning often assumed that discrete experiences with some task or category were not stored individually in memory, but instead promoted the formation of a summary representation, often referred to as a prototype or schema, and that exposure to novel examples would then prompt the retrieval of whichever preexisting prototype was most similar (Posner & Keele, 1968). Prototype models were later challenged by the success of instance-based or exemplar models – which were shown to provide an account of generalization as good or better than prototype models, with the advantage of not assuming the explicit construction of an internal prototype (Estes, 1994; Hintzman, 1984; Medin & Schaffer, 1978; Nosofsky, 1986). Instance-based models assume that learners encode each experience with a task as a separate instance/exemplar/trace, and that each encoded trace is in turn compared against novel stimuli. As the number of stored instances increases, so does the likelihood that some previously stored instance will be retrieved to aid in the performance of a novel task. Stored instances are retrieved in the context of novel stimuli or tasks if they are sufficiently similar, thus suggesting that the process of computing similarity is of central importance to generalization.\nSimilarity, defined in this literature as a function of psychological distance between instances or categories, has provided a successful account of generalization across numerous tasks and domains. In an influential study demonstrating an ordinal similarity effect, experimenters employed a numerosity judgment task in which participants quickly report the number of dots flashed on a screen. Performance (in terms of response times to new patterns) on novel dot configurations varied as an inverse function of their similarity to previously trained dot configurations Palmeri (1997). That is, performance was better on novel configurations moderately similar to trained configurations than to configurations with low-similarity, and also better on low-similarity configurations than to even less similar, unrelated configurations. Instance-based approaches have had some success accounting for performance in certain sub-domains of motor learning (Cohen & Rosenbaum, 2004; Crump & Logan, 2010; Meigh et al., 2018; Poldrack et al., 1999; Wifall et al., 2017). Crump & Logan (2010) trained participants to type words on an unfamiliar keyboard, while constraining the letters composing the training words to a pre-specified letter set. Following training, typing speed was tested on previously experienced words composed of previously experienced letters; novel words composed of letters from the trained letter set; and novel words composed of letters from an untrained letter set. Consistent with an instance-based account, transfer performance was graded such that participants were fastest at typing the words they had previously trained on, followed by novel words composed of letters they had trained on, and slowest performance for new words composed of untrained letters.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#issues-with-previous-research",
    "href": "Sections/IGAS.html#issues-with-previous-research",
    "title": "IGAS Project",
    "section": "Issues with Previous Research",
    "text": "Issues with Previous Research\nAlthough the benefits of training variation in visuomotor skill learning have been observed many times, null findings have also been repeatedly found, leading some researchers to question the veracity of the variability of practice hypothesis (Newell, 2003; Van Rossum, 1990). Critics have also pointed out that investigations of the effects of training variability, of the sort described above, often fail to control for the effect of similarity between training and testing conditions. For training tasks in which participants have numerous degrees of freedom (e.g. projectile throwing tasks where participants control the x and y velocity of the projectile), varied groups are likely to experience a wider range of the task space over the course of their training (e.g. more unique combinations of x and y velocities). Experimenters may attempt to account for this possibility by ensuring that the training location(s) of the varied and constant groups are an equal distance away from the eventual transfer locations, such that their training throws are, on average, equally similar to throws that would lead to good performance at the transfer locations. However, even this level of experimental control may still be insufficient to rule out the effect of similarity on transfer. Given that psychological similarity is typically best described as either a Gaussian or exponentially decaying function of psychological distance (Ennis et al., 1988; Ghahramani et al., 1996; Logan, 1988; Nosofsky, 1992; Shepard, 1987; Thoroughman & Taylor, 2005), it is plausible that a subset of the most similar training instances could have a disproportionate impact on generalization to transfer conditions, even if the average distance between training and transfer conditions is identical between groups. Figure 1 demonstrates the consequences of a generalization gradient that drops off as a Gaussian function of distance from training, as compared to a linear drop-off.\n\n\nDisplay code\np=2\nc&lt;- .0002\nsimdat &lt;- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %&gt;%\n                     mutate(plotjitter=ifelse(condit==\"varied\",0,7),\n                            linScale=ifelse(condit==\"varied\",980,1000),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genLinear=1000-abs(x-train.position)+plotjitter) %&gt;% \n  #group_by(condit) %&gt;% mutate(scaleLinear=(genLinear-min(genLinear))/(max(genLinear)-min(genLinear))) \n  group_by(x,condit) %&gt;%\n  reframe(genGauss=mean(genGauss),genLinear=mean(genLinear)/linScale,.groups = 'keep')\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs &lt;- list(geom_line(alpha=.7,size=.4),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=447,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=450,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=659,y=1.05,label=\"Constant\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=657,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=847,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=850,y=1.02,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1),\n                        legend.position=\"none\"))\n\nip1 &lt;- simdat  %&gt;% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"\")\nip2 &lt;- simdat %&gt;%  ggplot(aes(x,y=genLinear,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")\n\nplot_grid(ip1,ip2,ncol=2,rel_heights=c(1))\n\n\n\n\n\n\n\n\nFigure 1: Left panel- Generalization predicted from a simple model that assumes a linear generalization function. A varied group (red vertical lines indicate the 2 training locations) trained from positions 400 and 800, and a constant group (blue vertical line), trained from position 600. Right panel- if a Gaussian generalization function is assumed, then varied training (400, 800) is predicted to result in better generalization to positions close to 400 and 800 than does constant training at 600. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)\n\n\n\n\n\nIn addition to largely overlooking the potential for non-linear generalization to confound interpretations of training manipulations, the visuomotor skill learning literature also rarely considers alternatives to schema representations (Chamberlin & Magill, 1992b). Although schema-theory remains influential within certain literatures, instance or exemplar-based models have accounted for human behavior across myriad domains (Jamieson et al., 2022; Logan, 2002). As mentioned above, instance based accounts have been shown to perform well on a variety of different tasks with motoric components (Crump & Logan, 2010; Gandolfo et al., 1996; Meigh et al., 2018; Rosenbaum et al., 1995; van Dam & Ernst, 2015). However, such accounts have received little attention within the subdomain of visuomotor skill learning focused on the benefits of varied training.\nThe present work examines whether the commonly observed benefits of varied training can be accounted for by a theoretrically motivated measure of the similarity between training throws and the testing solution space. We first attempt to replicate previous work finding an advantage of varied training over constant training in a projectile launching task. We then examine the extent to which this advantage can be explained by an instance-based similarity model.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#methods",
    "href": "Sections/IGAS.html#methods",
    "title": "IGAS Project",
    "section": "Methods",
    "text": "Methods\n\nSample Size Estimation\nTo obtain an independent estimate of effect size, we identified previous investigations which included between-subjects contrasts of varied and constant conditions following training on an accuracy based projectile launching task (Chua et al., 2019; Goodwin et al., 1998; Kerr & Booth, 1978; Wulf, 1991). We then averaged effects across these studies, yielding a Cohen’s f =.43. The GPower 3.1 software package (Faul et al., 2009) was then used to determine that a power of 80% requires a sample size of at least 23 participants per condition. All experiments reported in the present manuscript exceed this minimum number of participants per condition.\n\n\nParticipants\nParticipants were recruited from an undergraduate population that is 63% female and consists almost entirely of individuals aged 18 to 22 years. A total of 110 Indiana University psychology students participated in Experiment 1. We subsequently excluded 34 participants poor performance at one of the dependent measures of the task (2.5-3 standard deviations worse than the median subject at the task) or for displaying a pattern of responses that was clearly indicative of a lack of engagement with the task (e.g. simply dropping the ball on each trial rather than throwing it at the target), or for reporting that they completed the experiment on a phone or tablet device, despite the instructions not to use one of these devices. A total of 74 participants were retained for the final analyses, 35 in the varied group and 39 in the constant group.\n\nTask\nThe experimental task was programmed in JavaScript, using packages from the Phaser physics engine (https://phaser.io) and the jsPsych library (de Leeuw, 2015). The stimuli, presented on a black background, consisted of a circular blue ball - controlled by the participant via the mouse or trackpad cursor; a rectangular green target; a red rectangular barrier located between the ball and the target; and an orange square within which the participant could control the ball before releasing it in a throw towards the target. Because the task was administered online, the absolute distance between stimuli could vary depending on the size of the computer monitor being used, but the relative distance between the stimuli was held constant. Likewise, the distance between the center of the target and the training and testing locations was scaled such that relative distances were preserved regardless of screen size. For the sake of brevity, subsequent mentions of this relative distance between stimuli, or the position where the ball landed in relation to the center of the target, will be referred to simply as distance. Figure 2 displays the layout of the task, as it would appear to a participant at the start of a trial, with the ball appearing in the center of the orange square. Using a mouse or trackpad, participants click down on the ball to take control of the ball, connecting the movement of the ball to the movement of the cursor. Participants can then “wind up” the ball by dragging it (within the confines of the orange square) and then launch the ball by releasing the cursor. If the ball does not land on the target, participants are presented with feedback in red text at the top right of the screen, specifying how many scaled units away the ball was from the center of the target. If the ball was thrown outside of the boundary of the screen participants are given feedback as to how far away from the target center the ball would have been if it had continued its trajectory. If the ball strikes the barrier (from the side or by landing on top), feedback is presented telling participants to avoid hitting the barrier. If participants drag the ball outside of the orange square before releasing it, the trial terminates, and they are reminded to release the ball within the orange square. If the ball lands on the target, feedback is presented in green text, confirming that the target was hit, and presenting additional feedback on how many units away the ball was from the exact center of the target.\nLink to abbreviated example of task.\n\n\nDisplay code\nmf &lt;- cowplot::ggdraw()+cowplot::draw_image(here::here(\"Assets/methodsFig1.png\"),hjust=0)+theme(plot.margin = margin(0, 0, 0, 0))\nplot_grid(mf,ncol=1)\n\n\n\n\n\n\n\n\nFigure 2: The stimuli of the task consisted of a blue ball, which the participants would launch at the green target, while avoiding the red barrier. On each trial, the ball would appear in the center of the orange square, with the position of the orange square varying between experimental conditions. Participants were constrained to release the ball within the square\n\n\n\n\n\n\n\nProcedure\nParticipants first electronically consented to participate, and then read instructions for the task which explained how to control the ball, and the goal of throwing the ball as close to the center of the target as possible. The training phase was split into 10 blocks of 20 trials, for a total of 200 training trials. Participants in the constant condition trained exclusively from a single location (760 scaled units from the target center). Participants in the varied condition trained from two locations (610 and 910 scaled units from the target center), encountering each location 100 times. The sequence of throwing locations was pseudo-random for the varied group, with the constraint that within every block of 20 training throws both training locations would occur 10 times. Participants in both conditions also received intermittent testing trials after every 20 training trials. Intermittent testing trials provided no feedback of any kind. The ball would disappear from view as soon as it left the orange square, and participants were prompted to start the next trial without receiving any information about the accuracy of the throw. Each intermittent testing stage consisted of two trials from each of the three training positions (i.e. all participants executed two trials each from Positions 610, 760, and 910 during each of the 10 intermittent testing stages). Following training, all participants completed a final testing phase from four positions: 1) their training location, 2) the training location(s) of the other group, 3) a location novel to both groups. The testing phase consisted of 15 trials from each of the four locations, presented in a randomized order. All trials in the final testing phase included feedback. After finishing the final testing portion of the study, participants were queried as to whether they completed the study using a mouse, a trackpad, or some other device (this information was used in the exclusion process described above). Finally, participants were debriefed as to the hypotheses and manipulation of the study.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#results",
    "href": "Sections/IGAS.html#results",
    "title": "IGAS Project",
    "section": "Results",
    "text": "Results",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#data-processing-and-statistical-packages",
    "href": "Sections/IGAS.html#data-processing-and-statistical-packages",
    "title": "IGAS Project",
    "section": "Data Processing and Statistical Packages",
    "text": "Data Processing and Statistical Packages\nTo prepare the data, we removed trials that were not easily interpretable as performance indicators in our task. Removed trials included: 1) those in which participants dragged the ball outside of the orange starting box without releasing it, 2) trials in which participants clicked on the ball, and then immediately released it, causing the ball to drop straight down, 3) outlier trials in which the ball was thrown more than 2.5 standard deviations further than the average throw (calculated separately for each throwing position), and 4) trials in which the ball struck the barrier. The primary measure of performance used in all analyses was the absolute distance away from the center of the target. The absolute distance was calculated on every trial, and then averaged within each subject to yield a single performance score, for each position. A consistent pattern across training and testing phases in both experiments was for participants to perform worse from throwing positions further away from the target – a pattern which we refer to as the difficulty of the positions. However, there were no interactions between throwing position and training conditions, allowing us to collapse across positions in cases where contrasts for specific positions were not of interest. All data processing and statistical analyses were performed in R version 4.32 (Team, 2020). ANOVAs for group comparisons were performed using the rstatix package (Kassambara, 2021).",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#training-phase",
    "href": "Sections/IGAS.html#training-phase",
    "title": "IGAS Project",
    "section": "Training Phase",
    "text": "Training Phase\nFigure 3 below shows aggregate training performance binned into three stages representing the beginning, middle, and end of the training phase. Because the two conditions trained from target distances that were not equally difficult, it was not possible to directly compare performance between conditions in the training phase. Our focus for the training data analysis was instead to establish that participants did improve their performance over the course of training, and to examine whether there was any interaction between training stage and condition. Descriptive statistics for the intermittent testing phase are provided in the supplementary materials.\nWe performed an ANOVA comparison with stage as a within-group factor and condition as between-group factor. The analysis revealed a significant effect of training stage F(2,142)=62.4, p&lt;.001, \\(\\eta^{2}_G\\) = .17, such that performance improved over the course of training. There was no significant effect of condition F(1,71)=1.42, p=.24, \\(\\eta^{2}_G\\) = .02, and no significant interaction between condition and training stage, F(2,142)=.10, p=.91, \\(\\eta^{2}_G\\) &lt; .01.\n\n\nDisplay code\nexp1TrainPosition &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1TrainPosition3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1) %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train3 &lt;- e1 %&gt;% filter(stage!=\"Transfer\",mode==1)  %&gt;%\n  group_by(sbjCode,Group,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\n\ne1train2 &lt;- exp1TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",fun=mean,position=dodge)+\n  facet_wrap(~conditType,ncol=2)+\n  stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  xlab(\"Training Location(s)\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)\n\n\n#plot_grid(title,e1train2,capt,ncol=1,rel_heights=c(.18,1,.15))\nplot_grid(e1train2,ncol=1)\n\n\n\n\n\n\n\n\nFigure 3: Training performance for varied and constant participants binned into three stages. Shorter bars indicate better performance (ball landing closer to the center of the target). Error bars indicate standard error of the mean.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#testing-phase",
    "href": "Sections/IGAS.html#testing-phase",
    "title": "IGAS Project",
    "section": "Testing Phase",
    "text": "Testing Phase\nIn Experiment 1, a single constant-trained group was compared against a single varied-trained group. At the transfer phase, all participants were tested from 3 positions: 1) the positions(s) from their own training, 2) the training position(s) of the other group, and 3) a position novel to both groups. Overall, group performance was compared with a mixed type III ANOVA, with condition (varied vs. constant) as a between-subject factor and throwing location as a within-subject variable. The effect of throwing position was strong, F(3,213) = 56.12, p&lt;.001, η2G = .23. The effect of training condition was significant F(1,71)=8.19, p&lt;.01, η2G = .07. There was no significant interaction between group and position, F(3,213)=1.81, p=.15, η2G = .01.\n\n\nDisplay code\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n#manuscript plot\ne1test1=exp1.Test %&gt;% ggplot(aes(x=positionX,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",fun=mean,position=dodge)+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)+scale_x_discrete(name=\"Testing Location\",labels=e1Labels)\n\ne1test1\n\n\n\n\n\n\n\n\nFigure 4: Testing performance for each of the 4 testing positions, compared between training conditions. Positions 610 and 910 were trained on by the varied group, and novel for the constant group. Position 760 was trained on by the constant group, and novel for the varied group. Position 835 was novel for both groups. Shorter bars are indicative of better performance (the ball landing closer to the center of the target). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nDisplay code\nexp1.Test &lt;- e1 %&gt;% filter(stage==\"Transfer\") %&gt;% select(-trainHalf)%&gt;% group_by(positionX) %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%\n  ungroup() %&gt;% group_by(sbjCode,conditType,positionX,ThrowPosition) %&gt;%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%&gt;% as.data.frame()\n\n\ntest= exp1.Test %&gt;% dplyr::rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n test=test %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n test = test %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n test=test[,1:3]\n\nkable(test,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;%   \n  kable_classic() #%&gt;% kableExtra::footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 1: Testing performance for varied and constant groups in experiment 1. Mean absolute deviation from the center of the target, with standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n610\n132.48(50.85)\n104.2(38.92)\n\n\n760\n207.26(89.19)\n167.12(72.29)\n\n\n835\n249.13(105.92)\n197.22(109.71)\n\n\n910\n289.36(122.48)\n212.86(113.93)",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#discussion",
    "href": "Sections/IGAS.html#discussion",
    "title": "IGAS Project",
    "section": "Discussion",
    "text": "Discussion\nIn Experiment 1, we found that varied training resulted in superior testing performance than constant training, from both a position novel to both groups, and from the position at which the constant group was trained, which was novel to the varied condition. The superiority of varied training over constant training even at the constant training position is of particular note, given that testing at this position should have been highly similar for participants in the constant condition. It should also be noted, though, that testing at the constant trained position is not exactly identical to training from that position, given that the context of testing is different in several ways from that of training, such as the testing trials from the different positions being intermixed, as well as a simple change in context as a function of time. Such contextual differences will be further considered in the General Discussion.\nIn addition to the variation of throwing position during training, the participants in the varied condition of Experiment 1 also received training practice from the closest/easiest position, as well as from the furthest/most difficult position that would later be encountered by all participants during testing. The varied condition also had the potential advantage of interpolating both of the novel positions from which they would later be tested. Experiment 2 thus sought to address these issues by comparing a varied condition to multiple constant conditions.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#methods-1",
    "href": "Sections/IGAS.html#methods-1",
    "title": "IGAS Project",
    "section": "Methods",
    "text": "Methods\n\nParticipants\nA total of 306 Indiana University psychology students participated in Experiment 2, which was also conducted online. As was the case in Experiment 1, the undergraduate population from which we recruited participants was 63% female and primarily composed of 18–22-year-old individuals. Using the same procedure as Experiment 1, we excluded 98 participants for exceptionally poor performance at one of the dependent measures of the task, or for displaying a pattern of responses indicative of a lack of engagement with the task. A total of 208 participants were included in the final analyses with 31 in the varied group and 32, 28, 37, 25, 29, 26 participants in the constant groups training from location 400, 500, 625, 675, 800, and 900, respectively. All participants were compensated with course credit.\n\n\nTask and Procedure\nThe task of Experiment 2 was identical to that of Experiment 1, in all but some minor adjustments to the height of the barrier, and the relative distance between the barrier and the target. Additionally, the intermittent testing trials featured in Experiment 1 were not utilized in Experiment 2. An abbreviated demo of the task used for Experiment 2 can be found at (https://pcl.sitehost.iu.edu/tg/demos/igas_expt2_demo.html).\nThe procedure for Experiment 2 was also quite similar to Experiment 1. Participants completed 140 training trials, all of which were from the same position for the constant groups and split evenly (70 trials each - randomized) for the varied group. In the testing phase, participants completed 30 trials from each of the six locations that had been used separately across each of the constant groups during training. Each of the constant groups thus experienced one trained location and five novel throwing locations in the testing phase, while the varied group experiences 2 previously trained, and 4 novel locations.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#results-1",
    "href": "Sections/IGAS.html#results-1",
    "title": "IGAS Project",
    "section": "Results",
    "text": "Results\n\nData Processing and Statistical Packages\nAfter confirming that condition and throwing position did not have any significant interactions, we standardized performance within each position, and then average across position to yield a single performance measure per participant. This standardization did not influence our pattern of results. As in Experiment 1, we performed type III ANOVAs due to our unbalanced design, however the pattern of results presented below is not altered if type 1 or type III tests are used instead. The statistical software for the primary analyses was the same as for Experiment 1. Individual learning rates in the testing phase, compared between groups in the supplementary analyses, were fit using the TEfit package in R (Cochrane, 2020).\n\n\nTraining Phase\nThe different training conditions trained from positions that were not equivalently difficult and are thus not easily amenable to comparison. As previously stated, the primary interest of the training data is confirmation that some learning did occur. Figure 5 depicts the training performance of the varied group alongside that of the aggregate of the six constant groups (5a), and each of the 6 separate constant groups (5b). An ANOVA comparison with training stage (beginning, middle, end) as a within-group factor and group (the varied condition vs. the 6 constant conditions collapsed together) as a between-subject factor revealed no significant effect of group on training performance, F(1,206)=.55,p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,412)=77.91, p&lt;.001, \\(\\eta^{2}_G\\) =.05, and no significant interaction between group and training stage, F(2,412)=.489 p=.61, \\(\\eta^{2}_G\\) &lt;.01. We also tested for a difference in training performance between the varied group and the two constant groups that trained matching throwing positions (i.e., the constant groups training from position 500, and position 800). The results of our ANOVA on this limited dataset mirrors that of the full-group analysis, with no significant effect of group F(1,86)=.48, p=.49, \\(\\eta^{2}_G\\) &lt;.01, a significant effect of training stage F(2,172)=56.29, p&lt;.001, \\(\\eta^{2}_G\\) =.11, and no significant interaction between group and training stage, F(2,172)=.341 p=.71, \\(\\eta^{2}_G\\) &lt;.01.\n\n\nDisplay code\ne2$stage &lt;- factor(e2$stage, levels = c(\"Beginning\", \"Middle\", \"End\",\"Transfer\"),ordered = TRUE)\n\nexp2TrainPosition &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter))%&gt;% as.data.frame()\n\nexp2TrainPosition3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\") %&gt;%ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage,positionX) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05))%&gt;% as.data.frame()\n\nexp2Train &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% \n  group_by(sbjCode,Group2,conditType,trainHalf) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter)) %&gt;% as.data.frame()\n\nexp2Train3 &lt;- e2  %&gt;% filter(stage!=\"Transfer\")  %&gt;% ungroup() %&gt;% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup() %&gt;%\n  group_by(sbjCode,Group2,conditType,stage) %&gt;% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\ntransfer &lt;- filter(e2, stage==\"Transfer\") %&gt;% droplevels() %&gt;% select(-trainHalf,-initialVelocityY,ThrowPosition2)%&gt;% ungroup()\ntransfer &lt;- transfer %&gt;% group_by(positionX) %&gt;% mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n  group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\ntransfer &lt;- transfer %&gt;% group_by(sbjCode,positionX) %&gt;% mutate(ind=1,testPosIndex=cumsum(ind),posN=max(testPosIndex)) %&gt;%\n  select(-ind) %&gt;% mutate(testHalf = case_when(testPosIndex&lt;15 ~\"1st Half\",testPosIndex&gt;=15 ~\"2nd Half\")) %&gt;% convert_as_factor(testHalf)\n\nvariedTest &lt;- transfer %&gt;% filter(condit==7) %&gt;% mutate(extrapolate=ifelse(positionX==\"900\" | positionX==\"400\",\"extrapolation\",\"interpolation\")) \nconstantTest &lt;- transfer %&gt;% filter(condit!=7) %&gt;% mutate(extrapolate=ifelse(distFromTrain==0,\"interpolation\",\"extrapolation\"))\n\ntransfer &lt;- rbind(variedTest,constantTest)\ntransfer&lt;- transfer %&gt;% mutate(novel=ifelse(distFromTrain3==0,\"trainedLocation\",\"novelLocation\"))%&gt;% convert_as_factor(novel,extrapolate)\n\ntransfer &lt;- transfer %&gt;% relocate(sbjCode,condit2,Group,conditType2,stage,trial,novel,extrapolate,positionX,AbsDistFromCenter,globalAvg,globalSd,scaledDev,distFromTrain3) %&gt;% ungroup()\n\n\n# novelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0) %&gt;% select(-globalAvg,-globalSd,-scaledDev)%&gt;% droplevels() %&gt;% ungroup()\n# novelAll &lt;- novelAll %&gt;% group_by(positionX) %&gt;%\n#  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %&gt;% \n#   group_by(sbjCode,positionX) %&gt;% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %&gt;%ungroup()\n\nnovelAll &lt;- transfer %&gt;% filter(distFromTrain!=0, distFromTrain3!=0)\nnovelAllMatched &lt;- novelAll %&gt;% filter(condit!=5,condit!=2)\n\n\nconstantIden &lt;- transfer %&gt;% filter(condit !=7,distFromTrain==0) # only constant groups from their training position\nvariedTest &lt;- transfer %&gt;% filter(condit==7) # only varied testing\nvariedVsIden &lt;- rbind(constantIden,variedTest) # all varied combined with constant identity\n\n\nvariedNovel &lt;- variedTest %&gt;% filter(distFromTrain3 !=0) # removes 500 and 800 from varied\nconstantIden2 &lt;- transfer %&gt;% filter(condit !=7,condit!=5,condit!=2,distFromTrain==0) # only constant groups from training position 400,625,675,900\nvariedVsNovelIden &lt;- rbind(constantIden2,variedNovel) # novel positions for varied, trained for constant\n\nexp2.Test &lt;- transfer %&gt;%group_by(sbjCode,conditType,positionX,ThrowPosition)%&gt;%\n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test2 &lt;- exp2.Test %&gt;% group_by(sbjCode,conditType)%&gt;%\n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;%ungroup() %&gt;% as.data.frame()\n\nexp2.Test7 &lt;- transfer %&gt;%group_by(Group2,sbjCode,positionX,Group,conditType,ThrowPosition4) %&gt;% \n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %&gt;% as.data.frame()\n\nexp2.Test7.agg &lt;- exp2.Test7  %&gt;%group_by(Group2,sbjCode,Group,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\nexp2.Test7.agg2 &lt;- exp2.Test7  %&gt;%group_by(sbjCode,conditType) %&gt;% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %&gt;% as.data.frame()\n\n\n\n\nDisplay code\n### New - 3 stage\ne2train1&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=stage,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=stage,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+facet_wrap(~conditType,ncol=2)+\n  ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Stage\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0.0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"A\")\n\ne2train2&lt;-exp2TrainPosition3 %&gt;% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  facet_wrap(~conditType,ncol=2)+stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Location(s)\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"B\")\n\n#plot_grid(e2train1,e2train2,ncol=1)\n\ne2train1/e2train2\n\n\n\n\n\n\n\n\nFigure 5: Training performance for the six constant conditions, and the varied condition, binned into three stages. On the left side, the six constant groups are averaged together, as are the two training positions for the varied group. On the right side, the six constant groups are shown separately, with each set of bars representing the beginning, middle, and end of training for a single constant group that trained from the position indicated on the x-axis. Figure 5b also shows training performance separately for both of the throwing locations trained by the varied group. Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\nTesting Phase\nIn Experiment 2, a single varied condition (trained from two positions, 500 and 800), was compared against six separate constant groups (trained from a single position, 400, 500, 625, 675, 800 or 900). For the testing phase, all participants were tested from all six positions, four of which were novel for the varied condition, and five of which were novel for each of the constant groups. For a general comparison, we took the absolute deviations for each throwing position and computed standardized scores across all participants, and then averaged across throwing position. The six constant groups were then collapsed together allowing us to make a simple comparison between training conditions (constant vs. varied). A type III between-subjects ANOVA was performed, yielding a significant effect of condition F(1,206)=4.33, p=.039, \\(\\eta^{2}_G\\) =.02. Descriptive statistics for each condition are shown in table 2. In Figure 6 visualizes the consistent advantage of the varied condition over the constant groups across the testing positions. Figure 6 shows performance between the varied condition and the individual constant groups.\n\n\nDisplay code\n# manuscript plot\ne2test1&lt;-exp2.Test %&gt;% ggplot(aes(x=ThrowPosition,y=MeanTargetDeviance,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25)+\n  ggtitle(\"A\")\n\n\ne2test2&lt;-exp2.Test7 %&gt;% \n  ggplot(aes(x=Group,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",position=position_dodge(),fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=position_dodge())+\n  facet_wrap(~ThrowPosition4)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25,\n        axis.text.x = element_text(size = 7,angle=45,hjust=1))+\n  scale_x_discrete(name=\" Training Group\",labels=e2Labels)+ggtitle(\"B\")\n\ne2test1 / e2test2\n\n\n\n\n\n\n\n\nFigure 6: Testing phase performance from each of the six testing positions. The six constant conditions are averaged together into a single constant group, compared against the single varied-trained group.B) Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\n\nDisplay code\ntab2= exp2.Test %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n tab2=tab2 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n tab2 = tab2 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% \n   select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab2=tab2[,1:3]\n\n\nkable(tab2,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 2: Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n100.59(46.3)\n83.92(33.76)\n\n\n500\n152.28(69.82)\n134.38(61.38)\n\n\n625\n211.21(90.95)\n183.51(75.92)\n\n\n675\n233.32(93.35)\n206.32(94.64)\n\n\n800\n283.24(102.85)\n242.65(89.73)\n\n\n900\n343.51(114.33)\n289.62(110.07)\n\n\n\n\n\n\n\n\n\n\nNext, we compared the testing performance of constant and varied groups from only positions that participants had not encountered during training. Constant participants each had 5 novel positions, whereas varied participants tested from 4 novel positions (400,625,675,900). We first standardized performance within in each position, and then averaged across positions. Here again, we found a significant effect of condition (constant vs. varied): F(1,206)=4.30, p=.039, \\(\\eta^{2}_G\\) = .02 .\n\n\nDisplay code\nsum.novelAll &lt;- novelAll %&gt;% group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ntab3=sum.novelAll %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\n tab3=tab3 %&gt;% group_by(Condition) %&gt;% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \n tab3 = tab3 %&gt;% \n   mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab3=tab3[,1:3]\n\n\n\nkable(tab3,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 3: Testing performance from novel positions. Includes data only from positions that were not encountered during the training stage (e.g. excludes positions 500 and 800 for the varied group, and one of the six locations for each of the constant groups). Table presents Mean absolute deviations from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n98.84(45.31)\n83.92(33.76)\n\n\n500\n152.12(69.94)\nNA\n\n\n625\n212.91(92.76)\n183.51(75.92)\n\n\n675\n232.9(95.53)\n206.32(94.64)\n\n\n800\n285.91(102.81)\nNA\n\n\n900\n346.96(111.35)\n289.62(110.07)\n\n\n\n\n\n\n\n\n\n\nFinally, corresponding to the comparison of position 760 from Experiment 1, we compared the test performance of the varied group against the constant group from only the positions that the constant groups trained. Such positions were novel to the varied group (thus this analysis omitted two constant groups that trained from positions 500 or 800 as those positions were not novel to the varied group). Figure 7 displays the particular subset of comparisons utilized for this analysis. Again, we standardized performance within each position before performing the analyses on the aggregated data. In this case, the effect of condition did not reach statistical significance F(1,149)=3.14, p=.079, \\(\\eta^{2}_G\\) = .02. Table 4 provides descriptive statistics.\n\n\nDisplay code\nsum.variedVsNovelIden &lt;- variedVsNovelIden  %&gt;%\n  group_by(sbjCode,conditType,positionX) %&gt;% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %&gt;% as.data.frame()\n\ne2Test2 &lt;- sum.variedVsNovelIden %&gt;% ggplot(aes(x=positionX,y=MeanTargetDev,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)\n\n\ne2Test2\n\n\n\n\n\n\n\n\nFigure 7: A comparison of throwing location that are identical to those trained by the constant participants (e.g. constant participants trained at position 900, tested from position 900), which are also novel to the varied-trained participants (thus excluding positions 500 and 800). Error bars indicate standard error of the mean.\n\n\n\n\n\n\n\n\n\n\nDisplay code\ntab4=sum.variedVsNovelIden %&gt;% rename(Condition=\"conditType\") %&gt;% group_by(Condition,positionX) %&gt;%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\ntab4=tab4 %&gt;% group_by(Condition) %&gt;% \n   mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \ntab4 = tab4 %&gt;% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %&gt;% select(positionX,Condition,msd,gsd)%&gt;%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab4=tab4[,1:3]\n\nkable(tab4,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  %&gt;% kableExtra::kable_styling(position=\"left\") %&gt;% \n  kable_classic() #%&gt;% footnote(general=captionText,general_title = \"\")\n\n\n\n\nTable 4: Testing performance from the locations trained by constant participants and novel to varied participants. Locations 500 and 800 are not included as these were trained by the varied participants. Table presents Mean absolute deviation from the center of the target, and standard deviations in parenthesis.\n\n\n\n\n\n\nPosition\nConstant\nVaried\n\n\n\n\n400\n108.85(50.63)\n83.92(33.76)\n\n\n625\n204.75(84.66)\n183.51(75.92)\n\n\n675\n235.75(81.15)\n206.32(94.64)\n\n\n900\n323.5(130.9)\n289.62(110.07)",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#discussion-1",
    "href": "Sections/IGAS.html#discussion-1",
    "title": "IGAS Project",
    "section": "Discussion",
    "text": "Discussion\nThe results of Experiment 2 largely conform to the findings of Experiment 1. Participants in both varied and constant conditions improved at the task during the training phase. We did not observe the common finding of training under varied conditions producing worse performance during acquisition than training under constant conditions (Catalano & Kleiner, 1984; Wrisberg et al., 1987), which has been suggested to relate to the subsequent benefits of varied training in retention and generalization testing (Soderstrom & Bjork, 2015). However our finding of no difference in training performance between constant and varied groups has been observed in previous work (Chua et al., 2019; Moxley, 1979; Pigott & Shapiro, 1984).\nIn the testing phase, our varied group significantly outperformed the constant conditions in both a general comparison, and in an analysis limited to novel throwing positions. The observed benefit of varied over constant training echoes the findings of many previous visuomotor skill learning studies that have continued to emerge since the introduction of Schmidt’s influential Schema Theory (Catalano & Kleiner, 1984; Chua et al., 2019; Goodwin et al., 1998; McCracken & Stelmach, 1977; Moxley, 1979; Newell & Shapiro, 1976; Pigott & Shapiro, 1984; Roller et al., 2001; Schmidt, 1975; Willey & Liu, 2018; Wrisberg et al., 1987; Wulf, 1991). We also join a much smaller set of research to observe this pattern in a computerized task (Seow et al., 2019). One departure from the Experiment 1 findings concerns the pattern wherein the varied group outperformed the constant group even from the training position of the constant group, which was significant in Experiment 1, but did not reach significance in Experiment 2. Although this pattern has been observed elsewhere in the literature (Goode et al., 2008; Kerr & Booth, 1978), the overall evidence for this effect appears to be far weaker than for the more general benefit of varied training in conditions novel to all training groups. # Computational Model\nControlling for the similarity between training and testing. The primary goal of Experiment 2 was to examine whether the benefits of variability would persist after accounting for individual differences in the similarity between trained and tested throwing locations. To this end, we modelled each throw as a two-dimensional point in the space of x and y velocities applied to the projectile at the moment of release. For each participant, we took each individual training throw, and computed the similarity between that throw and the entire population of throws within the solution space for each of the 6 testing positions. We defined the solution space empirically as the set of all combinations of x and y throw velocities that resulted in hitting the target. We then summed each of the trial-level similarities to produce a single similarity for each testing position score relating how the participant threw the ball during training and the solutions that would result in target hits from each of the six testing positions – thus resulting in six separate similarity scores for each participant. Figure 8 visualizes the solution space for each location and illustrates how different combinations of x and y velocity result in successfully striking the target from different launching positions. As illustrated in Figure 8, the solution throws represent just a small fraction of the entire space of velocity combinations used by participants throughout the experiment.\n\n\nDisplay code\ntaskspace &lt;- e2 %&gt;% filter(AbsDistFromCenter&lt;900)\ntaskspace$hitOrMiss &lt;- ifelse(taskspace$trialType==11,\"Hit Target\",\"Missed Target\")\n\nsolSpace &lt;- e2 %&gt;% filter(trialType==11)\n#solSpace %&gt;% ggplot(aes(x=X_Velocity,y=Y_Velocity)) + geom_point(aes(colour=ThrowPosition),alpha=0.58) + ggtitle(\"\") \n\nsolSpace$Result = ifelse(solSpace$ThrowPosition==400,\"400\",solSpace$ThrowPosition)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==500,\"500\",solSpace$Result)\nsolSpace$Result= ifelse(solSpace$ThrowPosition==625,\"625\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==675,\"675\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==800,\"800\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==900,\"900\",solSpace$Result)\n\n\nmissSpace &lt;- e2 %&gt;% filter(trialType !=11)\nmissSpace$Result = \"Missed Target\"\nsolSpace$Result &lt;- solSpace$Result\n\n# the usual method of changing the legend title does not seem to work after the colours are manually scaled. \n# multiplied velocoties by -1 to make the axes less confusing\nss=solSpace %&gt;% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1)) + \n  geom_point(aes(colour=Result),alpha=0.6) + \n  scale_color_manual(values =brewer.pal(n=6,name=\"Set1\"))+\n  labs(colour=\"Target Hit Thrown from Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\")+ggtitle(\"A\")\n\nfullSpace &lt;- rbind(missSpace,solSpace)\n\nfs&lt;- fullSpace %&gt;% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1,colour=Result)) + \n  geom_point(aes(),alpha=0.6) + scale_color_manual(values =brewer.pal(n=7,name=\"Set1\"))+\n  labs(colour=\"Target Hit or Miss From Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\") +ggtitle(\"B\")\n\n\nss/fs\n\n\n\n\n\n\n\n\nFigure 8: A) A visual representation of the combinations of throw parameters (x and y velocities applied to the ball at launch), which resulted in target hits during the testing phase. This empirical solution space was compiled from all of the participants in experiment 2. B) shows the solution space within the context of all of the throws made throughout the testing phase of the experiment.\n\n\n\n\n\nFor each individual trial, the Euclidean distance (Equation 1) was computed between the velocity components (x and y) of that trial and the velocity components of each individual solution throw for each of the 6 positions from which participants would be tested in the final phase of the study. The P parameter in Equation 1 is set equal to 2, reflecting a Gaussian similarity gradient. Then, as per an instance-based model of similarity (Logan, 2002; Nosofsky, 1992), these distances were multiplied by a sensitivity parameter, c, and then exponentiated to yield a similarity value. The parameter c controls the rate with which similarity-based generalization drops off as the Euclidean distance between two throws in x- and y-velocity space increases. If c has a large value, then even a small difference between two throws’ velocities greatly decreases the extent of generalization from one to the other. A small value for c produces broad generalization from one throw to another despite relatively large differences in their velocities. The similarity values for each training individual throw made by a given participant were then summed to yield a final similarity score, with a separate score computed for each of the 6 testing positions. The final similarity score is construable as index of how accurate the throws a participant made during the training phase would be for each of the testing positions.\nEquation 1: \\[ Similarity_{I,J} = \\sum_{i=I}\\sum_{j=J}\n(e^{-c^\\cdot d^{p}_{i,j}}) \\]\nEquation 2: \\[ d_{i,j} = \\sqrt{(x_{Train_i}-x_{Solution_j})^2 + (y_{Train_i}-y_{Solution_j})^2 } \\]\nA simple linear regression revealed that these similarity scores were significantly predictive of performance in the transfer stage, t =-15.88, p&lt;.01, \\(r^2\\)=.17, such that greater similarity between training throws and solution spaces for each of the test locations resulted in better performance. We then repeated the group comparisons above while including similarity as a covariate in the model. Comparing the varied and constant groups in testing performance from all testing positions yielded a significant effect of similarity, F(1, 205)=85.66, p&lt;.001, \\(\\eta^{2}_G\\) =.29, and also a significant effect of condition (varied vs. constant), F(1, 205)=6.03, p=.015, \\(\\eta^{2}_G\\) =.03. The group comparison limited to only novel locations for the varied group pit against trained location for the constant group resulted in a significant effect of similarity, F(1,148)=31.12, p&lt;.001, \\(\\eta^{2}_G\\) =.18 as well as for condition F(1,148)=11.55, p&lt;.001, \\(\\eta^{2}_G\\) =.07. For all comparisons, the pattern of results was consistent with the initial findings from Experiment 2, with the varied group still performing significantly better than the constant group.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#fitting-model-parameters-separately-by-group",
    "href": "Sections/IGAS.html#fitting-model-parameters-separately-by-group",
    "title": "IGAS Project",
    "section": "Fitting model parameters separately by group",
    "text": "Fitting model parameters separately by group\nTo directly control for similarity in Experiment 2, we developed a model-based measure of the similarity between training throws and testing conditions. This similarity measure was a significant predictor of testing performance, e.g., participants whose training throws were more similar to throws that resulted in target hits from the testing positions, tended to perform better during the testing phase. Importantly, the similarity measure did not explain away the group-level benefits of varied training, which remained significant in our linear model predicting testing performance after similarity was added to the model. However, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum & Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the c parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky & Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions.\nTo address this possibility, we compared the original instance-based model of similarity fit against a modified model which separately fits the generalization parameter, c, to varied and constant participants. To perform this parameter fitting, we used the optim function in R, and fit the model to find the c value(s) that maximized the correlation between similarity and testing performance.\nBoth models generate distinct similarity values between training and testing locations. Much like the analyses in Experiment 2, these similarity values are regressed against testing performance in models of the form shown below. As was the case previously, testing performance is defined as the mean absolute distance from the center of the target (with a separate score for each participant, from each position).\nLinear models 1 and 3 both show that similarity is a significant predictor of testing performance (p&lt;.01). Of greater interest is the difference between linear model 2, in which similarity is computed from a single c value fit from all participants (Similarity1c), with linear model 4, which fits the c parameter separately between groups (Similarity2c). In linear model 2, the effect of training group remains significant when controlling for Similarity1c (p&lt;.01), with the varied group still performing significantly better. However, in linear model 4 the addition of the Similarity2c predictor results in the effect of training group becoming nonsignificant (p=.40), suggesting that the effect of varied vs. constant training is accounted for by the Similarity2c predictor. Next, to further establish a difference between the models, we performed nested model comparisons using ANOVA, to see if the addition of the training group parameter led to a significant improvement in model performance. In the first comparison, ANOVA(Linear Model 1, Linear Model 2), the addition of the training group predictor significantly improved the performance of the model (F=22.07, p&lt;.01). However, in the second model comparison, ANOVA (Linear model 3, Linear Model 4) found no improvement in model performance with the addition of the training group predictor (F=1.61, p=.20).\nFinally, we sought to confirm that similarity values generated from the adjusted Similarity2c model had more predictive power than those generated from the original Similarity1c model. Using the BIC function in R, we compared BIC values between linear model 1 (BIC=14604.00) and linear model 3 (BIC = 14587.64). The lower BIC value of model 3 suggests a modest advantage for predicting performance using a similarity measure computed with two c values over similarity computed with a single c value. When fit with separate c values, the best fitting c parameters for the model consistently optimized such that the c value for the varied group (c=.00008) was smaller in magnitude than the c value for the constant group(c= .00011). Recall that similarity decreases as a Gaussian function of distance (equation 1 above), and a smaller value of c will result in a more gradual drop-off in similarity as the distance between training throws and testing solutions increases.\nIn summary, our modeling suggests that an instance-based model which assumes equivalent generalization gradients between constant and varied trained participants is unable to account for the extent of benefits of varied over constant training observed at testing. The evidence for this in the comparative model fits is that when a varied/constant dummy-coded variable for condition is explicitly added to the model, the variable adds a significant contribution to the prediction of test performance, with the variable condition yielding better performance than the constant conditions. However, if the instance-based generalization model is modified to assume that the training groups can differ in the steepness of their generalization gradient, by incorporating a separate generalization parameter for each group, then the instance-based model can account for our experimental results without explicitly taking training group into account. Henceforth this model will be referred to as the Instance-based Generalization with Adaptive Similarity (IGAS) model.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#limitations",
    "href": "Sections/IGAS.html#limitations",
    "title": "IGAS Project",
    "section": "Limitations",
    "text": "Limitations\nA limitation of this study concerns the ordering of the testing/transfer trials at the conclusion of both experiments. Participants were tested from each separate position (4 in Experiment 1, 6 in Experiment 2) in a random, intermixed order. Because the varied group was trained from two positions that were also randomly ordered, they may have benefited from experience with this type of sequencing, whereas the constant groups had no experience with switching between positions trial to trial. This concern is somewhat ameliorated by the fact that the testing phase performance of the constant groups from their trained position was not significantly worse than their level of performance at the end of the training phase, suggesting that they were not harmed by random ordering of positions during testing. It should also be noted that the computerized task utilized in the present work is relatively simple compared to many of the real-world tasks utilized in prior research. It is thus conceivable that the effect of variability in more complex tasks is distinct from the process put forward in the present work. An important challenge for future work will be to assess the extent to which IGAS can account for generalization in relatively complex tasks with far more degrees of freedom.\nIt is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g. the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/IGAS.html#conclusion",
    "href": "Sections/IGAS.html#conclusion",
    "title": "IGAS Project",
    "section": "Conclusion",
    "text": "Conclusion\nOur experiments add to the longstanding body of research investigating the effect of training variability on learning and generalization. Both experiments demonstrate a reliable benefit of varied training over constant training in a projectile launching task. We also introduce the Instance-based Generalization with Adaptive Similarity (IGAS) as a novel explanation for the effect of variability on generalization. The key assumption made by IGAS is that the amount of variation encountered during training influences the steepness of the generalization gradient. Instance-based models augmented with this assumption may be a valuable approach towards better understanding skill generalization and transfer.",
    "crumbs": [
      "IGAS Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html",
    "href": "Sections/HTW.html",
    "title": "HTW Project",
    "section": "",
    "text": "Link to project page\nWorking Draft of Manuscript\nrepo\n_______________________________________________________________________________\nDisplay code\npacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n  brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n  data.table, stringr, here,conflicted, gt, ggh4x, patchwork, knitr)\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\noptions(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n\nwalk(c(\"brms\",\"dplyr\",\"bayestestR\",\"here\"), conflict_prefer_all, quiet = TRUE)\nwalk(c(\"Display_Functions\",\"deLosh_data\",\"fun_alm\",\"fun_indv_fit\",\"fun_model\", \"prep_model_data\",\"org_functions\"), ~source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\n# source(here::here(\"Functions\",\"deLosh_data.R\"))\n# source(here::here(\"Functions\",\"Display_Functions.R\"))",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#function-learning-and-extrapolation",
    "href": "Sections/HTW.html#function-learning-and-extrapolation",
    "title": "HTW Project",
    "section": "Function Learning and Extrapolation",
    "text": "Function Learning and Extrapolation\nThe study of human function learning investigates how people learn relationships between continuous input and output values. Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs (DeLosh et al., 1997; McDaniel et al., 2013), or situations where observers are presented with an incomplete scatterplot or line graph and make predictions about regions of the plot that do not contain data (Ciccione & Dehaene, 2021; Courrieu, 2012; Said & Fischer, 2021; Schulz et al., 2020). Studies of function learning often compare the difficulty of learning functions of different underlying forms (e.g. linear, bi-linear, power, sinusoidal), and the extent to which participants can accurately respond to novel inputs that fall in-between previously experienced inputs (interpolation testing), or that fall outside the range of previously experienced inputs (extrapolation).\nCarroll (1963) conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function. Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a rule-based model wherein learners form an abstract representation of the underlying function. Subsequent work by Brehmer (1974), testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out assuming a linear function, but given sufficient error will progressively test alternative hypotheses with polynomials of greater degree. Koh & Meyer (1991) employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power law, as opposed to linear function. Koh and Meyer developed the log-polynomial adaptive-regression model to account for their results.\nThe first significant challenge to rule-based accounts of function learning was put forth by DeLosh et al. (1997) . In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to accurately respond on interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performances are depicted in Figure 1.\nThe authors evaluated the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model (Brehmer, 1974; Carroll, 1963) tended to mimic the true function closely in extrapolation, and thus offered a poor account of the under and over-estimation biases shown in the human data. The log-polynomial adaptive regression model (Koh & Meyer, 1991) was able to mimic some of the systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred.\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature (Kruschke, 1992). ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance (see Figure 17). The EXAM model retains the same similarity-based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participants’ training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, by contrast, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM’s ability to account for cases where human participants learn and extrapolate a sinusoidal function (Bott & Heit, 2004) or to scenarios where different functions apply to different regions of the input space (Kalish et al., 2004), though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V-shaped input spaces (McDaniel et al., 2009).\n\n\n\n\n\n\n\n\nFigure 1: The generalization patterns of human particpiants observed in DeLosh et al. (1997) (reproduced from Figure 3 in their manuscript). Dots represent the average responses of human participants, and solid lines represent the true functions. The dashed vertical lines indicate the lower and upper bounds of the trained examples. Stimulii that fall within the dashed lines are interpolations of the training examples, while those that fall outside the dashed lines are extrapolations.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#variability-and-function-learning",
    "href": "Sections/HTW.html#variability-and-function-learning",
    "title": "HTW Project",
    "section": "Variability and Function Learning",
    "text": "Variability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#overview-of-present-study",
    "href": "Sections/HTW.html#overview-of-present-study",
    "title": "HTW Project",
    "section": "Overview Of Present Study",
    "text": "Overview Of Present Study\nThe present study investigates the influence of training variability on learning, generalization, and extrapolation in a uni-dimensional visuomotor function learning task. To the best of our knowledge, this research is the first to employ the classic constant vs. varied training manipulation, commonly used in the literature studying the benefits of variability, in the context of a uni-dimensional function learning task. Across three experiments, we compare constant and varied training conditions in terms of learning performance, extrapolation accuracy, and the ability to reliably discriminate between stimuli.\nTo account for the empirical results, we will apply a series of computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM). Notably, this study is the first to employ approximate Bayesian computation (ABC) to fit these models to individual subject data, enabling us to thoroughly investigate the full range of posterior predictions of each model, and to examine the ability of these influential models of function learning to account for both the group level and individual level data.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#methods",
    "href": "Sections/HTW.html#methods",
    "title": "HTW Project",
    "section": "Methods",
    "text": "Methods\nParticipants. A total of 183 participants were initially recruited from Indiana University Introductory Psychology Courses. Of these, 27 participants were excluded from further analysis due to meeting the exclusion criteria, resulting in a final sample of 156 participants. The exclusion criteria was defined as performance worse (i.e., larger deviations) than the condition average in either the training or testing stage of the experiment. The remaining participants were randomly assigned to one of two training conditions: varied training or constant training.\nTask. The “Hit The Wall” (HTW) visuomotor extrapolation task task was programmed in JavaScript, making use of the phaser.io game library. The HTW task involved launching a projectile such that it would strike the “wall” at the target speed indicated at the top of the screen (see Figure 2). The target velocities were given as a range, or band, of acceptable velocity values (e.g., band 800-1000). During the training stage, participants received feedback indicating whether they had hit the wall within the target velocity band, or how many units their throw was above or below the target band. Participants were instructed that only the x velocity component of the ball was relevant to the task. The y velocity, or the location at which the ball struck the wall, had no influence on the task feedback.\n\n\n\n\n\n\n\n\n\n\nFigure 2: The Hit the wall task. Participants launch the blue ball to hit the red wall at the target velocity band indicated at the top of the screen. The ball must be released from within the orange square - but the location of release, and the location at which the ball strikes the wall are both irrelevant to the task feedback.\n\n\n\n\n\nProcedure. All participants completed the task online. Participants were provided with a description of the experiment and indicated informed consent. Figure 3 illustrates the general procedure. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). Participants in the constant training condition trained on only one velocity band (800-1000) - the closest band to what would be the novel extrapolation bands in the testing stage.\nFollowing the training stage, participants proceeded immediately to the testing stage. Participants were tested from all six velocity bands, in two separate stages. In the novel extrapolation testing stage, participants completed “no-feedback” testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials. Participants were also tested from the three velocity bands that were trained by the varied condition (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training. The order in which participants completed the novel-extrapolation and testing-from-3-varied bands was counterbalanced across participants. A final training stage presented participants with “feedback” testing for each of the three extrapolation bands (100-300, 350-550, and 600-800).\n\n\n\n\n\n\n\n\n\n\ncluster\n\nTest Phase \n(Counterbalanced Order)\n\n\n\ndata1\n\n Varied Training \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1\n\nTest  \nNovel Bands \n100-300\n350-550\n600-800\n\n\n\ndata1-&gt;Test1\n\n\n\n\n\ndata2\n\n Constant Training \n800-1000\n\n\n\ndata2-&gt;Test1\n\n\n\n\n\nTest3\n\n    Final Test \n  Novel With Feedback  \n100-300\n350-550\n600-800\n\n\n\nTest2\n\n  Test \n  Varied Training Bands  \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest1-&gt;Test2\n\n\n\n\n\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\nFigure 3: Experiment 1 Design. Constant and Varied participants complete different training conditions.\n\n\n\n\n\n\n\nDisplay code\n# pacman::p_load(dplyr,purrr,tidyr,tibble,ggplot2,\n#   brms,tidybayes, rstanarm,emmeans,broom,bayestestR,\n#   stringr, here,conflicted, patchwork, knitr)\n# #options(brms.backend=\"cmdstanr\",mc.cores=4)\n# options(digits=2, scipen=999, dplyr.summarise.inform=FALSE)\n# walk(c(\"brms\",\"dplyr\",\"bayestestR\"), conflict_prefer_all, quiet = TRUE)\n# walk(c(\"Display_Functions\",\"org_functions\"), ~ source(here::here(paste0(\"Functions/\", .x, \".R\"))))\ne1 &lt;- readRDS(here(\"data/e1_08-21-23.rds\")) \ne1Sbjs &lt;- e1 |&gt; group_by(id,condit) |&gt; summarise(n=n())\ntestE1 &lt;- e1 |&gt; filter(expMode2 == \"Test\")\nnbins=5\ntrainE1 &lt;-  e1 |&gt; filter(expMode2==\"Train\") |&gt; group_by(id,condit, vb) |&gt; \n    mutate(Trial_Bin = cut( gt.train, breaks = seq(1, max(gt.train),length.out=nbins+1),include.lowest = TRUE, labels=FALSE)) \ntrainE1_max &lt;- trainE1 |&gt; filter(Trial_Bin == nbins, bandInt==800)\ntrainE1_avg &lt;- trainE1_max |&gt; group_by(id,condit) |&gt; summarise(avg = mean(dist))",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#analyses-strategy",
    "href": "Sections/HTW.html#analyses-strategy",
    "title": "HTW Project",
    "section": "Analyses Strategy",
    "text": "Analyses Strategy\nAll data processing and statistical analyses were performed in R version 4.32 (Team, 2020). To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R (Bürkner, 2017), and descriptive stats and tables were extracted with the BayestestR package (Makowski et al., 2019). Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as avoid convergence issues common to the frequentist analogues of our mixed models.\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 discarded as warmup chains. Rhat values were within an acceptable range, with values &lt;=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for the random effects. For each model, we report 1) the mean values of the posterior distribution for the parameters of interest, 2) the lower and upper credible intervals (CrI), and the probability of direction value (pd).\n\n\n\nTable 1: Statistical Model Specifications. The specifications for the Bayesian regression models used in the analyses of each of the 3 experiments. Comparisons of accuracy use absolute deviation as the dependent variable, while comparisons of discrimination use the raw velocities produced by participants as the dependent variable.\n\n\n\n\n\n\n\n\n\n\nGroup Comparison\nCode\nData\n\n\n\n\nEnd of Training Accuracy\nbrm(Abs. Deviation ~ condit)\nFinal Training Block\n\n\nTest Accuracy\nbrm(Abs. Deviation ~ condit * bandType + (1|id) + (1|bandInt)\nAll Testing trials\n\n\nBand Discrimination\nbrm(vx ~ condit * band +(1 + bandInt|id)\nAll Testing Trials\n\n\n\n\n\n\n\n\nIn each experiment we compare varied and constant conditions in terms of 1) accuracy in the final training block; 2) testing accuracy as a function of band type (trained vs. extrapolation bands); 3) extent of discrimination between all six testing bands. We quantified accuracy as the absolute deviation between the response velocity and the nearest boundary of the target band. Thus, when the target band was velocity 600-800, throws of 400, 650, and 900 would result in deviation values of 200, 0, and 100, respectively. The degree of discrimination between bands was measured by fitting a linear model predicting the response velocity as a function of the target velocity. Participants who reliably discriminated between velocity bands tended to have slope values ~1, while participants who made throws irrespective of the current target band would have slopes ~0.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#results",
    "href": "Sections/HTW.html#results",
    "title": "HTW Project",
    "section": "Results",
    "text": "Results\n\n\nDisplay code\np1 &lt;- trainE1 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e1_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 4: Experiment 1 - Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e1-train-dist\n##| tbl-cap: \"Experiment 1 - Learning curves. \"\n##| output: asis\n\nbmm_e1_train&lt;- trainE1_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e1_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\nmtr1 &lt;- as.data.frame(describe_posterior(bmm_e1_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# mtr1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#    kable(booktabs = TRUE)\n\ncdtr1 &lt;- get_coef_details(bmm_e1_train, \"conditVaried\")\n\n\n\n\n\nTable 2: Experiment 1 - End of training performance. Comparing final training block accuracy in the band common to both groups. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive estimates indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n106.34\n95.46\n117.25\n1\n\n\nconditVaried\n79.64\n57.92\n101.63\n1\n\n\n\n\n\n\n\n\nTraining. Figure 4 displays the average deviations across training blocks for the varied group, which trained on three velocity bands, and the constant group, which trained on one velocity band. To compare the training conditions at the end of training, we analyzed performance on the 800-1000 velocity band, which both groups trained on. The full model results are shown in Table 1. The varied group had a significantly greater deviation from the target band than the constant group in the final training block, (\\(\\beta\\) = 79.64, 95% CrI [57.92, 101.63]; pd = 100%).\n\n\nDisplay code\n##| label: tbl-e1-bmm-dist\n##| tbl-cap: \"E1. Training vs. Extrapolation\"\n#| \nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e1_dist_Cond_Type_RF_2\")\nbmtd &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE1, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted1 &lt;- as.data.frame(describe_posterior(bmtd, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted1) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n# \n# r_bandInt_params &lt;- get_variables(bmtd)[grepl(\"r_id:bandInt\", get_variables(bmtd))]\n# posterior_summary(bmtd,variable=r_bandInt_params)\n\n# mted1 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt; kable(booktabs = TRUE)\ncdted1 &lt;- get_coef_details(bmtd, \"conditVaried\")\ncdted2 &lt;-get_coef_details(bmtd, \"bandTypeExtrapolation\")\ncdted3 &lt;-get_coef_details(bmtd, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\n\nTable 3: Experiment 1 testing accuracy. Main effects of condition and band type (training vs. extrapolation bands), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproporionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n152.55\n70.63\n229.85\n1.0\n\n\nconditVaried\n39.00\n-21.10\n100.81\n0.9\n\n\nbandTypeExtrapolation\n71.51\n33.24\n109.60\n1.0\n\n\nconditVaried:bandTypeExtrapolation\n66.46\n32.76\n99.36\n1.0\n\n\n\n\n\n\nTesting. To compare accuracy between groups in the testing stage, we fit a Bayesian mixed effects model predicting deviation from the target band as a function of training condition (varied vs. constant) and band type (trained vs. extrapolation), with random intercepts for participants and bands. The model results are shown in Table 3. The main effect of training condition was not significant (\\(\\beta\\) = 39, 95% CrI [-21.1, 100.81]; pd = 89.93%). The extrapolation testing items had a significantly greater deviation than the training bands (\\(\\beta\\) = 71.51, 95% CrI [33.24, 109.6]; pd = 99.99%). Most importantly, the interaction between training condition and band type was significant (\\(\\beta\\) = 66.46, 95% CrI [32.76, 99.36]; pd = 99.99%), As shown in Figure 5, the varied group had disproportionately larger deviations compared to the constant group in the extrapolation bands.\n\n\nDisplay code\npe1td &lt;- testE1 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target Band\")\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe1ce &lt;- bmtd |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Target Band\", x=\"Band Type\")\n\np2 &lt;- (pe1td + pe1ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 5: Experiment 1 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e1-bmm-vx\n##| tbl-cap: \"Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\ne1_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e1_vxBMM) |&gt; kable(booktabs = TRUE)\n\ncd1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\n\n\nTable 4: Experiment 1 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condit and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n408.55\n327.00\n490.61\n1.00\n\n\nconditVaried\n164.05\n45.50\n278.85\n1.00\n\n\nBand\n0.71\n0.62\n0.80\n1.00\n\n\ncondit*Band\n-0.14\n-0.26\n-0.01\n0.98\n\n\n\n\n\n\nFinally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. See Table 4 for the full model results. The estimated coefficient for training condition (\\(\\beta\\) = 164.05, 95% CrI [45.5, 278.85], pd = 99.61%) suggests that the varied group tends to produce harder throws than the constant group, though this is not, in and of itself, useful for assessing discrimination. Most relevant to the issue of discrimination is the coefficient on the Band predictor (\\(\\beta\\) = 0.71 95% CrI [0.62, 0.8], pd = 100%). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The significant negative estimate for the interaction between slope and condition (\\(\\beta\\) = -0.14, 95% CrI [-0.26, -0.01], pd = 98.39%), indicates that the discrimination was modulated by training condition, with the varied participants showing less sensitivity between bands than the constant condition (see Figure 6 and Figure 7).\n\n\nDisplay code\ntestE1 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 6: Experiment 1. Empirical distribution of velocities produced in the testing stage. Translucent bands with dashed lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nDisplay code\npe1vce &lt;- e1_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE1$vb), \n                     limits = c(0, 1400)) + \n  scale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e1_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e1_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e1_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e1_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e1Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\n\npid_den1 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes1 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n     theme(legend.title=element_blank(), \n           axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\n\np3 &lt;- (pe1vce + pid_den1 + pid_slopes1) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e1_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\n\n\nFigure 7: Experiment 1 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination between target bands. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#experiment-1-summary",
    "href": "Sections/HTW.html#experiment-1-summary",
    "title": "HTW Project",
    "section": "Experiment 1 Summary",
    "text": "Experiment 1 Summary\nIn Experiment 1, we investigated how variability in training influenced participants’ ability to learn and extrapolate in a visuomotor task. Our findings that training with variable conditions resulted in lower final training performance are consistent with much of the prior research on the influence of training variability (Raviv et al., 2022; Soderstrom & Bjork, 2015), and are particularly unsurprising in the present work, given that the constant group received three times the amount of training on the velocity band common to the two conditions.\nMore importantly, the varied training group exhibited significantly larger deviations from the target velocity bands during the testing phase, particularly for the extrapolation bands that were not encountered by either condition during training.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#methods-procedure",
    "href": "Sections/HTW.html#methods-procedure",
    "title": "HTW Project",
    "section": "Methods & Procedure",
    "text": "Methods & Procedure\nInitially, 131 participants were recruited. After applying the same exclusion procedure as in Experiment 1, 21 participants were excluded, resulting in a final sample of 110 participants who completed the experiment (Varied: 55, Constant: 55). The task and procedure of Experiment 2 was identical to Experiment 1, with the exception that the training and testing bands were reversed (see Figure 8). The Varied group trained on bands 100-300, 350-550, 600-800, and the constant group trained on band 600-800. Both groups were tested from all six bands.\n\n\n\n\n\n\n\n\n\n\ncluster\n\nTest Phase \n(Counterbalanced Order)\n\n\n\ndata1\n\n Varied Training \n100-300\n350-550\n600-800\n\n\n\nTest1\n\nTest  \nNovel Bands  \n800-1000\n1000-1200\n1200-1400\n\n\n\ndata1-&gt;Test1\n\n\n\n\n\ndata2\n\n Constant Training \n600-800\n\n\n\ndata2-&gt;Test1\n\n\n\n\n\nTest3\n\n    Final Test \n  Novel With Feedback  \n800-1000\n1000-1200\n1200-1400\n\n\n\nTest2\n\n  Test \n  Varied Training Bands  \n100-300\n350-550\n600-800\n\n\n\nTest1-&gt;Test2\n\n\n\n\n\nTest2-&gt;Test3\n\n\n\n\n\n\n\n\nFigure 8: Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#results-1",
    "href": "Sections/HTW.html#results-1",
    "title": "HTW Project",
    "section": "Results",
    "text": "Results\n\n\nDisplay code\np1 &lt;- trainE2 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    facet_wrap(~vb)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e2_train_deviation.png\"), p1, width = 8, height = 4,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 9: Experiment 2 Training Stage. Deviations from target band across training blocks. Lower values represent greater accuracy.\n\n\n\n\n\n\n\nDisplay code\nbmm_e2_train &lt;- trainE2_max %&gt;% \n  brm(dist ~ condit, \n      file=here(\"data/model_cache/e2_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\nmtr2 &lt;- as.data.frame(describe_posterior(bmm_e2_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\ncolnames(mtr2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n\ncdtr2 &lt;- get_coef_details(bmm_e2_train, \"conditVaried\")\n# mtr2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\n\n\n\n\nTable 5: Experiment 2 - End of training performance. The Intercept represents the average of the baseline condition (constant training), and the conditVaried coefficient reflects the difference between the constant and varied groups. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n91.01\n80.67\n101.26\n1\n\n\nconditVaried\n36.15\n16.35\n55.67\n1\n\n\n\n\n\n\n\n\nTraining. Figure 9 presents the deviations across training blocks for both constant and varied training groups. We again compared training performance on the band common to both groups (600-800). The full model results are shown in Table 1. The varied group had a significantly greater deviation than the constant group in the final training block, ( \\(\\beta\\) = 36.15, 95% CrI [16.35, 55.67]; pd = 99.95%).\n\n\nDisplay code\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e2_dist_Cond_Type_RF_2\")\nbmtd2 &lt;- brm(dist ~ condit * bandType + (1|bandInt) + (1|id), \n    data=testE2, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted2 &lt;- as.data.frame(describe_posterior(bmtd2, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted2) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted2 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\ncd2ted1 &lt;- get_coef_details(bmtd2, \"conditVaried\")\ncd2ted2 &lt;-get_coef_details(bmtd2, \"bandTypeExtrapolation\")\ncd2ted3 &lt;-get_coef_details(bmtd2, \"conditVaried:bandTypeExtrapolation\")\n\n\n\n\n\nTable 6: Experiment 2 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition (constant training & trained bands). Larger coefficients indicate larger deviations from the baselines - and a positive interaction coefficient indicates disproportionate deviation for the varied condition on the extrapolation bands. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate).\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n190.91\n125.03\n259.31\n1.00\n\n\nconditVaried\n-20.58\n-72.94\n33.08\n0.78\n\n\nbandTypeExtrapolation\n38.09\n-6.94\n83.63\n0.95\n\n\nconditVaried:bandTypeExtrapolation\n82.00\n41.89\n121.31\n1.00\n\n\n\n\n\n\n \n\nTesting Accuracy. The analysis of testing accuracy examined deviations from the target band as influenced by training condition (Varied vs. Constant) and band type (training vs. extrapolation bands). The results, summarized in Table 6, reveal no significant main effect of training condition (\\(\\beta\\) = -20.58, 95% CrI [-72.94, 33.08]; pd = 77.81%). However, the interaction between training condition and band type was significant (\\(\\beta\\) = 82, 95% CrI [41.89, 121.31]; pd = 100%), with the varied group showing disproportionately larger deviations compared to the constant group on the extrapolation bands (see Figure 10).\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\npe2td &lt;- testE2 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\n\npe2ce &lt;- bmtd2 |&gt; emmeans( ~condit + bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\")\n\np2 &lt;- (pe2td + pe2ce) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-dev.png\"), p2, width=8, height=4, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 10: Experiment 2 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e2-bmm-vx\n##| tbl-cap: \"Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne2_vxBMM &lt;- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n#GetModelStats(e2_vxBMM ) |&gt; kable(escape=F,booktabs=T, caption=\"Fit to all 6 bands\")\n\ncd2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc2 &lt;- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef2 &lt;- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n\n\n\n\n\nTable 7: Experiment 2 Testing Discrimination. Bayesian Mixed Model Predicting velocity as a function of condition (Constant vs. Varied) and Velocity Band. Larger coefficients for the Band term reflect a larger slope, or greater sensitivity/discrimination. The interaction between condition and Band indicates the difference between constant and varied slopes. CrI values indicate 95% credible intervals. pd is the probability of direction (the % of the posterior on the same side of 0 as the coefficient estimate)\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n362.64\n274.85\n450.02\n1.00\n\n\nconditVaried\n-8.56\n-133.97\n113.98\n0.55\n\n\nBand\n0.71\n0.58\n0.84\n1.00\n\n\ncondit*Band\n-0.06\n-0.24\n0.13\n0.73\n\n\n\n\n\n\nTesting Discrimination. Finally, to assess the ability of both conditions to discriminate between velocity bands, we fit a model predicting velocity as a function of training condition and velocity band, with random intercepts and random slopes for each participant. The full model results are shown in Table 7. The overall slope on target velocity band predictor was significantly positive, (\\(\\beta\\) = 0.71, 95% CrI [0.58, 0.84]; pd= 100%), indicating that participants exhibited discrimination between bands. The interaction between slope and condition was not significant, (\\(\\beta\\) = -0.06, 95% CrI [-0.24, 0.13]; pd= 72.67%), suggesting that the two conditions did not differ in their ability to discriminate between bands (see Figure 11 and Figure 12).\n\n\nDisplay code\ntestE2 %&gt;% group_by(id,vb,condit) |&gt; plot_distByCondit()\n\n\n\n\n\n\n\n\nFigure 11: Experiment 2. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n}\n\npe2vce &lt;- e2_vxBMM |&gt; emmeans( ~condit + bandInt,re_formula=NA, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e2_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e2_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction &lt;- fixef(e2_vxBMM)[,1][\"conditVaried:bandInt\"]\n\nre &lt;- data.frame(ranef(e2_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e2Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction*(condit==\"Varied\"),slope = Estimate + adjust )\n\npid_den2 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\")\n\npid_slopes2 &lt;- re |&gt;  mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n      theme(legend.title=element_blank(), \n        axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_wrap2(~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe2vce + pid_den2 + pid_slopes2) + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e2_test-vx.png\"), p3,width=9,height=11, bg=\"white\",dpi=600)\np3\n\n\n\n\n\n\n\n\nFigure 12: Experiment 2 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#experiment-2-summary",
    "href": "Sections/HTW.html#experiment-2-summary",
    "title": "HTW Project",
    "section": "Experiment 2 Summary",
    "text": "Experiment 2 Summary\nExperiment 2 extended the findings of Experiment 1 by examining the effects of training variability on extrapolation performance in a visuomotor function learning task, but with reversed training and testing bands. Similar to Experiment 1, the Varied group exhibited poorer performance during training and testing. However unlike experiment 1, the Varied and Constant groups did not show a significant difference in their discrimination between bands.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#methods-procedure-1",
    "href": "Sections/HTW.html#methods-procedure-1",
    "title": "HTW Project",
    "section": "Methods & Procedure",
    "text": "Methods & Procedure\nThe major adjustment of Experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the prior experiments. After each training throw, participants are informed whether a throw was too soft, too hard, or correct (i.e. within the target velocity range). All other aspects of the task and design are identical to Experiments 1 and 2. We utilized the order of training and testing bands from both of the prior experiments, thus assigning participants to both an order condition (Original or Reverse) and a training condition (Constant or Varied). Participants were once again recruited from the online Indiana University Introductory Psychology Course pool. 227 participants were recruited initially. Following exclusions, 195 participants were included in the final analysis, n=51 in the Constant-Original condition, n=59 in the Constant-Reverse condition, n=39 in the Varied-Original condition, and n=46 in the Varied-Reverse condition.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#results-2",
    "href": "Sections/HTW.html#results-2",
    "title": "HTW Project",
    "section": "Results",
    "text": "Results\n\n\nDisplay code\nbmm_e3_train &lt;- trainE3_max %&gt;% \n  brm(dist ~ condit*bandOrder, \n      file=here(\"data/model_cache/e3_train_deviation\"),\n      data = .,\n      iter = 2000,\n      chains = 4,\n      control = list(adapt_delta = .94, max_treedepth = 13))\n\n# mtr3 &lt;- as.data.frame(describe_posterior(bmm_e3_train, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mtr3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mtr3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T) \n\ncd3tr1 &lt;- get_coef_details(bmm_e3_train, \"conditVaried\")\ncd3tr2 &lt;-get_coef_details(bmm_e3_train, \"bandOrderReverse\")\ncd3tr3 &lt;-get_coef_details(bmm_e3_train, \"conditVaried:bandOrderReverse\")\n\n\n\n\n\nTable 8: Experiment 3 - End of training performance. The Intercept represents the average of the baseline condition (constant training & original band order), the conditVaried coefficient reflects the difference between the constant and varied groups, and the bandOrderReverse coefficient reflects the difference between original and reverse order. A larger positive coefficient indicates a greater deviation (lower accuracy) for the varied group. The negative value for the interaction between condit and bandOrder indicates that varied condition with reverse order had significantly lower deviations than the varied condition with the original band order\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n121.86\n109.24\n134.60\n1.00\n\n\nconditVaried\n64.93\n36.99\n90.80\n1.00\n\n\nbandOrderReverse\n1.11\n-16.02\n18.16\n0.55\n\n\nconditVaried:bandOrderReverse\n-77.02\n-114.16\n-39.61\n1.00\n\n\n\n\n\n\nTraining. Figure 13 displays the average deviations from the target band across training blocks, and Table 8 shows the results of the Bayesian regression model predicting the deviation from the common band at the end of training (600-800 for reversed order, and 800-1000 for original order conditions). The main effect of training condition is significant, with the varied condition showing larger deviations ( \\(\\beta\\) = 64.93, 95% CrI [36.99, 90.8]; pd = 100%). The main effect of band order is not significant \\(\\beta\\) = 1.11, 95% CrI [-16.02, 18.16]; pd = 55.4%, however the interaction between training condition and band order is significant, with the varied condition showing greater accuracy in the reverse order condition ( \\(\\beta\\) = -77.02, 95% CrI [-114.16, -39.61]; pd = 100%).\n\n\nDisplay code\np1 &lt;- trainE3 |&gt; ggplot(aes(x = Trial_Bin, y = dist, color = condit)) +\n    stat_summary(geom = \"line\", fun = mean) +\n    stat_summary(geom = \"errorbar\", fun.data = mean_se, width = .4, alpha = .7) +\n    ggh4x::facet_nested_wrap(~bandOrder*vb,ncol=3)+\n    scale_x_continuous(breaks = seq(1, nbins + 1)) +\n    theme(legend.title=element_blank()) + \n    labs(y = \"Absolute Deviation\", x=\"Training Block\") \n#ggsave(here(\"Assets/figs/e3_train_deviation.png\"), p1, width = 9, height = 8,bg=\"white\")\np1\n\n\n\n\n\n\n\n\nFigure 13: Experiment 3 training. Deviations from target band during training, shown separately for groups trained with the original order (used in E1) and reverse order (used in E2).\n\n\n\n\n\n\n\nDisplay code\n#options(brms.backend=\"cmdstanr\",mc.cores=4)\nmodelFile &lt;- paste0(here::here(\"data/model_cache/\"), \"e3_dist_Cond_Type_RF_2\")\nbmtd3 &lt;- brm(dist ~ condit * bandType*bandOrder + (1|bandInt) + (1|id), \n    data=testE3, file=modelFile,\n    iter=5000,chains=4, control = list(adapt_delta = .94, max_treedepth = 13))\n                        \n# mted3 &lt;- as.data.frame(describe_posterior(bmtd3, centrality = \"Mean\"))[, c(1,2,4,5,6)]\n# colnames(mted3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")\n# mted3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(booktabs=TRUE) \n\n#ce_bmtd3 &lt;- plot(conditional_effects(bmtd3),points=FALSE,plot=FALSE)\n#wrap_plots(ce_bmtd3)\n\n#ggsave(here::here(\"Assets/figs\", \"e3_cond_effects_dist.png\"), wrap_plots(ce_bmtd3), width=11, height=11, bg=\"white\")\n\ncd3ted1 &lt;- get_coef_details(bmtd3, \"conditVaried\")\ncd3ted2 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation\")\ncd3ted3 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation\")\ncd3ted4 &lt;-get_coef_details(bmtd3, \"bandOrderReverse\")\ncd3ted5 &lt;-get_coef_details(bmtd3, \"conditVaried:bandOrderReverse\")\ncd3ted6 &lt;-get_coef_details(bmtd3, \"bandTypeExtrapolation:bandOrderReverse\")\ncd3ted7 &lt;-get_coef_details(bmtd3, \"conditVaried:bandTypeExtrapolation:bandOrderReverse\")\n\n\n\n\n\nTable 9: Experiment 3 testing accuracy. Main effects of condition and band type (training vs. extrapolation), and the interaction between the two factors. The Intercept represents the baseline condition, (constant training, trained bands & original order), and the remaining coefficients reflect the deviation from that baseline. Positive coefficients thus represent worse performance relative to the baseline, and a positive interaction coefficient indicates disproportionate deviation for the varied condition or reverse order condition.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n288.65\n199.45\n374.07\n1.00\n\n\nconditVaried\n-40.19\n-104.68\n23.13\n0.89\n\n\nbandTypeExtrapolation\n-23.35\n-57.28\n10.35\n0.92\n\n\nbandOrderReverse\n-73.72\n-136.69\n-11.07\n0.99\n\n\nconditVaried:bandTypeExtrapolation\n52.66\n14.16\n90.23\n1.00\n\n\nconditVaried:bandOrderReverse\n-37.48\n-123.28\n49.37\n0.80\n\n\nbandTypeExtrapolation:bandOrderReverse\n80.69\n30.01\n130.93\n1.00\n\n\nconditVaried:bandTypeExtrapolation:bandOrder\n30.42\n-21.00\n81.65\n0.87\n\n\n\n\n\n\nTesting Accuracy. Table 9 presents the results of the Bayesian mixed effects model predicting absolute deviation from the target band during the testing stage. There was no significant main effect of training condition,\\(\\beta\\) = -40.19, 95% CrI [-104.68, 23.13]; pd = 89.31%, or band type,\\(\\beta\\) = -23.35, 95% CrI [-57.28, 10.35]; pd = 91.52%. However the effect of band order was significant, with the reverse order condition showing lower deviations, \\(\\beta\\) = -73.72, 95% CrI [-136.69, -11.07]; pd = 98.89%. The interaction between training condition and band type was also significant \\(\\beta\\) = 52.66, 95% CrI [14.16, 90.23]; pd = 99.59%, with the varied condition showing disproprionately large deviations on the extrapolation bands compared to the constant group. There was also a significant interaction between band type and band order, \\(\\beta\\) = 80.69, 95% CrI [30.01, 130.93]; pd = 99.89%, such that the reverse order condition showed larger deviations on the extrapolation bands. No other interactions were significant.\n\n\n\nDisplay code\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3td &lt;- testE3 |&gt;  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n    facet_wrap(~bandOrder,ncol=1) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +\n  labs(x=\"Band\", y=\"Deviation From Target\")\n\n\npe3ce &lt;- bmtd3 |&gt; emmeans( ~condit *bandOrder*bandType) |&gt;\n  gather_emmeans_draws() |&gt;\n condEffects(bandType) + labs(y=\"Absolute Deviation From Band\", x=\"Band Type\") + \n facet_wrap(~bandOrder,ncol=1)\n\np2 &lt;- pe3td + pe3ce + plot_annotation(tag_levels= 'A')\n#ggsave(here::here(\"Assets/figs\", \"e3_test-dev.png\"), p2, width=9, height=8, bg=\"white\")\np2\n\n\n\n\n\n\n\n\nFigure 14: Experiment 3 Testing Accuracy. A) Empirical Deviations from target band during testing without feedback stage. B) Conditional effect of condition (Constant vs. Varied) and testing band type (trained bands vs. novel extrapolation bands) on testing accuracy. Shown separately for groups trained with the original order (used in E1) and reverse order (used in E2). Error bars represent 95% credible intervals.\n\n\n\n\n\n\n\nDisplay code\n##| label: tbl-e3-bmm-vx\n##| tbl-cap: \"Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band\"\n\ne3_vxBMM &lt;- brm(vx ~ condit * bandOrder * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\n\n# m1 &lt;- as.data.frame(describe_posterior(e3_vxBMM, centrality = \"Mean\"))\n# m2 &lt;- fixef(e3_vxBMM)\n# mp3 &lt;- m1[, c(1,2,4,5,6)]\n# colnames(mp3) &lt;- c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")                       \n# mp3 |&gt; mutate(across(where(is.numeric), \\(x) round(x, 2))) |&gt;\n#   tibble::remove_rownames() |&gt; \n#   mutate(Term = stringr::str_replace_all(Term, \"b_bandInt\", \"Band\")) |&gt;\n#   mutate(Term = stringr::str_remove(Term, \"b_\")) |&gt;\n#   kable(escape=F,booktabs=T)\n\n#wrap_plots(plot(conditional_effects(e3_vxBMM),points=FALSE,plot=FALSE))\n\ncd1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 &lt;- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 &lt;- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\nintCoef2 &lt;- get_coef_details(e3_vxBMM, \"bandOrderReverse:bandInt\")\ncoef3 &lt;- get_coef_details(e3_vxBMM,\"conditVaried:bandOrderReverse:bandInt\")\n\n\n\n\n\nTable 10: Experiment 3 testing discrimination. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band. The Intercept represents the baseline condition (constant training & original order), and the Band coefficient represents the slope for the baseline condition. The interaction terms which include condit and Band (e.g., conditVaried:Band & conditVaried:bandOrderReverse:band) respectively indicate how the slopes of the varied-original condition differed from the baseline condition, and how varied-reverse condition differed from the varied-original condition\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\n95% CrI Lower\n95% CrI Upper\npd\n\n\n\n\nIntercept\n601.83\n504.75\n699.42\n1.00\n\n\nconditVaried\n12.18\n-134.94\n162.78\n0.56\n\n\nbandOrderReverse\n13.03\n-123.89\n144.67\n0.58\n\n\nBand\n0.49\n0.36\n0.62\n1.00\n\n\nconditVaried:bandOrderReverse\n-338.15\n-541.44\n-132.58\n1.00\n\n\nconditVaried:Band\n-0.04\n-0.23\n0.15\n0.67\n\n\nbandOrderReverse:band\n-0.10\n-0.27\n0.08\n0.86\n\n\nconditVaried:bandOrderReverse:band\n0.42\n0.17\n0.70\n1.00\n\n\n\n\n\n\nTesting Discrimination. The full results of the discrimination model are presented in Table 9. For the purposes of assessing group differences in discrimination, only the coefficients including the band variable are of interest. The baseline effect of band represents the slope cofficient for the constant training - original order condition, this effect was significant \\(\\beta\\) = 0.49, 95% CrI [0.36, 0.62]; pd = 100%. Neither of the two way interactions reached significance, \\(\\beta\\) = -0.04, 95% CrI [-0.23, 0.15]; pd = 66.63%, \\(\\beta\\) = -0.1, 95% CrI [-0.27, 0.08]; pd = 86.35%. However, the three way interaction between training condition, band order, and target band was significant, \\(\\beta\\) = 0.42, 95% CrI [0.17, 0.7]; pd = 99.96% - indicating a greater slope for the varied condition trained with reverse order bands. This interaction is shown in Figure 15, where the steepness of the best fitting line for the varied-reversed condition is noticably steeper than the other conditions.\n\n\nDisplay code\n##| column: screen-inset-right\n# testE3 |&gt; filter(bandOrder==\"Original\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit()\n# testE3 |&gt; filter(bandOrder==\"Reverse\")|&gt; group_by(id,vb,condit) |&gt; plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |&gt; group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + \n   ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\n\n\n\n\n\n\nFigure 15: Experiment 3. Empirical distribution of velocities produced in the testing stage. Translucent bands with dash lines indicate the correct range for each velocity band.\n\n\n\n\n\n\n\n\nDisplay code\n# pe3tv &lt;- testE3 %&gt;% group_by(id,vb,condit,bandOrder) |&gt; plot_distByCondit() + ggh4x::facet_nested_wrap(bandOrder~condit,scale=\"free_x\")\n\n\ncondEffects &lt;- function(m,xvar){\n  m |&gt; ggplot(aes(x = {{xvar}}, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() + \n  stat_halfeye(alpha=.1, height=.5) +\n  theme(legend.title=element_blank(),axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n  \n}\n\npe3vce &lt;- e3_vxBMM |&gt; emmeans( ~condit* bandOrder* bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |&gt;\n  gather_emmeans_draws() |&gt; \n  condEffects(bandInt) +\n  facet_wrap(~bandOrder,ncol=1) +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) + \nscale_y_continuous(expand=expansion(add=100),breaks=round(seq(0,2000,by=200),2)) +\n  theme(legend.title=element_blank()) + \n  labs(y=\"Velcoity\", x=\"Band\")\n\nfe &lt;- fixef(e3_vxBMM)[,1]\nfixed_effect_bandInt &lt;- fixef(e3_vxBMM)[,1][\"bandInt\"]\nfixed_effect_interaction1 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandInt\"]\nfixed_effect_interaction2 &lt;- fixef(e3_vxBMM)[,1][\"bandOrderReverse:bandInt\"]\nfixed_effect_interaction3 &lt;- fixef(e3_vxBMM)[,1][\"conditVaried:bandOrderReverse:bandInt\"]\n\nre &lt;- data.frame(ranef(e3_vxBMM, pars = \"bandInt\")$id[, ,'bandInt']) |&gt; \n  rownames_to_column(\"id\") |&gt; \n  left_join(e3Sbjs,by=\"id\") |&gt;\n  mutate(adjust= fixed_effect_bandInt + fixed_effect_interaction1*(condit==\"Varied\") + \n           fixed_effect_interaction2*(bandOrder==\"Reverse\") + \n           fixed_effect_interaction3*(condit==\"Varied\" & bandOrder==\"Reverse\"),\n  slope = Estimate + adjust )\n\npid_den3 &lt;- ggplot(re, aes(x = slope, fill = condit)) + \n  geom_density(alpha=.5) + \n  xlim(c(min(re$slope)-.3, max(re$slope)+.3))+\n  geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n   theme(legend.title=element_blank()) + \n  labs(x=\"Slope Coefficient\",y=\"Density\") +\n  facet_wrap(~bandOrder,ncol=1)\n\npid_slopes3 &lt;- re |&gt;  \n    mutate(id=reorder(id,slope)) |&gt;\n  ggplot(aes(y=id, x=slope,fill=condit,color=condit)) + \n    geom_pointrange(aes(xmin=Q2.5+adjust, xmax=Q97.5+adjust)) + \n    geom_vline(xintercept = 1, linetype=\"dashed\",alpha=.5) +\n    theme(legend.title=element_blank(), \n      axis.text.y = element_text(size=6) ) + \n    labs(x=\"Estimated Slope\", y=\"Participant\")  + \n    ggh4x::facet_nested_wrap(bandOrder~condit,axes=\"all\",scales=\"free_y\")\n\np3 &lt;- (pe3vce + pid_den3 + pid_slopes3) + plot_annotation(tag_levels= 'A')\n\n#ggsave(here::here(\"Assets/figs\", \"e3_test-vx.png\"), p3,width=11,height=13, bg=\"white\",dpi=800)\np3\n\n\n\n\n\n\n\n\nFigure 16: Experiment 3 Discrimination. A) Conditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands. B) The distribution of slope coefficients for each condition. Larger slopes indicates better discrimination. C) Individual participant slopes. Error bars represent 95% HDI.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#experiment-3-summary",
    "href": "Sections/HTW.html#experiment-3-summary",
    "title": "HTW Project",
    "section": "Experiment 3 Summary",
    "text": "Experiment 3 Summary\nIn Experiment 3, we investigated the effects of training condition (constant vs. varied) and band type (training vs. extrapolation) on participants’ accuracy and discrimination during the testing phase. Unlike the previous experiments, participants received only ordinal, not continuous valued, feedback during the training phase. Additionally, Experiment 3 included both the original order condition from Experiment 1 and the reverse order condition from Experiment 2. The results revealed no significant main effects of training condition on testing accuracy, nor was there a significant difference between groups in band discrimination. However, we observed a significant three-way interaction for the discrimination analysis, indicating that the varied condition showed a steeper slope coefficient on the reverse order bands compared to the constant condition. This result suggests that varied training enhanced participants’ ability to discriminate between velocity bands, but only when the band order was reversed during testing.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#alm-exam",
    "href": "Sections/HTW.html#alm-exam",
    "title": "HTW Project",
    "section": "ALM & Exam",
    "text": "ALM & Exam\nALM is a localist neural network model (Page, 2000), with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus ( a_i(X) = exp(-c(X - X_i)^2) ). So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on the value of the generalization parameter, the nearby units (e.g., 54 and 56; 53 and 57) may also activate to some degree. The units in the input layer activate as a function of their similarity to a presented stimulus. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\nThe EXAM model is an extension of ALM, with the same learning rule and representational scheme for input and output units. EXAM differs from ALM only in its response rule, as it includes a linear extrapolation mechanism for generating novel responses. When a novel test stimulus, \\(X\\), is presented, EXAM first identifies the two nearest training stimuli, \\(X_1\\) and \\(X_2\\), that bracket \\(X\\). This is done based on the Gaussian activation of input nodes, similar to ALM, but focuses on identifying the closest known points for extrapolation.\nSlope Calculation: EXAM calculates a local slope, \\(S\\), using the responses associated with \\(X_1\\) and \\(X_2\\). This is computed as:\n\\[\n   S = \\frac{m(X_{1}) - m(X_{2})}{X_{1} - X_{2}}\n   \\]\nwhere \\(m(X_1)\\) and \\(m(X_2)\\) are the output values from ALM corresponding to the \\(X_1\\) and \\(X_2\\) inputs.\nResponse Generation: The response for the novel stimulus \\(X\\) is then extrapolated using the slope \\(S\\):\n\\[\n   E[Y|X] = m(X_1) + S \\cdot |X - X_1|\n   \\]\nHere, \\(m(X_1)\\) is the ALM response value from the training data for the stimulus closest to \\(X\\), and \\((X - X_1)\\) represents the distance between the novel stimulus and the nearest training stimulus.\nAlthough this extrapolation rule departs from a strictly similarity-based generalization mechanism, EXAM is distinct from pure rule-based models in that it remains constrained by the weights learned during training. EXAM retrieves the two nearest training inputs, and the ALM responses associated with those inputs, and computes the slope between these two points. The slope is then used to extrapolate the response to the novel test stimulus. Because EXAM requires at least two input-output pairs to generate a response, additional assumptions were required in order for it to generate resposnes for the constant group. We assumed that participants come to the task with prior knowledge of the origin point (0,0), which can serve as a reference point necessary for the model to generate responses for the constant group. This assumption is motivated by previous function learning research (Brown & Lacroix, 2017), which through a series of manipulations of the y intercept of the underlying function, found that participants consistently demonstrated knowledge of, or a bias towards, the origin point (see Kwantes & Neal (2006) for additional evidence of such a bias in function learning tasks).\nSee Table 11 for a full specification of the equations that define ALM and EXAM, and Figure 17 for a visual representation of the ALM model.\n\n\n\n\nTable 11: ALM & EXAM Equations\n\n\n\n\n\n\n\n\n\n\n\nALM Response Generation\n\n\n\n\n\nInput Activation\n\\(a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}\\)\nInput nodes activate as a function of Gaussian similarity to stimulus\n\n\nOutput Activation\n\\(O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)\\)\nOutput unit \\(O_j\\) activation is the weighted sum of input activations and association weights\n\n\nOutput Probability\n\\(P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}\\)\nThe response, \\(Y_j\\) probabilites computed via Luce’s choice rule\n\n\nMean Output\n\\(m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}\\)\nWeighted average of probabilities determines response to X\n\n\n\nALM Learning\n\n\n\nFeedback\n\\(f_j(Z) = e^{-c(Z-Y_j)^2}\\)\nfeedback signal Z computed as similarity between ideal response and observed response\n\n\nmagnitude of error\n\\(\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)\\)\nDelta rule to update weights.\n\n\nUpdate Weights\n\\(w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}\\)\nUpdates scaled by learning rate parameter \\(\\eta\\).\n\n\n\nEXAM Extrapolation\n\n\n\nInstance Retrieval\n\\(P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}\\)\nNovel test stimulus \\(X\\) activates input nodes \\(X_i\\)\n\n\nSlope Computation\n\\(S =\\) \\(\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}\\)\nSlope value, \\(S\\) computed from nearest training instances\n\n\nResponse\n\\(E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]\\)\nFinal EXAM response is the ALM response for the nearest training stimulus, \\(m(X_i)\\), adjusted by local slope \\(S\\).",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#model-fitting",
    "href": "Sections/HTW.html#model-fitting",
    "title": "HTW Project",
    "section": "Model Fitting",
    "text": "Model Fitting\nTo fit ALM and EXAM to our participant data, we employ a similar method to McDaniel et al. (2009), wherein we examine the performance of each model after being fit to various subsets of the data. Each model was fit to the data with three separate procedures: 1) fit to maximize predictions of the testing data, 2) fit to maximize predictions of both the training and testing data, 3) fit to maximize predictions of the just the training data. We refer to this fitting manipulations as “Fit Method” in the tables and figures below. It should be emphasized that for all three fit methods, the ALM and EXAM models behave identically - with weights updating only during the training phase. Models were fit separately to the data of each individual participant. The free parameters for both models are the generalization (\\(c\\)) and learning rate (\\(lr\\)) parameters. Parameter estimation was performed using approximate Bayesian computation (ABC), which we describe in detail below.\n\n\n\n\n\n\n Approximate Bayesian Computation\nTo estimate the parameters of ALM and EXAM, we used approximate Bayesian computation (ABC), enabling us to obtain an estimate of the posterior distribution of the generalization and learning rate parameters for each individual. ABC belongs to the class of simulation-based inference methods (Cranmer et al., 2020), which have begun being used for parameter estimation in cognitive modeling relatively recently (Kangasrääsiö et al., 2019; Turner et al., 2016; Turner & Van Zandt, 2012). Although they can be applied to any model from which data can be simulated, ABC methods are most useful for complex models that lack an explicit likelihood function (e.g., many neural network models).\nThe general ABC procedure is to 1) define a prior distribution over model parameters. 2) sample candidate parameter values, \\(\\theta^*\\), from the prior. 3) Use \\(\\theta^*\\) to generate a simulated dataset, \\(Data_{sim}\\). 4) Compute a measure of discrepancy between the simulated and observed datasets, \\(discrep\\)(\\(Data_{sim}\\), \\(Data_{obs}\\)). 5) Accept \\(\\theta^*\\) if the discrepancy is less than the tolerance threshold, \\(\\epsilon\\), otherwise reject \\(\\theta^*\\). 6) Repeat until the desired number of posterior samples are obtained.\nAlthough simple in the abstract, implementations of ABC require researchers to make a number of non-trivial decisions as to i) the discrepancy function between observed and simulated data, ii) whether to compute the discrepancy between trial level data, or a summary statistic of the datasets, iii) the value of the minimum tolerance \\(\\epsilon\\) between simulated and observed data. For the present work, we follow the guidelines from previously published ABC tutorials (Farrell & Lewandowsky, 2018; Turner & Van Zandt, 2012). For the test stage, we summarized datasets with mean velocity of each band in the observed dataset as \\(V_{obs}^{(k)}\\) and in the simulated dataset as \\(V_{sim}^{(k)}\\), where \\(k\\) represents each of the six velocity bands. For computing the discrepancy between datasets in the training stage, we aggregated training trials into three equally sized blocks (separately for each velocity band in the case of the varied group). After obtaining the summary statistics of the simulated and observed datasets, the discrepancy was computed as the mean of the absolute difference between simulated and observed datasets (Equation 1 and Equation 2). For the models fit to both training and testing data, discrepancies were computed for both stages, and then averaged together.\n\n\\[\ndiscrep_{Test}(Data_{sim}, Data_{obs}) = \\frac{1}{6} \\sum_{k=1}^{6} |V_{obs}^{(k)} - V_{sim}^{(k)}|\n\\tag{1}\\]\n\\[\n\\begin{aligned} \\\\\ndiscrep_{Train,constant}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks}} \\sum_{j=1}^{N_{blocks}} |V_{obs,constant}^{(j)} - V_{sim,constant}^{(j)}| \\\\ \\\\\ndiscrep_{Train,varied}(Data_{sim}, Data_{obs}) = \\frac{1}{N_{blocks} \\times 3} \\sum_{j=1}^{N_{blocks}} \\sum_{k=1}^{3} |V_{obs,varied}^{(j,k)} - V_{sim,varied}^{(j,k)}|\n\\end{aligned}\n\\tag{2}\\]\n\nThe final component of our ABC implementation is the determination of an appropriate value of \\(\\epsilon\\). The setting of \\(\\epsilon\\) exerts strong influence on the approximated posterior distribution. Smaller values of \\(\\epsilon\\) increase the rejection rate, and improve the fidelity of the approximated posterior, while larger values result in an ABC sampler that simply reproduces the prior distribution. Because the individual participants in our dataset differed substantially in terms of the noisiness of their data, we employed an adaptive tolerance setting strategy to tailor \\(\\epsilon\\) to each individual. The initial value of \\(\\epsilon\\) was set to the overall standard deviation of each individual’s velocity values. Thus, sampled parameter values that generated simulated data within a standard deviation of the observed data were accepted, while worse performing parameters were rejected. After every 300 samples the tolerance was allowed to increase only if the current acceptance rate of the algorithm was less than 1%. In such cases, the tolerance was shifted towards the average discrepancy of the 5 best samples obtained thus far. To ensure the acceptance rate did not become overly permissive, \\(\\epsilon\\) was also allowed to decrease every time a sample was accepted into the posterior.\n\n\n\nFor each of the 156 participants from Experiment 1, the ABC algorithm was run until 200 samples of parameters were accepted into the posterior distribution. Obtaining this number of posterior samples required an average of 205,000 simulation runs per participant. Fitting each combination of participant, Model (EXAM & ALM), and fitting method (Test only, Train only, Test & Train) required a total of 192 million simulation runs. To facilitate these intensive computational demands, we used the Future Package in R (Bengtsson, 2021), allowing us to parallelize computations across a cluster of ten M1 iMacs, each with 8 cores.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/HTW.html#modelling-results",
    "href": "Sections/HTW.html#modelling-results",
    "title": "HTW Project",
    "section": "Modelling Results",
    "text": "Modelling Results\n\n\nDisplay code\npost_tabs &lt;- abc_tables(post_dat,post_dat_l)\ntrain_tab &lt;- abc_train_tables(pd_train,pd_train_l)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\n\ne1_tab &lt;- rbind(post_tabs$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Test\"), train_tab$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; mutate(Fit_Method=rename_fm(Fit_Method)) \n\nif (primary) {\ne1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = c(`Task Stage`)) %&gt;%\n  cols_label(\n    `Task Stage` = \"Task Stage\"\n  ) %&gt;%\n  fmt_number(\n    columns = starts_with(\"ALM\") | starts_with(\"EXAM\"),\n    decimals = 2\n  ) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n     locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = \"top\", color = \"black\", weight = px(1)),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\n} else {\n\n  e1_tab %&gt;%\n  group_by(`Task Stage`, Fit_Method, Model, condit) %&gt;%\n  summarize(ME = mean(mean_error), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = c(Model, condit),\n    values_from = ME,\n    names_sep = \"_\"  # Add this line to specify the separator for column names\n  ) %&gt;%\n  rename(\"Fit Method\" = Fit_Method) %&gt;% kable(escape=F,booktabs=T)\n\n  }\n\n\n\n\nTable 12: Model errors predicting empirical data from Experiment 1 - aggregated over the full posterior distribution for each participant. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n  \n    \n      Task Stage\n      Fit Method\n      \n        ALM\n      \n      \n        EXAM\n      \n    \n    \n      Constant\n      Varied\n      Constant\n      Varied\n    \n  \n  \n    Test\nFit to Test Data\n199.93\n103.36\n104.01\n85.68\n    Test\nFit to Test & Training Data\n216.97\n170.28\n127.94\n144.86\n    Test\nFit to Training Data\n467.73\n291.38\n273.30\n297.91\n    Train\nFit to Test Data\n297.82\n2,016.01\n53.90\n184.00\n    Train\nFit to Test & Training Data\n57.40\n132.32\n42.92\n127.90\n    Train\nFit to Training Data\n51.77\n103.48\n51.43\n107.03\n  \n  \n  \n\n\n\n\n\n\n\n\n\nDisplay code\nc_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=log(c), x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.2)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"c parameter\") +\n  theme(legend.title = element_blank(), legend.position=\"right\",plot.title=element_text(hjust=.4))\n\nlr_post &lt;- post_dat_avg %&gt;%\n    group_by(id, condit, Model, Fit_Method, rank) %&gt;%\n    slice_head(n = 1) |&gt;\n    ggplot(aes(y=lr, x = Fit_Method,col=condit)) + stat_pointinterval(position=position_dodge(.4)) +\n    ggh4x::facet_nested_wrap(~Model) + labs(title=\"learning rate parameter\") +\n  theme(legend.title = element_blank(), legend.position = \"none\",plot.title=element_text(hjust=.5))\nc_post + lr_post\n\n\n\n\n\n\n\n\nFigure 18: Posterior Distributions of \\(c\\) and \\(lr\\) parameters. Points represent median values, thicker intervals represent 66% credible intervals and thin intervals represent 95% credible intervals around the median. Note that the y-axes of the plots for the c parameter are scaled logarithmically.\n\n\n\n\n\n\n\nDisplay code\ntrain_resid &lt;- pd_train |&gt; group_by(id,condit,Model,Fit_Method, Block,x) |&gt; \n  summarise(y=mean(y), pred=mean(pred), mean_error=abs(y-pred)) |&gt;\n  group_by(id,condit,Model,Fit_Method,Block) |&gt;\n  summarise(mean_error=mean(mean_error)) |&gt;\n  ggplot(aes(x=interaction(Block,Model), y = mean_error, fill=factor(Block))) + \n  stat_bar + \n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, scales=\"free\",ncol=2) +\n   scale_x_discrete(guide = \"axis_nested\") +\n  scale_fill_manual(values=c(\"gray10\",\"gray50\",\"gray92\"))+\n  labs(title=\"Model Residual Errors - Training Stage\", y=\"RMSE\", x= \"Model\",fill=\"Training Block\") +\n  theme(legend.position=\"top\")\n\ntest_resid &lt;-  post_dat |&gt; \n   group_by(id,condit,x,Model,Fit_Method,bandType) |&gt;\n    summarise(y=mean(y), pred=mean(pred), error=abs(y-pred)) |&gt; \n  mutate(vbLab = factor(paste0(x,\"-\",x+200))) |&gt;\n  ggplot(aes(x = Model, y = abs(error), fill=vbLab,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n  #scale_fill_manual(values=wes_palette(\"AsteroidCity2\"))+\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n  ggh4x::facet_nested_wrap(rename_fm(Fit_Method)~condit, axes = \"all\",ncol=2,scale=\"free\") +\n  labs(title=\"Model Residual Errors - Testing Stage\",y=\"RMSE\", x=\"Velocity Band\") \n\n(train_resid / test_resid) +\n  #plot_layout(heights=c(1,1.5)) & \n  plot_annotation(tag_levels = list(c('A','B')),tag_suffix = ') ') \n\n\n\n\n\n\n\n\nFigure 19: Model residuals for each combination of training condition, fit method, and model. Residuals reflect the difference between observed and predicted values. Lower values indicate better model fit. Note that y-axes are scaled differently between facets. A) Residuals predicting each block of the training data. B) Residuals predicting each band during the testing stage. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\nThe posterior distributions of the \\(c\\) and \\(lr\\) parameters are shown Figure 18, and model predictions are shown alongside the empirical data in Figure 20. There were substantial individual differences in the posteriors of both parameters, with the within-group individual differences generally swamped any between-group or between-model differences. The magnitude of these individual differences remains even if we consider only the single best parameter set for each subject.\nWe used the posterior distribution of \\(c\\) and \\(lr\\) parameters to generate a posterior predictive distribution of the observed data for each participant, which then allows us to compare the empirical data to the full range of predictions from each model. Aggregated residuals are displayed in Figure 19. The pattern of training stage residual errors are unsurprising across the combinations of models and fitting method . Differences in training performance between ALM and EXAM are generally minor (the two models have identical learning mechanisms). The differences in the magnitude of residuals across the three fitting methods are also straightforward, with massive errors for the ‘fit to Test Only’ model, and the smallest errors for the ‘fit to train only’ models. It is also noteworthy that the residual errors are generally larger for the first block of training, which is likely due to the initial values of the ALM weights being unconstrained by whatever initial biases participants tend to bring to the task. Future work may explore the ability of the models to capture more fine grained aspects of the learning trajectories. However for the present purposes, our primary interest is in the ability of ALM and EXAM to account for the testing patterns while being constrained, or not constrained, by the training data. All subsequent analyses and discussion will thus focus on the testing stage.\nThe residuals of the model predictions for the testing stage (Figure 19) show an unsurprising pattern across fitting methods - with models fit only to the test data showing the best performance, followed by models fit to both training and test data, and with models fit only to the training data showing the worst performance (note that Y-axes are scaled different between plots). Although EXAM tends to perform better for both Constant and Varied participants (see also Figure 21), the relative advantage of EXAM is generally larger for the Constant group - a pattern consistent across all three fitting methods. The primary predictive difference between ALM and EXAM is made clear in Figure 20, which directly compares the observed data against the posterior predictive distributions for both models. Regardless of how the models are fit, only EXAM can capture the pattern where participants are able to discriminate all 6 target bands.\n\n\nDisplay code\npost_dat_l |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; \n #left_join(testAvgE1, by=join_by(id,condit,x==bandInt)) |&gt;\n ggplot(aes(x=Resp,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar + \n    facet_wrap(~rename_fm(Fit_Method)+condit, ncol=2,strip.position = \"top\", scales = \"free_x\") +\n        scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .5), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n         strip.background = element_blank(),\n         strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(10,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions - Experiment 1 Data\", y=\"Vx\")\n\n\n\n\n\n\n\n\nFigure 20: Empirical data and Model predictions for mean velocity across target bands. Fitting methods (Test Only, Test & Train, Train Only) - are separated across rows, and Training Condition (Constant vs. Varied) are separated by columns. Each facet contains the predictions of ALM and EXAM, alongside the observed data.\n\n\n\n\n\n\n\nDisplay code\n###| eval: false\n\npacman::p_load(dplyr,purrr,tidyr,ggplot2, data.table, here, patchwork, conflicted, \n               stringr,future,furrr, knitr, reactable,ggstance, htmltools,\n               ggdist,ggh4x,brms,tidybayes,emmeans,bayestestR, gt)\n\npdl &lt;- post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; left_join(testAvgE1,by=c(\"id\",\"condit\",\"bandInt\")) |&gt; \n  filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\n# aerror is model error, which is predicted by Model(ALM vs. EXAM) & condit (Constant vs. Varied)\ne1_ee_brm_ae &lt;- brm(data=pdl,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e1_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbct_e1 &lt;- as.data.frame(bayestestR::describe_posterior(e1_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\n\np1 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e1_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n#p_ce_1 &lt;- (p1 + p2+ p3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n# plot_custom_effects &lt;- function(model) {\n#   # Extract posterior samples for fixed effects\n#   post_samples &lt;- posterior_samples(model, pars = c(\"b_Intercept\", \"b_ModelEXAM\", \"b_conditVaried\", \"b_ModelEXAM:conditVaried\"))\n  \n#   # Calculate conditional effects\n#   post_samples &lt;- post_samples %&gt;%\n#     mutate(\n#       ALM_Constant = b_Intercept,\n#       EXAM_Constant = b_Intercept + b_ModelEXAM,\n#       ALM_Varied = b_Intercept + b_conditVaried,\n#       EXAM_Varied = b_Intercept + b_ModelEXAM + b_conditVaried + `b_ModelEXAM:conditVaried`\n#     )\n  \n#   # Reshape data for plotting\n#   plot_data &lt;- post_samples %&gt;%\n#     select(ALM_Constant, EXAM_Constant, ALM_Varied, EXAM_Varied) %&gt;%\n#     pivot_longer(everything(), names_to = \"Condition\", values_to = \"Estimate\") %&gt;%\n#     separate(Condition, into = c(\"Model\", \"Condit\"), sep = \"_\")\n  \n#   # Plot conditional effects\n#   ggplot(plot_data, aes(x = Model, y = Estimate, color = Condit)) +\n#     geom_boxplot() +\n#     theme_minimal() +\n#     labs(x = \"Model\", y = \"Estimate\", color = \"Condition\")\n# }\n# p_ce_1 &lt;- plot_custom_effects(e1_ee_brm_ae)\n\n\n\n\nbm1 &lt;- get_coef_details(e1_ee_brm_ae, \"conditVaried\")\nbm2 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM\")\nbm3 &lt;- get_coef_details(e1_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nposterior_estimates &lt;- as.data.frame(e1_ee_brm_ae) %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  setNames(c(\"Intercept\", \"ModelEXAM\", \"conditVaried\", \"ModelEXAM_conditVaried\"))\n\nconstant_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM\nvaried_EXAM &lt;- posterior_estimates$Intercept + posterior_estimates$ModelEXAM + posterior_estimates$conditVaried + posterior_estimates$ModelEXAM_conditVaried\ncomparison_EXAM &lt;- constant_EXAM - varied_EXAM\nsummary_EXAM &lt;- bayestestR::describe_posterior(comparison_EXAM, centrality = \"Mean\")\n\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NULL)\n# e1_ee_brm_ae |&gt; emmeans(pairwise ~ Model * condit, re_formula=NA)\n\n# full set of Model x condit contrasts\n# ALM - EXAM\n# btw_model &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model | condit, re_formula=NULL)  |&gt; \n#   pluck(\"contrasts\") |&gt; \n#   gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw,condit) |&gt; summarise(value=mean(.value), n=n()) \n\n# btw_model |&gt; ggplot(aes(x=value,y=contrast,fill=condit)) +stat_halfeye()\n\n# Constant - Varied\n# emm_condit &lt;- e1_ee_brm_ae |&gt; emmeans(~ condit | Model, re_formula = NULL)\n# btw_con &lt;- emm_condit |&gt;  pairs() |&gt; gather_emmeans_draws() |&gt; \n#   group_by(contrast,.draw, Model) |&gt; summarise(value=mean(.value), n=n()) \n# # btw_con |&gt; ggplot(aes(x=value,y=Model,fill=Model)) +stat_halfeye()                              \n\np_em_1 &lt;- e1_ee_brm_ae |&gt; emmeans(pairwise~ Model*condit, re_formula=NA)  |&gt; \n  pluck(\"contrasts\") |&gt;\n  gather_emmeans_draws() |&gt; \n  group_by(contrast,.draw) |&gt; summarise(value=mean(.value), n=n()) |&gt; \n  filter(!(contrast %in% c(\"ALM Constant - EXAM Constant\",\"ALM Constant - EXAM Varied\",\"ALM Varied - EXAM Varied \", \"EXAM Constant - ALM Varied\" ))) |&gt; \n  ggplot(aes(x=value,y=contrast,fill=contrast)) +stat_halfeye() + labs(x=\"Model Error Difference\",y=\"Contrast\") + theme(legend.position=\"none\") \n\n#p_ce_1 / p_em_1\n\n(p1 + p2+ p3) /p_em_1 + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\n\n\nFigure 21: A-C) Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied). Lower values on the y axis indicate better model fit. D) Specific contrasts of model performance comparing 1) EXAM fits between constant and varied training; 2) ALM vs. EXAM for the varied group; 3) ALM fits between constant and varied. Negative error differences indicate that the term on the left side (e.g., EXAM Constant) tended to have smaller model residuals.\n\n\n\n\n\nTo quantitatively assess the differences in performance between models, we fit a Bayesian regression model predicting the errors of the posterior predictions of each models as a function of the Model (ALM vs. EXAM) and training condition (Constant vs. Varied).\nModel errors were significantly lower for EXAM (\\(\\beta\\) = -37.54, 95% CrI [-60.4, -14.17], pd = 99.85%) than ALM. There was also a significant interaction between Model and Condition (\\(\\beta\\) = 60.42, 95% CrI [36.17, 83.85], pd = 100%), indicating that the advantage of EXAM over ALM was significantly greater for the constant group. To assess whether EXAM predicts performance significantly better for Constant than for Varied subjects, we calculated the difference in model error between the Constant and Varied conditions specifically for EXAM. The results indicated that the model error for EXAM was significantly lower in the Constant condition compared to the Varied condition, with a mean difference of -22.88 (95% CrI [-46.02, -0.97], pd = 0.98).\n\n\nDisplay code\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\n\npost_tabs2 &lt;- abc_tables(e2_model$post_dat,e2_model$post_dat_l)\ntrain_tab2 &lt;- abc_train_tables(e2_model$pd_train,e2_model$pd_train_l)\n\npdl2 &lt;- e2_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne2_tab &lt;- rbind(post_tabs2$agg_pred_full |&gt;\n mutate(\"Task Stage\"=\"Test\"), train_tab2$agg_pred_full |&gt; \n mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\npost_tabs3 &lt;- abc_tables(e3_model$post_dat,e3_model$post_dat_l)\ntrain_tab3 &lt;- abc_train_tables(e3_model$pd_train,e3_model$pd_train_l)\n\npdl3 &lt;- e3_model$post_dat_l |&gt; rename(\"bandInt\"=x) |&gt; filter(rank&lt;=1,Fit_Method==\"Test_Train\", !(Resp==\"Observed\")) |&gt; mutate(aerror = abs(error))\n\ne3_tab &lt;- rbind(post_tabs3$agg_pred_full |&gt; \n  mutate(\"Task Stage\"=\"Test\"), train_tab3$agg_pred_full |&gt; mutate(\"Task Stage\"=\"Train\")) |&gt; \n  mutate(Fit_Method=rename_fm(Fit_Method)) \n\ne23_tab &lt;- rbind(e2_tab |&gt; mutate(Exp=\"E2\"), e3_tab |&gt; mutate(Exp=\"E3\")) \n\nif (primary) {\ngt_table &lt;- e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  gt() %&gt;%\n  cols_move_to_start(columns = `Task Stage`) %&gt;%\n  cols_label(`Task Stage` = \"Task Stage\") %&gt;%\n  fmt_number(columns = matches(\"E2|E3\"), decimals = 1) %&gt;%\n  tab_spanner_delim(delim = \"_\") %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(columns = everything(), rows = everything())\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n  )\ngt_table\n} else {\n  e23_tab %&gt;%\n  pivot_wider(\n    names_from = c(Exp, Model, condit),\n    values_from = mean_error,\n    names_glue = \"{Exp}_{Model}_{condit}\"\n  ) %&gt;%\n  arrange(Fit_Method, `Task Stage`) %&gt;%\n  kable(escape=F,booktabs=T)\n}\n\n\n\n\nTable 13: Models errors predicting empirical data - aggregated over all participants, posterior parameter values, and velocity bands. Note that Fit Method refers to the subset of the data that the model was trained on, while Task Stage refers to the subset of the data that the model was evaluated on.\n\n\n\n\n\n\n  \n    \n      \n      \n        E2\n      \n      \n        E3\n      \n    \n    \n      Task Stage\n      \n        ALM\n      \n      \n        EXAM\n      \n      \n        ALM\n      \n      \n        EXAM\n      \n    \n    \n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n      Constant\n      Varied\n    \n  \n  \n    \n      Fit to Test Data\n    \n    Test\n239.7\n129.8\n99.7\n88.2\n170.1\n106.1\n92.3\n72.8\n    Train\n53.1\n527.1\n108.1\n169.3\n70.9\n543.5\n157.8\n212.7\n    \n      Fit to Test & Training Data\n    \n    Test\n266.0\n208.2\n125.1\n126.4\n197.7\n189.5\n130.0\n128.5\n    Train\n40.0\n35.4\n30.4\n23.6\n49.1\n85.6\n49.2\n78.4\n    \n      Fit to Training Data\n    \n    Test\n357.4\n295.9\n305.1\n234.5\n415.0\n298.8\n295.5\n243.7\n    Train\n42.5\n23.0\n43.2\n22.6\n51.4\n63.8\n51.8\n65.3\n  \n  \n  \n\n\n\n\n\n\n\n   \n\n\nDisplay code\nrbind(e2_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb) |&gt; \n summarize(vx=median(val)) |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\"), \n e3_model$post_dat_l |&gt; filter( Fit_Method==\"Test_Train\") |&gt; \n  group_by(id,condit, Fit_Method,Resp,bandType,x,vb,bandOrder) |&gt;\n  summarize(vx=median(val)) |&gt; mutate(Exp=\"E3\")) |&gt;\n  ggplot( aes(x=condit,y=vx, fill=vb,col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) +\n  stat_bar + \n    facet_nested_wrap(~Exp+bandOrder+Resp, strip.position = \"top\", scales = \"free_x\") +\n    scale_color_manual(values = c(\"black\" = \"black\"), guide = \"none\") +\n  scale_size_manual(values = c(\"black\" = .7), guide = \"none\") +\n    theme(panel.spacing = unit(0, \"lines\"), \n        #  strip.background = element_blank(),\n        #  strip.placement = \"outside\",\n         legend.position = \"none\",plot.title = element_text(hjust=.50),\n         axis.title.x = element_blank(),\n         plot.margin = unit(c(20,0,0,0), \"pt\")) + \n         labs(title=\"Model Predictions Experiment 2 & 3\", y=\"vx\")\n\n\n\n\n\n\n\n\nFigure 22: Empirical data and Model predictions from Experiment 2 and 3 for the testing stage. Observed data is shown on the right. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.\n\n\n\n\n\n\n\n\nDisplay code\ne2_ee_brm_ae &lt;- brm(data=pdl2,\n  aerror ~  Model * condit + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e2_ae_modelCond_RFint.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"conditVaried\")\nbm2_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM\")\nbm3_e2 &lt;- get_coef_details(e2_ee_brm_ae, \"ModelEXAM:conditVaried\")\n\nbct_e2 &lt;- as.data.frame(bayestestR::describe_posterior(e2_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) # %&gt;% kable(booktabs = TRUE)\n\ne3_ee_brm_ae &lt;- brm(data=pdl3,\n  aerror ~  Model * condit*bandOrder + (1+bandInt|id), \n  file = paste0(here(\"data/model_cache/e3_ae_modelCondBo_RFint2.rds\")),\n  chains=4,silent=1, iter=2000, control=list(adapt_delta=0.92, max_treedepth=11))\n\nbm1_e3 &lt;- get_coef_details(e3_ee_brm_ae, \"conditVaried\")\nbm2_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM\")\nbm3_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried\")\nbm4_e3  &lt;- get_coef_details(e3_ee_brm_ae, \"ModelEXAM:conditVaried:bandOrderReverse\")\n\n\nbct_e3  &lt;- as.data.frame(bayestestR::describe_posterior(e3_ee_brm_ae, centrality = \"Mean\")) %&gt;%\n  select(1,2,4,5,6) %&gt;%\n  setNames(c(\"Term\", \"Estimate\",\"95% CrI Lower\", \"95% CrI Upper\", \"pd\")) %&gt;%\n  mutate(across(where(is.numeric), \\(x) round(x, 2))) %&gt;%\n  tibble::remove_rownames() %&gt;%\n  mutate(Term = stringr::str_remove(Term, \"b_\")) #%&gt;% kable(booktabs = TRUE)\n\nbct &lt;- rbind(bct_e1 |&gt; mutate(exp=\"Exp 1\"),bct_e2 |&gt; \n               mutate(exp= \"Exp 2\"),bct_e3 |&gt; mutate(exp=\"Exp 3\")) |&gt; \n  relocate(exp, .before=Term)\n\n\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\nprimary=out_type == \"html\" || out_type == \"pdf\" || out_type ==\"latex\" || out_type == \"docx\"\n\nif (primary) {\nbct_table &lt;- bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;%\n  gt() %&gt;%\n  # tab_header(\n  #   title = \"Bayesian Model Results\",\n  #   subtitle = \"Estimates and Credible Intervals for Each Term Across Experiments\"\n  # ) %&gt;%\n  cols_label(\n    exp = \"Experiment\",\n    Term = \"Term\",\n    Estimate = \"Estimate\",\n    `95% CrI Lower` = \"95% CrI Lower\",\n    `95% CrI Upper` = \"95% CrI Upper\",\n    pd = \"pd\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Estimate, `95% CrI Lower`, `95% CrI Upper`),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = pd,\n    decimals = 2\n  ) %&gt;%\n  tab_spanner(\n    label = \"Credible Interval\",\n    columns = c(`95% CrI Lower`, `95% CrI Upper`)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      #cell_fill(color = \"lightgray\"),\n      cell_text(weight = \"bold\"), \n      cell_fill(color = \"white\"),\n      cell_borders(sides = \"top\", color = \"black\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = c(Estimate, pd),\n      rows = Term==\"ModelEXAM:conditVaried\"\n    )\n  ) %&gt;%\n   tab_row_group(\n    label = \"Experiment 3\",\n    rows = exp == \"Exp 3\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 2\",\n    rows = exp == \"Exp 2\"\n  ) %&gt;%\n  tab_row_group(\n    label = \"Experiment 1\",\n    rows = exp == \"Exp 1\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    quarto.disable_processing = TRUE,\n    column_labels.padding = 2,\n    data_row.padding = 2\n    #row_group.background.color = \"gray95\"\n  )\nbct_table\n} else {\n  bct %&gt;%\n  mutate(\n    across(c(Estimate, `95% CrI Lower`, `95% CrI Upper`), ~ round(., 2)),\n    pd = round(pd, 2)\n  ) %&gt;% kable(booktabs = TRUE)\n\n}\n\n\n\n\nTable 14: Results of Bayesian Regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and the interaction between Model and Condition. The values represent the estimated coefficient for each term, with 95% credible intervals in brackets. The intercept reflects the baseline of ALM and Constant. The other estimates indicate deviations from the baseline for the EXAM mode and varied condition. Lower values indicate better model fit.\n\n\n\n\n\n\n  \n    \n      Experiment\n      Term\n      Estimate\n      \n        Credible Interval\n      \n      pd\n    \n    \n      95% CrI Lower\n      95% CrI Upper\n    \n  \n  \n    \n      Experiment 1\n    \n    Exp 1\nIntercept\n176.3\n156.9\n194.6\n1.00\n    Exp 1\nModelEXAM\n−88.4\n−104.5\n−71.8\n1.00\n    Exp 1\nconditVaried\n−37.5\n−60.4\n−14.2\n1.00\n    Exp 1\nModelEXAM:conditVaried\n60.4\n36.2\n83.8\n1.00\n    \n      Experiment 2\n    \n    Exp 2\nIntercept\n245.9\n226.2\n264.5\n1.00\n    Exp 2\nModelEXAM\n−137.7\n−160.2\n−115.5\n1.00\n    Exp 2\nconditVaried\n−86.4\n−113.5\n−59.3\n1.00\n    Exp 2\nModelEXAM:conditVaried\n56.9\n25.3\n88.0\n1.00\n    \n      Experiment 3\n    \n    Exp 3\nIntercept\n164.8\n140.1\n189.4\n1.00\n    Exp 3\nModelEXAM\n−65.7\n−86.0\n−46.0\n1.00\n    Exp 3\nconditVaried\n−40.6\n−75.9\n−3.0\n0.98\n    Exp 3\nbandOrderReverse\n25.5\n−9.3\n58.7\n0.93\n    Exp 3\nModelEXAM:conditVaried\n41.9\n11.2\n72.5\n0.99\n    Exp 3\nModelEXAM:bandOrderReverse\n−7.3\n−34.5\n21.1\n0.70\n    Exp 3\nconditVaried:bandOrderReverse\n30.8\n−19.6\n83.6\n0.88\n    Exp 3\nModelEXAM:conditVaried:bandOrderReverse\n−60.6\n−101.8\n−18.7\n1.00\n  \n  \n  \n\n\n\n\n\n\n\nModel Fits to Experiment 2 and 3. Data from Experiments 2 and 3 were fit to ALM and EXAM in the same manner as Experiment 1. For brevity, we only plot and discuss the results of the “fit to training and testing data” models - results from the other fitting methods can be found in the appendix. The model fitting results for Experiments 2 and 3 closely mirrored those observed in Experiment 1. The Bayesian regression models predicting model error as a function of Model (ALM vs. EXAM), Condition (Constant vs. Varied), and their interaction (see Table 14) revealed a consistent main effect of Model across all three experiments. The negative coefficients for the ModelEXAM term (Exp 2: \\(\\beta\\) = -86.39, 95% CrI -113.52, -59.31, pd = 100%; Exp 3: \\(\\beta\\) = -40.61, 95% CrI -75.9, -3.02, pd = 98.17%) indicate that EXAM outperformed ALM in both experiments. Furthermore, the interaction between Model and Condition was significant in both Experiment 2 (\\(\\beta\\) = 56.87, 95% CrI 25.26, 88.04, pd = 99.98%) and Experiment 3 (\\(\\beta\\) = 41.9, 95% CrI 11.2, 72.54, pd = 99.35%), suggesting that the superiority of EXAM over ALM was more pronounced for the Constant group compared to the Varied group, as was the case in Experiment 1. Recall that Experiment 3 included participants in both the original and reverse order conditions - and that this manipulation interacted with the effect of training condition. We thus also controlled for band order in our Bayesian Regression assessing the relative performance of EXAM and ALM in Experiment 3. There was a significant three way interaction between Model, Training Condition, and Band Order (\\(\\beta\\) = -60.6, 95% CrI -101.8, -18.66, pd = 99.83%), indicating that the relative advantage of EXAM over ALM was only more pronounced in the original order condition, and not the reverse order condition (see Figure 23).\n\n\nDisplay code\n#wrap_plots(plot(conditional_effects(e1_ee_brm_ae),points=FALSE,plot=FALSE))\np1 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\") + labs(title=\"E2. Model Error\")\np2 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e2_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n  p_e2 &lt;- (p1 + p2+ p3) \n# #wrap_plots(plot(conditional_effects(e3_ee_brm_ae),points=FALSE,plot=FALSE))\n\np_e3 &lt;- plot(conditional_effects(e3_ee_brm_ae, \n                         effects = \"Model:condit\", \n                         conditions=make_conditions(e3_ee_brm_ae,vars=c(\"bandOrder\"))),\n     points=FALSE,plot=FALSE)$`Model:condit` + \n     labs(x=\"Model\",y=\"Model Error\", title=\"E3. Model Error\", fill=NULL, col=NULL) + \n     theme(legend.position=\"right\") + \n     scale_color_manual(values=wes_palette(\"Darjeeling1\")) \n\np1 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"condit\"),points=FALSE, plot=FALSE)$condit + \n  ggplot2::xlab(\"Condition\") +ylab(\"Model Error\")\np2 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model\"),points=FALSE, plot=FALSE)$Model + \n  labs(x=\"Model\",y=NULL)\np3 &lt;- plot(conditional_effects(e3_ee_brm_ae, effects=\"Model:condit\"),points=FALSE, plot=FALSE)$`Model:condit` + \n  scale_color_manual(values=wes_palette(\"Darjeeling1\")) +\n  labs(x=\"Model\",y=NULL,fill=NULL,col=NULL) + theme(legend.position=\"right\") \n  \n p2 &lt;- (p1 + p2+ p3)\n (p_e2 / p_e3) + plot_annotation(tag_levels = c('A'), tag_suffix=\".\")\n\n\n\n\n\n\n\n\nFigure 23: Conditional effects of Model (ALM vs EXAM) and Condition (Constant vs. Varied) on Model Error for Experiments 2 and 3 data. Experiment 3 also includes a condition for the order of training vs. testing bands (original order vs. reverse order).\n\n\n\n\n\nComputational Model Summary.\nAcross all three experiments, the model fits consistently favored the Extrapolation-Association Model (EXAM) over the Associative Learning Model (ALM). This preference for EXAM was particularly pronounced for participants in the constant training conditions (note the positive coefficients on ModelEXAM:conditVaried interaction terms Table 14). This pattern is clearly illustrated in Figure 24, which plots the difference in model errors between ALM and EXAM for each individual participant. Both varied and constant conditions have a greater proportion of subjects better fit by EXAM (positive error differences), with the magnitude of EXAM’s advantage visibly larger for the constant group.\nThe superior performance of EXAM, especially for the constant training groups, may initially seem counterintuitive. One might assume that exposure to multiple, varied examples would be necessary to extract an abstract rule. However, EXAM is not a conventional rule-based model; it does not require the explicit abstraction of a rule. Instead, rule-based responses emerge during the retrieval process. The constant groups’ formation of a single, accurate input-output association, combined with the usefulness of the zero point, seem to have been sufficient for EXAM to capture their performance. A potential concern is that the assumption of participants utilizing the zero point essentially transforms the extrapolation problem into an interpolation problem. However, this concern is mitigated by the consistency of the results across both the original and reversed order conditions (the testing extrapolation bands fall in between the constant training band and the 0 point in experiment 1, but not in experiment 2).\nThe fits to the individual participants also reveal a number of interesting cases where the models struggle to capture the data (Figure 25). For example participant 68 exhibits a strong non-monotonicity in the highest velocity band, a pattern which ALM can mimic, but which EXAM cannot capture, given that it enforces a simple linear relationship between target velocity and response. Participant 70 (lower right corner of Figure 25) had a roughly parabolic response pattern in their observed data, a pattern which neither model can properly reproduce, but which causes EXAM to perform particularly poorly.\nModeling Limitations. The present work compared models based on their ability to predict the observed data, without employing conventional model fit indices such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). These indices, which penalize models based on their number of free parameters, would have been of limited utility in the current case, as both ALM and EXAM have two free parameters. However, despite having the same number of free parameters, EXAM could still be considered the more complex model, as it incorporates all the components of ALM plus an additional mechanism for rule-based responding. A more comprehensive model comparison approach might involve performing cross-validation with a held-out subset of the data (Mezzadri et al., 2022) or penalizing models based on the range of patterns they can produce (Dome & Wills, 2023), under the assumption that more constrained models are more impressive when they do adequately fit a given pattern of results.\n\n\n\nDisplay code\ntid1 &lt;- post_dat  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-pred_dist, -dist) |&gt;\n  rbind(e2_model$post_dat |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat |&gt; mutate(Exp=\"E3\")) |&gt;\n  filter(Fit_Method==\"Test_Train\") |&gt;\n  group_by(id,condit,Model,Fit_Method,x, Exp) |&gt; \n    mutate(e2=abs(y-pred)) |&gt; \n    summarise(y1=median(y), pred1=median(pred),mean_error=abs(y1-pred1)) |&gt;\n    group_by(id,condit,Model,Fit_Method,Exp) |&gt; \n    summarise(mean_error=mean(mean_error)) |&gt; \n    arrange(id,condit,Fit_Method) |&gt;\n    round_tibble(1) \n\nbest_id &lt;- tid1 |&gt; \n  group_by(id,condit,Fit_Method) |&gt; \n  mutate(best=ifelse(mean_error==min(mean_error),1,0)) \n\nlowest_error_model &lt;- best_id %&gt;%\n  group_by(id, condit,Fit_Method, Exp) %&gt;%\n  summarise(Best_Model = Model[which.min(mean_error)],\n            Lowest_error = min(mean_error),\n            differential = min(mean_error) - max(mean_error)) %&gt;%\n  ungroup()\n\nerror_difference&lt;- best_id %&gt;%\n  select(id, condit, Model,Fit_Method, mean_error) %&gt;%\n  pivot_wider(names_from = Model, values_from = c(mean_error)) %&gt;%\n  mutate(Error_difference = (ALM - EXAM))\n\nfull_comparison &lt;- lowest_error_model |&gt; \n  left_join(error_difference, by=c(\"id\",\"condit\",\"Fit_Method\"))  |&gt; \n  group_by(condit,Fit_Method,Best_Model) |&gt; \n  mutate(nGrp=n(), model_rank = nGrp - rank(Error_difference) ) |&gt; \n  arrange(Fit_Method,-Error_difference)\n\nfull_comparison |&gt; \n  filter(Fit_Method==\"Test_Train\") |&gt; \n  ungroup() |&gt;\n  mutate(id = reorder(id, Error_difference)) %&gt;%\n  ggplot(aes(y=id,x=Error_difference,fill=Best_Model))+\n  geom_col() +\n  #ggh4x::facet_grid2(~condit,axes=\"all\",scales=\"free_y\", independent = \"y\")+\n  ggh4x::facet_nested_wrap(~condit+Exp,scales=\"free\") + \n  theme(axis.text.y = element_text(size=8)) +\n  labs(fill=\"Best Model\",\n  x=\"Mean Model Error Difference (ALM - EXAM)\",\n  y=\"Participant\")\n\n\n\n\n\n\n\n\nFigure 24: Difference in model errors for each participant, with models fit to both train and test data. Positive values favor EXAM, while negative values favor ALM.\n\n\n\n\n\n\n\nDisplay code\ncId_tr &lt;- c(137, 181, 11)\nvId_tr &lt;- c(14, 193, 47)\ncId_tt &lt;- c(11, 93, 35)\nvId_tt &lt;- c(1,14,74)\ncId_new &lt;- c(175, 68, 93, 74)\n# filter(id %in% (filter(bestTestEXAM,group_rank&lt;=9, Fit_Method==\"Test\")\n\ne1_sbjs &lt;- c(49,68,155, 175,74)\ne3_sbjs &lt;-  c(245, 280, 249)\ne2_sbjs &lt;- c(197, 157, 312, 334)\ncFinal &lt;- c(49, 128,202 )\nvFinal &lt;- c(68,70,245)\n\n\nindv_post_l &lt;- post_dat_l  |&gt; mutate(Exp=\"E1\",bandOrder=\"Original\") |&gt; select(-signed_dist) |&gt;\n  rbind(e2_model$post_dat_l |&gt; mutate(Exp=\"E2\",bandOrder=\"Reverse\")) |&gt;\n  rbind(e3_model$post_dat_l |&gt; mutate(Exp=\"E3\") |&gt; select(-fb)) |&gt;\n  filter(Fit_Method==\"Test_Train\", id %in% c(cFinal,vFinal))\n\ntestIndv &lt;- indv_post_l |&gt; \n#filter(id %in% c(cId_tt,vId_tt,cId_new), Fit_Method==\"Test_Train\") |&gt; \n   mutate(x=as.factor(x), Resp=as.factor(Resp)) |&gt;\n  group_by(id,condit,Fit_Method,Model,Resp) |&gt;\n   mutate(flab=paste0(\"Subject: \",id)) |&gt;\n  ggplot(aes(x = Resp, y = val, fill=vb, col=ifelse(bandType==\"Trained\",\"black\",NA),size=ifelse(bandType==\"Trained\",\"black\",NA))) + \n  stat_bar_sd + \n  ggh4x::facet_nested_wrap(condit~flab, axes = \"all\",ncol=3) +\n  scale_color_manual(values = c(\"black\" = \"black\"), guide = FALSE) +\n  scale_size_manual(values = c(\"black\" = .5), guide = FALSE) + \n  labs(title=\"Individual Participant fits from Test & Train Fitting Method\",\n       y=\"X Velocity\",fill=\"Target Velocity\") +\n   guides(fill = guide_legend(nrow = 1)) + \n  theme(legend.position = \"bottom\",axis.title.x = element_blank())\n\ntestIndv \n\n\n\n\n\n\n\n\nFigure 25: Model predictions alongside observed data for a subset of individual participants. A) 3 constant and 3 varied participants fit to both the test and training data. B) 3 constant and 3 varied subjects fit to only the trainign data. Bolded bars indicate bands that were trained, non-bold bars indicate extrapolation bands.",
    "crumbs": [
      "HTW Project"
    ]
  },
  {
    "objectID": "Sections/test_apa.html",
    "href": "Sections/test_apa.html",
    "title": "My document",
    "section": "",
    "text": "The influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam & Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.\n(van Dam & Ernst, 2015)"
  },
  {
    "objectID": "Sections/test_apa.html#variability-and-function-learning",
    "href": "Sections/test_apa.html#variability-and-function-learning",
    "title": "My Paper’s Title: A Full Analysis of Everything",
    "section": "Variability and Function Learning",
    "text": "Variability and Function Learning\nThe influence of variability on function learning tasks has received relatively little attention. The study by DeLosh et al. (1997) (described in detail above) did include a variability manipulation (referred to as density in their paper), wherein participants were trained with either 8, 20, or 50 unique input-output pairs, with the total number of training trials held constant. They found a minimal influence of variability on training performance, and no difference between groups in interpolation or extrapolation, with all three variability conditions displaying accurate interpolation, and linearly biased extrapolation that was well accounted for by the EXAM model.\nIn the domain of visuomotor learning, van Dam and Ernst (2015) employed a task which required participants to learn a linear function between the spikiness of shape stimuli and the correct horizontal position to make a rapid pointing response. The shapes ranged from very spiky to completely circular at the extreme ends of the space. Participants trained with intermediate shapes having lower variation (2 shapes) or higher variation (5 shapes) condition, with the 2 items of the lower variation condition matching the items used on the extreme ends of the higher variation training space. Learning was significantly slower in the higher variation group. However, the two conditions did not differ when tested with novel shapes, with both groups producing extrapolation responses of comparable magnitude to the most similar training item, rather than in accordance with the true linear function. The authors accounted for both learning and extrapolation performance with a Bayesian learning model. Similar to ALM, the model assumes that generalization occurs as a Gaussian function of the distance between stimuli. However, unlike ALM, the Bayesian learning model utilizes more elaborate probabilistic stimulus representations, with a separate Kalman Filter for each shape stimulus.\n(van Dam & Ernst, 2015)"
  },
  {
    "objectID": "revisions.html",
    "href": "revisions.html",
    "title": "Revisions",
    "section": "",
    "text": "Plausiblity of fitting ALM & EXAM to project 1 - parameter comparison across projects.  \nRationale for ordinal feedback in experiment 3. Literature suggesting ordinal is different from continuous. Discussion of how ordinal feedback could be implemented in the model\n\nProject 2 take aways - implications of empirical results, and of modeling\n\nMore foreshadowing of function learning literature. Show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization.\n\nHow c is learned - how how does this show up in training data? Process by which c is updated? Can it account for massed vs. distributed presentation of stimulii?\n\nSimilarity spaces of stimuli vs. actions - important distinctions?"
  },
  {
    "objectID": "revisions.html#primary-issues",
    "href": "revisions.html#primary-issues",
    "title": "Revisions",
    "section": "",
    "text": "Plausiblity of fitting ALM & EXAM to project 1 - parameter comparison across projects.  \nRationale for ordinal feedback in experiment 3. Literature suggesting ordinal is different from continuous. Discussion of how ordinal feedback could be implemented in the model\n\nProject 2 take aways - implications of empirical results, and of modeling\n\nMore foreshadowing of function learning literature. Show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization.\n\nHow c is learned - how how does this show up in training data? Process by which c is updated? Can it account for massed vs. distributed presentation of stimulii?\n\nSimilarity spaces of stimuli vs. actions - important distinctions?"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Dissertation",
    "section": "",
    "text": "Revisions"
  },
  {
    "objectID": "revisions.html#distinction-between-experienced-stimulii-and-behavioral-action",
    "href": "revisions.html#distinction-between-experienced-stimulii-and-behavioral-action",
    "title": "Revisions",
    "section": "Distinction between experienced stimulii and behavioral action",
    "text": "Distinction between experienced stimulii and behavioral action\n\n\nrelevant content 1\n\n\nOur modelling approach does differ from category learning implementations of instance-based models in several ways. One such difference is the nature of the training instances that are assumed to be stored. In category learning studies, instances are represented as points in a multidimensional space of all of the attributes that define a category item (e.g., size/color/shape). Rather than defining instances in terms of what stimuli learners experience, our approach assumes that stored, motor instances reflect how they act, in terms of the velocity applied to the ball on each throw. An advantage of many motor learning tasks is the relative ease with which task execution variables can be directly measured (e.g., movement force, velocity, angle, posture) in addition to the decision and response time measures that typically exhaust the data generated from more classical cognitive tasks. Of course, whether learners actually are storing each individual motor instance is a fundamental question beyond the scope of the current work – though as described in the introduction there is some evidence in support of this idea (Chamberlin & Magill, 1992a; Crump & Logan, 2010; Hommel, 1998; Meigh et al., 2018; Poldrack et al., 1999). A particularly noteworthy instance-based model of sensory-motor behavior is the Knowledge II model of Rosenbaum and colleagues (R. G. Cohen & Rosenbaum, 2004; Rosenbaum et al., 1995). Knowledge II explicitly defines instances as postures (joint combinations), and is thus far more detailed than IGAS in regards to the contents of stored instances. Knowledge II also differs from IGAS in that learning is accounted for by both the retrieval of stored postures, and the generation of novel postures via the modification of retrieved postures. A promising avenue for future research would be to combine the adaptive similarity mechanism of IGAS with the novel instance generation mechanisms of Knowledge II.\n\n\n\n\nrelevant content 2\n\n\n\nIt is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g., the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.\n\n\n\nRelevant Papers\n\nWifall et al. (2014)\nWifall et al. (2017)"
  },
  {
    "objectID": "revisions.html#learning-c",
    "href": "revisions.html#learning-c",
    "title": "Revisions",
    "section": "Learning c",
    "text": "Learning c\nHow how does this show up in training data? Process by which c is updated? Can it account for massed vs. distributed presentation of stimulii?\n\n\n\nrelevant content\n\n\nHowever, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum & Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the c parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky & Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions."
  },
  {
    "objectID": "revisions.html#ordinal-feedback-rationale",
    "href": "revisions.html#ordinal-feedback-rationale",
    "title": "Revisions",
    "section": "Ordinal feedback rationale",
    "text": "Ordinal feedback rationale\n\n\nAddition\n\n\nIn Experiment 3, we sought to further explore the generality of the findings from the first two experiments by modifying the type of feedback provided during training. Specifically, we provided ordinal feedback instead of the continuous feedback used in the previous two experiments. Ordinal feedback provides learners with directional information about the results of their throw (e.g., above the target, below the target, or hitting the target) rather than precise numerical deviations. This form of feedback resembles many real-world learning scenarios, such as a coach instructing an athlete to perform a movement using “more force” or “less force”, or a teacher providing letter grades rather than numeric scores. Although ordinal feedback provides less detailed information per trial, prior research has shown that less detailed feedback isn’t necessarily detrimental to learning. For example, Cornwall et al., manipulated whether participants received categorical (correct or incorrect) vs. numerical feedback (reward points ranging from 50-100).\n\n\n\nRelevant Papers\n\nCornwall et al. (2022)"
  },
  {
    "objectID": "revisions.html#full-revision-instructions",
    "href": "revisions.html#full-revision-instructions",
    "title": "Revisions",
    "section": "Full Revision Instructions",
    "text": "Full Revision Instructions\n\n\nshow\n\n\nAdd participant/trial exclusion conditions for Project 2\nCompare parameters values across projects or explain why this isn’t possible.  In the General Discussion to both projects, describe the prospects for applying ALM+EXAM to Project 1 (you don’t have to actually fit Project 1 with ALM+EXAM).  If ALM+EXAM can likely fit both sets of results, with their opposite generalization patterns vis-a-vis constant vs variable training, then does ALM+EXAM provide a compelling explanation for constant &gt; variable extrapolation in Project 2 or is it too flexible a model?\nJustify why the ordinal feedback in Project 2 Experiment 3 is an interesting manipulation.  Is there a literature suggesting that there would be an important difference between continuous vs ordinal feedback?  How would you incorporate ordinal feedback into ALM+EXAM (you don’t have to actually implement this model)?\nIn the discussion to Project 2, explain what are the most important implications of the empirical results for theories of human learning and generalization.  Also explain what are the most important implications of the ALM and EXAM modeling are for our understanding of human learning and generalization.  As it currently stands, there is not much of a take-home message from Project 2.\nSomewhere in the introduction to both of the projects, foreshadow the function learning literature that will be relevant to Project 2, and show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization.\nAre there new predictions to make about how the process of c being learned during training would show up in the performance during training, or anywhere else?  For instance, do you propose that c is adjusted on the basis of experienced differences from one trial to the next, or on the basis of running estimates of SD in the data, etc.? Could differences in possible learning processes for c account for effects of massed vs. distributed presentation of stimuli? \nThe distinction between experienced stimuli and behavioral actions  seems like an important factor to explore more in a discussion, given that the similarity space in the two could differ in ways that could affect behavior (and appropriate modeling)."
  }
]