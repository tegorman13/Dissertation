<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Revisions – Dissertation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="Assets/Style/calloutTG.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Dissertation
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Dissertation</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tegorman13/Dissertation" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./paper.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dissertation Manuscript</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Sections/Appendix/Full_Appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Presentation/slides.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Slides</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Sections/full.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Full Dissertation w/Code</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Sections/IGAS.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IGAS Project</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Sections/HTW.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HTW Project</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#primary-issues" id="toc-primary-issues" class="nav-link active" data-scroll-target="#primary-issues">Primary Issues</a></li>
  <li><a href="#participanttrial-exclusion-conditions-for-project-2" id="toc-participanttrial-exclusion-conditions-for-project-2" class="nav-link" data-scroll-target="#participanttrial-exclusion-conditions-for-project-2">1) Participant/trial exclusion conditions for Project 2</a></li>
  <li><a href="#igas-vs.-almexam-model-comparisondiscussion" id="toc-igas-vs.-almexam-model-comparisondiscussion" class="nav-link" data-scroll-target="#igas-vs.-almexam-model-comparisondiscussion">2) IGAS vs.&nbsp;ALM/EXAM Model Comparison/Discussion</a></li>
  <li><a href="#ordinal-feedback-rationale" id="toc-ordinal-feedback-rationale" class="nav-link" data-scroll-target="#ordinal-feedback-rationale">3) Ordinal feedback rationale</a></li>
  <li><a href="#project-2-take-aways" id="toc-project-2-take-aways" class="nav-link" data-scroll-target="#project-2-take-aways">4) Project 2 take-aways:</a></li>
  <li><a href="#stimuli-vs.-action-similarity" id="toc-stimuli-vs.-action-similarity" class="nav-link" data-scroll-target="#stimuli-vs.-action-similarity">5) Stimuli vs.&nbsp;Action Similarity</a></li>
  <li><a href="#how-c-might-be-learned" id="toc-how-c-might-be-learned" class="nav-link" data-scroll-target="#how-c-might-be-learned">6) How c might be learned</a></li>
  <li><a href="#foreshadowing-function-learning-literature" id="toc-foreshadowing-function-learning-literature" class="nav-link" data-scroll-target="#foreshadowing-function-learning-literature">7) Foreshadowing function learning literature</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Revisions</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<style>
/* Style for Addition callouts */
.addition-callout summary {
    background-color: #f0f8ff;
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 5px;
}

.addition-callout div {
    background-color: #f9f9f9;
    padding: 15px;
    border: 1px solid #ddd;
    border-radius: 5px;
}

/* Style for Relevant Content callouts */
.relevant-callout summary {
    background-color: #f5f5f5;
    padding: 10px;
    border: 1px solid #bbb;
    border-radius: 5px;
}

.relevant-callout div {
    background-color: #ffffff;
    padding: 15px;
    border: 1px solid #ccc;
    border-radius: 5px;
}
</style>
<section id="primary-issues" class="level2">
<h2 class="anchored" data-anchor-id="primary-issues">Primary Issues</h2>
<ol type="1">
<li><strong>Participant/trial exclusion conditions for Project 2</strong><br>
</li>
<li><strong>Plausiblity of fitting ALM &amp; EXAM to project 1</strong> &nbsp;</li>
<li><strong>Rationale for ordinal feedback</strong></li>
<li><strong>Project 2 take aways</strong></li>
<li><strong>More foreshadowing of function learning literature</strong>.</li>
<li><strong>How c is learned/updated</strong></li>
<li><strong>Similarity spaces of stimuli vs.&nbsp;actions</strong></li>
</ol>
<details>
<summary>
show full revision instructions from Rob
</summary>
<ol type="1">
<li><p>Add participant/trial exclusion conditions for Project 2</p></li>
<li><p>Compare parameters values across projects or explain why this isn’t possible.&nbsp; In the General Discussion to both projects, describe the prospects for applying ALM+EXAM to Project 1 (you don’t have to actually fit Project 1 with ALM+EXAM).&nbsp; If ALM+EXAM can likely fit both sets of results, with their opposite generalization patterns vis-a-vis constant vs variable training, then does ALM+EXAM provide a compelling explanation for constant &gt; variable extrapolation in Project 2 or is it too flexible a model?</p></li>
<li><p>Justify why the ordinal feedback in Project 2 Experiment 3&nbsp;is an interesting manipulation.&nbsp; Is there a literature suggesting that there would be an important difference between continuous vs ordinal feedback?&nbsp; How would you incorporate ordinal feedback into ALM+EXAM (you don’t have to actually implement this model)?</p></li>
<li><p>In the discussion to Project 2, explain what are the most important implications of the empirical results for theories of human learning and generalization.&nbsp; Also explain what are the most important implications of the ALM and EXAM modeling are for our understanding of human learning and generalization.&nbsp; As it currently stands, there is not much of a take-home message from Project 2.</p></li>
<li><p>Somewhere in the introduction to both of the projects, foreshadow the function learning literature that will be relevant to Project 2, and show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization.</p></li>
<li><p>Are there new predictions to make about how the process of c being learned during training would show up in the performance during training, or anywhere else?&nbsp; For instance, do you propose that c is adjusted on the basis of experienced differences from one trial to the next, or on the basis of running estimates of SD in the data, etc.? Could differences in possible learning processes for c account for effects of massed vs.&nbsp;distributed presentation of stimuli?&nbsp;</p></li>
<li><p>The distinction between experienced stimuli and behavioral actions &nbsp;seems like an important factor to explore more in a discussion, given that the similarity space in the two could differ in ways that could affect behavior (and appropriate modeling).</p></li>
</ol>
</details>
<hr>
<p><strong>Note.</strong> For each issue - callouts distinguish between novel text that has been added (Additions), and relevant existing content that is also relevant to the issue. Each item also includes a link to where the change was implemented in the full manuscript.</p>
<hr>
</section>
<section id="participanttrial-exclusion-conditions-for-project-2" class="level2">
<h2 class="anchored" data-anchor-id="participanttrial-exclusion-conditions-for-project-2">1) Participant/trial exclusion conditions for Project 2</h2>
<details open="" class="addition-callout">
<summary>
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#methods-30" target="_blank">link to context in full manuscript</a>
</summary>
<div>
<p><em>Participants.</em> A total of 183 participants were initially recruited from Indiana University Introductory Psychology Courses. Of these, 27 participants were excluded from further analysis due to meeting the exclusion criteria, resulting in a final sample of 156 participants. The exclusion criteria was defined as performance worse (i.e., larger deviations) than the condition average in either the training or testing stage of the experiment. The remaining participants were randomly assigned to one of two training conditions: varied training or constant training.</p>
</div>
</details>
</section>
<section id="igas-vs.-almexam-model-comparisondiscussion" class="level2">
<h2 class="anchored" data-anchor-id="igas-vs.-almexam-model-comparisondiscussion">2) IGAS vs.&nbsp;ALM/EXAM Model Comparison/Discussion</h2>
<details open="" class="addition-callout">
<summary>
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=It%20is%20important%20to%20note" target="_blank">link to context in full manuscript</a>
</summary>
<div>
<p>It is important to note that while both projects utilize computational models, direct comparisons are complicated by the distinct purposes and structures of the models employed in each project. The IGAS model of Project 1 serves as a descriptive measurement model, capturing the similarity between training throws and testing conditions. In contrast, the ALM and EXAM models of Project 2 are full process models, capable of generating exact predictions for both learning and testing stages. The difference is also reflected in the interpretation of the generalization parameter (<span class="math inline">\(c\)</span>) across the models of the two projects. In IGAS, <span class="math inline">\(c\)</span> moderates the similarity between executed throws and subsequent testing solutions, while in ALM and EXAM, <span class="math inline">\(c\)</span> governs the extent to which the perceived stimuli activate the input layer nodes. Despite these differences, insights from ALM/EXAM, particularly the role of zero-point knowledge, may offer potential explanations for the contrasting empirical results. Particularly, EXAM’s reliance on zero-point knowledge in the simpler HTW task may explain why constant training was more effective in Project 2, while the lack of a clear zero-point reference in the more complex HTT task of Project 1 may have increased the value of varied training. This suggests that the benefits of variability depend critically on how task structure interacts with prior knowledge and the learner’s capacity to leverage such knowledge for generalization.</p>
<p>Future work could explore extending ALM and EXAM, which have traditionally been applied to one-dimensional function learning tasks, to more complex motor tasks such as HTT. The neural network structure of ALM could be adapted to handle 2D input by utilizing a 2D grid of input nodes, allowing the model to learn mappings between 2D throwing velocities and desired outcomes. This would allow the model to process the more complex spatial information inherent in tasks like HTT. Furthermore, the output layers of ALM/EXAM could be expanded to express more complex motor outputs in addition to velocity, such as the locations of grabbing and releasing the projectile or other parameters defining the unique trajectories produced. In addition to allowing the models to be applied to more complex tasks, these modifications could enable researchers to investigate how perceptual similarity (i.e., the similarity of stimuli) and motoric similarity (i.e., the similarity of behavioral actions) may separately and jointly influence learning and generalization.</p>
</div>
</details>
</section>
<section id="ordinal-feedback-rationale" class="level2">
<h2 class="anchored" data-anchor-id="ordinal-feedback-rationale">3) Ordinal feedback rationale</h2>
<details open="" class="addition-callout">
<summary>
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#experiment-3-38" target="_blank">link</a>
</summary>
<div>
<blockquote class="blockquote">
<p>In Experiment 3, we sought to further explore the generality of the findings from the first two experiments by modifying the type of feedback provided during training. Specifically, we provided ordinal feedback instead of the continuous feedback used in the previous two experiments. Ordinal feedback provides learners with directional information about the results of their throw (e.g., above the target, below the target, or hitting the target) rather than precise numerical deviations. This form of feedback resembles many real-world learning scenarios, such as a coach instructing an athlete to perform a movement using “more force” or “less force”, or a teacher providing letter grades rather than numeric scores. Although ordinal feedback provides less detailed information per trial, prior research has shown that less detailed feedback isn’t necessarily detrimental to learning. For example, <span class="citation" data-cites="cornwallEffectsCategoricalNumerical2022">Cornwall et al. (<a href="#ref-cornwallEffectsCategoricalNumerical2022" role="doc-biblioref">2022</a>)</span> manipulated whether participants received categorical (correct or incorrect) vs.&nbsp;numerical feedback (reward points ranging from 50-100). They found that the categorical condition produced superior learning, which they explained as arising from larger prediction errors. Although we do not make specific predictions about the ordinal condition in our study, it serves as a manipulation that might influence the learning process.</p>
</blockquote>
</div>
</details>
</section>
<section id="project-2-take-aways" class="level2">
<h2 class="anchored" data-anchor-id="project-2-take-aways">4) Project 2 take-aways:</h2>
<details open="">
<summary style="background-color: #f0f8ff; padding: 10px; border: 1px solid #ccc; border-radius: 5px;">
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=we%20turned%20to%20the%20well" target="_blank">link</a>
</summary>
<div style="background-color: #f9f9f9; padding: 15px; border: 1px solid #ddd; border-radius: 5px;">
<p>To explain our results, we turned to the well established EXAM and ALM models. The disproportionate success of EXAM in capturing the performance of participants under the constant training condition suggests that rule-based extrapolation can emerge even from a limited set of training examples. This success hinges on the assumption that participants are able to leverage prior knowledge of the zero-point reference <span class="citation" data-cites="brownUnderestimationLinearFunction2017 kwantesWhyPeopleUnderestimate2006">(<a href="#ref-brownUnderestimationLinearFunction2017" role="doc-biblioref">Brown &amp; Lacroix, 2017</a>; <a href="#ref-kwantesWhyPeopleUnderestimate2006" role="doc-biblioref">Kwantes &amp; Neal, 2006</a>)</span>. The zero-point reference, combined with accurate learning of the single trained velocity band enabled EXAM to capture the extrapolation patterns of the constant participants. However, it’s important to acknowledge that the ALM model provided a better fit for a subset of participants in each of our three experiment, highlighting the presence of substantial individual differences in generalization patterns.</p>
<p>This finding illustrates the importance of considering task structure when evaluating the effects of training variability on generalization and extrapolation. Some tasks, like the one in this study, may permit the use of zero-point knowledge or other prior information, while others may not. For example, a zero point may be less relevant in visuomotor tasks with complex rotations <span class="citation" data-cites="vandamMappingShapeVisuomotor2015 rollerVariablePracticeLenses2001">(<a href="#ref-rollerVariablePracticeLenses2001" role="doc-biblioref">Roller et al., 2001</a>; <a href="#ref-vandamMappingShapeVisuomotor2015" role="doc-biblioref">van Dam &amp; Ernst, 2015</a>)</span>, or in complex sports techniques <span class="citation" data-cites="northEffectConsistentVaried2019">(<a href="#ref-northEffectConsistentVaried2019" role="doc-biblioref">North et al., 2019</a>)</span>. Future research should systematically investigate how different task structures interact with training variability to influence learning outcomes and generalization abilities, taking into account factors such as the availability of prior knowledge, the complexity of the task, and the specific learning mechanisms involved. This approach could help reconcile seemingly contradictory findings in the literature and provide more nuanced guidelines for designing effective training protocols across various domains.</p>
</div>
</details>
<hr>
</section>
<section id="stimuli-vs.-action-similarity" class="level2">
<h2 class="anchored" data-anchor-id="stimuli-vs.-action-similarity">5) Stimuli vs.&nbsp;Action Similarity</h2>
<p><strong>Note.</strong> The novel text for this issue overlaps with the novel text for the IGAS vs.&nbsp;ALM/EXAM Model Comparison/Discussion issue.</p>
<details open="" class="addition-callout">
<summary>
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=It%20is%20important%20to%20note" target="_blank">link to context in full manuscript</a>
</summary>
<div>
<p>Future work could explore extending ALM and EXAM, which have traditionally been applied to one-dimensional function learning tasks, to more complex motor tasks such as HTT. The neural network structure of ALM could be adapted to handle 2D input by utilizing a 2D grid of input nodes, allowing the model to learn mappings between 2D throwing velocities and desired outcomes. This would allow the model to process the more complex spatial information inherent in tasks like HTT. Furthermore, the output layers of ALM/EXAM could be expanded to express more complex motor outputs in addition to velocity, such as the locations of grabbing and releasing the projectile or other parameters defining the unique trajectories produced. In addition to allowing the models to be applied to more complex tasks, these modifications could enable researchers to investigate how perceptual similarity (i.e., the similarity of stimuli) and motoric similarity (i.e., the similarity of behavioral actions) may separately and jointly influence learning and generalization.</p>
</div>
</details>
<details class="relevant-callout">
<summary>
relevant existing content 1 <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=Our%20modelling%20approach,of%20Knowledge%20II." target="_blank">link</a>
</summary>
<blockquote class="blockquote">
<p>Our modelling approach does differ from category learning implementations of instance-based models in several ways. One such difference is the nature of the training instances that are assumed to be stored. In category learning studies, instances are represented as points in a multidimensional space of all of the attributes that define a category item (e.g., size/color/shape). Rather than defining instances in terms of what stimuli learners experience, our approach assumes that stored, motor instances reflect how they act, in terms of the velocity applied to the ball on each throw. An advantage of many motor learning tasks is the relative ease with which task execution variables can be directly measured (e.g., movement force, velocity, angle, posture) in addition to the decision and response time measures that typically exhaust the data generated from more classical cognitive tasks. Of course, whether learners actually are storing each individual motor instance is a fundamental question beyond the scope of the current work – though as described in the introduction there is some evidence in support of this idea (Chamberlin &amp; Magill, 1992a; Crump &amp; Logan, 2010; Hommel, 1998; Meigh et al., 2018; Poldrack et al., 1999). A particularly noteworthy instance-based model of sensory-motor behavior is the Knowledge II model of Rosenbaum and colleagues (R. G. Cohen &amp; Rosenbaum, 2004; Rosenbaum et al., 1995). Knowledge II explicitly defines instances as postures (joint combinations), and is thus far more detailed than IGAS in regards to the contents of stored instances. Knowledge II also differs from IGAS in that learning is accounted for by both the retrieval of stored postures, and the generation of novel postures via the modification of retrieved postures. A promising avenue for future research would be to combine the adaptive similarity mechanism of IGAS with the novel instance generation mechanisms of Knowledge II.</p>
</blockquote>
</details>
<details class="relevant-callout">
<summary>
relevant existing content 2 <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=It%20is%20common,the%20task%20stimuli." target="_blank">link</a><br>

</summary>
<blockquote class="blockquote">
<p>It is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g., the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.</p>
</blockquote>
</details>
</section>
<section id="how-c-might-be-learned" class="level2">
<h2 class="anchored" data-anchor-id="how-c-might-be-learned">6) How c might be learned</h2>
<details open="" class="addition-callout">
<summary>
Addition in context (<strong>new text bolded</strong>) <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=Our%20results%20thus%20suggest" target="_blank">link</a>
</summary>
<blockquote class="blockquote">
<p>Our results thus suggest that the benefits of variation cannot be explained by the varied-trained participants simply covering a broader range of the task space. Rather, the modeling suggests that varied participants also learn to adaptively tune their generalization function such that throwing locations generalize more broadly to one another than they do in the constant condition. A learning system could end up adopting a higher c value in the constant than variable training conditions by monitoring the trial-by-trial variability of the training items. The <span class="math inline">\(c\)</span> parameter would be adapted downwards when adjacent training items are dissimilar to each other and adapted upwards when adjacent training items are the same. In this fashion, contextually appropriate <span class="math inline">\(c\)</span> values could be empirically learned. This learning procedure would capture the insight that if a situation has a high amount variability, then the learner should be predisposed toward thinking that subsequent test items will also show considerable variability, in which case generalization gradients should be broad, as is achieved by low values for <span class="math inline">\(c\)</span>. <strong><span class="citation" data-cites="sakamotoTrackingVariabilityLearning2006">Sakamoto et al. (<a href="#ref-sakamotoTrackingVariabilityLearning2006" role="doc-biblioref">2006</a>)</span> implemented a similar learning mechanism for updating the generalization paramater in an exemplar-based model (although in their model, a separate generalization parameter is assigned to each exemplar). In their experiment, participants were trained on a high variability and a low variability category, and the dynamically updated generalization parameter was necessary to account for broader generalione zation observed around the high variability category when participants were tested with an ambiguous intermediary item. In a subsequent work <span class="citation" data-cites="sakamotoPuttingPsychologyBack2008">(<a href="#ref-sakamotoPuttingPsychologyBack2008" role="doc-biblioref">Sakamoto et al., 2008</a>)</span>, the same authors showed that a similar learning mechanism could account for the pattern wherein participants generalize more broadly around a category when the average distance between the category exemplars is larger (however the only model tested in this work was a prototype model).</strong></p>
</blockquote>
</details>
<details class="relevant-callout">
<summary>
Relevant existing content <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=A%20learning%20system,values%20for%20c." target="_blank">link 1</a>
</summary>
<blockquote class="blockquote">
<p>As described above, the idea that learners flexibly adjust their generalization gradient based on prior experience does have precedent in the domains of category learning (Aha &amp; Goldstone, 1992; Briscoe &amp; Feldman, 2011; Hahn et al., 2005; Lamberts, 1994; Op de Beeck et al., 2008), and sensorimotor adaptation (Marongelli &amp; Thoroughman, 2013; Taylor &amp; Ivry, 2013; Thoroughman &amp; Taylor, 2005). Lamberts (1994) showed that a simple manipulation of background knowledge during a categorization test resulted in participants generalizing their training experience more or less broadly, and moreover that such a pattern could be captured by allowing the generalization parameter of an instance-based similarity model to be fit separately between conditions. The flexible generalization parameter has also successfully accounted for generalization behavior in cases where participants have been trained on categories that differ in their relative variability (Hahn et al., 2005; Sakamoto et al., 2006). However, to the best of our knowledge, IGAS is the first instance-based similarity model that has been put forward to account for the effect of varied training in a visuomotor skill task. Although IGAS was inspired by work in the domain of category learning, its success in a distinct domain may not be surprising in light of the numerous prior observations that at least certain aspects of learning and generalization may operate under common principles across different tasks and domains (Censor et al., 2012; Hills et al., 2010; Jamieson et al., 2022; Law &amp; Gold, 2010; Roark et al., 2021; Rosenbaum et al., 2001; Vigo et al., 2018; Wall et al., 2021; Wu et al., 2020; J. Yang et al., 2020).</p>
</blockquote>
</details>
<details class="relevant-callout">
<summary>
Relevant existing content <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=However%2C%20previous%20research,the%20trained%20positions." target="_blank">link 2</a>
</summary>
<blockquote class="blockquote">
<p>However, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum &amp; Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the <span class="math inline">\(c\)</span> parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky &amp; Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions.</p>
</blockquote>
</details>
</section>
<section id="foreshadowing-function-learning-literature" class="level2">
<h2 class="anchored" data-anchor-id="foreshadowing-function-learning-literature">7) Foreshadowing function learning literature</h2>
<details open="" class="addition-callout">
<summary>
Addition <a href="https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=Project%202%20will%20focus" target="_blank">link</a>
</summary>
<div>
<p>Project 2 will focus on the domain of function learning and in particular the issue of extrapolation. Function learning research examines how people acquire and generalize knowledge about continuous input-output relationships, and the factors influencing extrapolation to novel inputs following an initial learning phase. The domain of function learning has yielded influential computational models like the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM) <span class="citation" data-cites="busemeyerLearningFunctionalRelations1997">(<a href="#ref-busemeyerLearningFunctionalRelations1997" role="doc-biblioref">Busemeyer et al., 1997</a>)</span>, which have successfully accounted for human learning, interpolation, and extrapolation in numerous investigations<span class="citation" data-cites="deloshExtrapolationSineQua1997 mcdanielConceptualBasisFunction2005 mcdanielPredictingTransferPerformance2009">(<a href="#ref-deloshExtrapolationSineQua1997" role="doc-biblioref">DeLosh et al., 1997</a>; <a href="#ref-mcdanielPredictingTransferPerformance2009" role="doc-biblioref">McDaniel et al., 2009</a>; <a href="#ref-mcdanielConceptualBasisFunction2005" role="doc-biblioref">McDaniel &amp; Busemeyer, 2005</a>)</span>. However, the influence of training variability on function learning, particularly in visuomotor function learning tasks, remains relatively unexplored. Project 2 of this dissertation will address this gap by investigating how constant and varied training regimes affect learning, discrimination, and extrapolation in a novel visuomotor function learning task. We will leverage the ALM and EXAM models, fitted to individual participant data using advanced Bayesian techniques, to provide a detailed computational account of the observed empirical patterns.</p>
</div>
</details>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-brownUnderestimationLinearFunction2017" class="csl-entry" role="listitem">
Brown, M. A., &amp; Lacroix, G. (2017). Underestimation in linear function learning: <span>Anchoring</span> to zero or x-y similarity? <em>Canadian Journal of Experimental Psychology/Revue Canadienne de Psychologie Exp<span>é</span>rimentale</em>, <em>71</em>(4), 274–282. <a href="https://doi.org/10.1037/cep0000129">https://doi.org/10.1037/cep0000129</a>
</div>
<div id="ref-busemeyerLearningFunctionalRelations1997" class="csl-entry" role="listitem">
Busemeyer, J. R., Byun, E., DeLosh, E. L., &amp; McDaniel, M. A. (1997). Learning <span>Functional Relations Based</span> on <span>Experience</span> with <span class="nocase">Input-output Pairs</span> by <span>Humans</span> and <span>Artificial Neural Networks</span>. In <em>Knowledge <span>Concepts</span> and <span>Categories</span></em> (pp. 405–437). Psychology Press.
</div>
<div id="ref-cornwallEffectsCategoricalNumerical2022" class="csl-entry" role="listitem">
Cornwall, A. C., Davis, T., Byrne, K. A., &amp; Worthy, D. A. (2022). Effects of categorical and numerical feedback on category learning. <em>Cognition</em>, <em>225</em>, 105163. <a href="https://doi.org/10.1016/j.cognition.2022.105163">https://doi.org/10.1016/j.cognition.2022.105163</a>
</div>
<div id="ref-deloshExtrapolationSineQua1997" class="csl-entry" role="listitem">
DeLosh, E. L., McDaniel, M. A., &amp; Busemeyer, J. R. (1997). Extrapolation: <span>The Sine Qua Non</span> for <span>Abstraction</span> in <span>Function Learning</span>. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>23</em>(4), 19. <a href="https://doi.org/10.1037/0278-7393.23.4.968">https://doi.org/10.1037/0278-7393.23.4.968</a>
</div>
<div id="ref-kwantesWhyPeopleUnderestimate2006" class="csl-entry" role="listitem">
Kwantes, P. J., &amp; Neal, A. (2006). Why people underestimate y when extrapolating in linear functions. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>32</em>(5), 1019–1030. <a href="https://doi.org/10.1037/0278-7393.32.5.1019">https://doi.org/10.1037/0278-7393.32.5.1019</a>
</div>
<div id="ref-mcdanielConceptualBasisFunction2005" class="csl-entry" role="listitem">
McDaniel, M. A., &amp; Busemeyer, J. R. (2005). The conceptual basis of function learning and extrapolation: <span>Comparison</span> of rule-based and associative-based models. <em>Psychonomic Bulletin &amp; Review</em>, <em>12</em>(1), 24–42. <a href="https://doi.org/10.3758/BF03196347">https://doi.org/10.3758/BF03196347</a>
</div>
<div id="ref-mcdanielPredictingTransferPerformance2009" class="csl-entry" role="listitem">
McDaniel, M. A., Dimperio, E., Jacqueline A. Griego, &amp; Busemeyer, J. R. (2009). Predicting transfer performance: <span>A</span> comparison of competing function learning models. <em>Journal of Experimental Psychology. Learning, Memory, and Cognition</em>, <em>35</em>, 173–195. <a href="https://doi.org/10.1037/a0013982">https://doi.org/10.1037/a0013982</a>
</div>
<div id="ref-northEffectConsistentVaried2019" class="csl-entry" role="listitem">
North, J. S., Bezodis, N. E., Murphy, C. P., Runswick, O. R., Pocock, C., &amp; Roca, A. (2019). The effect of consistent and varied follow-through practice schedules on learning a table tennis backhand. <em>Journal of Sports Sciences</em>, <em>37</em>(6), 613–620. <a href="https://doi.org/10.1080/02640414.2018.1522683">https://doi.org/10.1080/02640414.2018.1522683</a>
</div>
<div id="ref-rollerVariablePracticeLenses2001" class="csl-entry" role="listitem">
Roller, C. A., Cohen, H. S., Kimball, K. T., &amp; Bloomberg, J. J. (2001). Variable practice with lenses improves visuo-motor plasticity. <em>Cognitive Brain Research</em>, <em>12</em>(2), 341–352. <a href="https://doi.org/10.1016/S0926-6410(01)00077-5">https://doi.org/10.1016/S0926-6410(01)00077-5</a>
</div>
<div id="ref-sakamotoPuttingPsychologyBack2008" class="csl-entry" role="listitem">
Sakamoto, Y., Jones, M., &amp; Love, B. C. (2008). Putting the psychology back into psychological models: <span>Mechanistic</span> versus rational approaches. <em>Memory &amp; Cognition</em>, <em>36</em>(6), 1057–1065. <a href="https://doi.org/10.3758/MC.36.6.1057">https://doi.org/10.3758/MC.36.6.1057</a>
</div>
<div id="ref-sakamotoTrackingVariabilityLearning2006" class="csl-entry" role="listitem">
Sakamoto, Y., Love, B. C., &amp; Jones, M. (2006). Tracking <span>Variability</span> in <span>Learning</span>: <span>Contrasting Statistical</span> and <span>Similarity-Based Accounts</span>. <em>Proceedings of the 28th Annual Conference of the Cognitive Science Society. Vancouver, Canada: Cognitive Science Society</em>.
</div>
<div id="ref-vandamMappingShapeVisuomotor2015" class="csl-entry" role="listitem">
van Dam, L. C. J., &amp; Ernst, M. O. (2015). Mapping <span>Shape</span> to <span>Visuomotor Mapping</span>: <span>Learning</span> and <span>Generalisation</span> of <span>Sensorimotor Behaviour Based</span> on <span>Contextual Information</span>. <em>PLOS Computational Biology</em>, <em>11</em>(3), e1004172. <a href="https://doi.org/10.1371/journal.pcbi.1004172">https://doi.org/10.1371/journal.pcbi.1004172</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/Dissertation\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>