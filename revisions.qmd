---
title: Revisions
toc: true
---



## Primary Issues

1) **Plausiblity of fitting ALM & EXAM to project 1** - parameter comparison across projects. \ 

2) **Rationale for ordinal feedback** in experiment 3. Literature suggesting ordinal is different from continuous. Discussion of how ordinal feedback could be implemented in the model \
3) **Project 2 take aways** - implications of empirical results, and of modeling \
4) **More foreshadowing of function learning literature**. Show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization. \
5) **How c is learned** - how how does this show up in training data? Process by which c is updated? Can it account for massed vs. distributed presentation of stimulii? \
6) **Similarity spaces of stimuli vs. actions** - important distinctions? \





## Distinction between experienced stimulii and behavioral action
<details><summary>
[relevant content 1](https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=Our%20modelling%20approach,of%20Knowledge%20II.){target="_blank"}
</summary>
> Our modelling approach does differ from category learning implementations of instance-based models in several ways. One such difference is the nature of the training instances that are assumed to be stored. In category learning studies, instances are represented as points in a multidimensional space of all of the attributes that define a category item (e.g., size/color/shape). Rather than defining instances in terms of what stimuli learners experience, our approach assumes that stored, motor instances reflect how they act, in terms of the velocity applied to the ball on each throw. An advantage of many motor learning tasks is the relative ease with which task execution variables can be directly measured (e.g., movement force, velocity, angle, posture) in addition to the decision and response time measures that typically exhaust the data generated from more classical cognitive tasks. Of course, whether learners actually are storing each individual motor instance is a fundamental question beyond the scope of the current work – though as described in the introduction there is some evidence in support of this idea (Chamberlin & Magill, 1992a; Crump & Logan, 2010; Hommel, 1998; Meigh et al., 2018; Poldrack et al., 1999). A particularly noteworthy instance-based model of sensory-motor behavior is the Knowledge II model of Rosenbaum and colleagues (R. G. Cohen & Rosenbaum, 2004; Rosenbaum et al., 1995). Knowledge II explicitly defines instances as postures (joint combinations), and is thus far more detailed than IGAS in regards to the contents of stored instances. Knowledge II also differs from IGAS in that learning is accounted for by both the retrieval of stored postures, and the generation of novel postures via the modification of retrieved postures. A promising avenue for future research would be to combine the adaptive similarity mechanism of IGAS with the novel instance generation mechanisms of Knowledge II.
</details>

<details><summary>
[relevant content 2](https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=It%20is%20common,the%20task%20stimuli.){target="_blank"}\
</summary>
> It is common for psychological process models of categorization learning to use an approach such as multidimensional scaling so as to transform the stimuli from the physical dimensions used in the particular task into the psychological dimensions more reflective of the actual human representations (Nosofsky, 1992; Shepard, 1987). Such scaling typically entails having participants rate the similarity between individual items and using these similarity judgements to then compute the psychological distances between stimuli, which can then be fed into a subsequent model. In the present investigation, there was no such way to scale the x and y velocity components in terms of the psychological similarity, and thus our modelling does rely on the assumption that the psychological distances between the different throwing positions are proportional to absolute distances in the metric space of the task (e.g., the relative distance between positions 400 and 500 is equivalent to that between 800 and 900). However, an advantage of our approach is that we are measuring similarity in terms of how participants behave (applying a velocity to the ball), rather than the metric features of the task stimuli.
</details>


#### **Relevant Papers**

- @wifallPerceptualSimilarityAffects2014

- @wifallReachingResponseSelection2017




## Learning c 

**How how does this show up in training data? Process by which c is updated? Can it account for massed vs. distributed presentation of stimulii?** \
<details><summary> 
[relevant content](https://tegorman13.github.io/Dissertation/Manuscript/output/manuscript.html#:~:text=However%2C%20previous%20research,the%20trained%20positions.)
</summary>
>However, previous research has suggested that participants may differ in their level of generalization as a function of prior experience, and that such differences in generalization gradients can be captured by fitting the generalization parameter of an instance-based model separately to each group (Hahn et al., 2005; Lamberts, 1994). Relatedly, the influential Bayesian generalization model developed by Tenenbaum & Griffiths (2001) predicts that the breadth of generalization will increase when a rational agent encounters a wider variety of examples. Following these leads, we assume that in addition to learning the task itself, participants are also adjusting how generalizable their experience should be. Varied versus constant participants may be expected to learn to generalize their experience to different degrees. To accommodate this difference, the generalization parameter of the instance-based model (in the present case, the c parameter) can be allowed to vary between the two groups to reflect the tendency of learners to adaptively tune the extent of their generalization. One specific hypothesis is that people adaptively set a value of c to fit the variability of their training experience (Nosofsky & Johansen, 2000; Sakamoto et al., 2006). If one’s training experience is relatively variable, as with the variable training condition, then one might infer that future test situations will also be variable, in which case a low value of c will allow better generalization because generalization will drop off slowly with training-to-testing distance. Conversely, if one’s training experience has little variability, as found in the constant training conditions, then one might adopt a high value of c so that generalization falls off rapidly away from the trained positions.
</details>

## Ordinal feedback rationale

<details><summary>Addition</summary>
> In Experiment 3, we sought to further explore the generality of the findings from the first two experiments by modifying the type of feedback provided during training. Specifically, we provided ordinal feedback instead of the continuous feedback used in the previous two experiments. Ordinal feedback provides learners with directional information about the results of their throw (e.g., above the target, below the target, or hitting the target) rather than precise numerical deviations. This form of feedback resembles many real-world learning scenarios, such as a coach instructing an athlete to perform a movement using "more force" or "less force", or a teacher providing letter grades rather than numeric scores. Although ordinal feedback provides less detailed information per trial, prior research has shown that less detailed feedback isn't necessarily detrimental to learning. For example, Cornwall et al., manipulated whether participants received categorical (correct or incorrect) vs. numerical feedback (reward points ranging from 50-100).
</details>

#### **Relevant Papers**

- @cornwallEffectsCategoricalNumerical2022




## Full Revision Instructions

<details><summary>
show
</summary>
1. Add participant/trial exclusion conditions for Project 2
    
2. Compare parameters values across projects or explain why this isn't possible.  In the General Discussion to both projects, describe the prospects for applying ALM+EXAM to Project 1 (you don't have to actually fit Project 1 with ALM+EXAM).  If ALM+EXAM can likely fit both sets of results, with their opposite generalization patterns vis-a-vis constant vs variable training, then does ALM+EXAM provide a compelling explanation for constant > variable extrapolation in Project 2 or is it too flexible a model?
    
3. Justify why the ordinal feedback in Project 2 Experiment 3 is an interesting manipulation.  Is there a literature suggesting that there would be an important difference between continuous vs ordinal feedback?  How would you incorporate ordinal feedback into ALM+EXAM (you don't have to actually implement this model)?
    
4. In the discussion to Project 2, explain what are the most important implications of the empirical results for theories of human learning and generalization.  Also explain what are the most important implications of the ALM and EXAM modeling are for our understanding of human learning and generalization.  As it currently stands, there is not much of a take-home message from Project 2.
    
5. Somewhere in the introduction to both of the projects, foreshadow the function learning literature that will be relevant to Project 2, and show how Projects 1 and 2 are unified in terms of exploring theories of how variability during learning affects generalization.
    
6. Are there new predictions to make about how the process of c being learned during training would show up in the performance during training, or anywhere else?  For instance, do you propose that c is adjusted on the basis of experienced differences from one trial to the next, or on the basis of running estimates of SD in the data, etc.? Could differences in possible learning processes for c account for effects of massed vs. distributed presentation of stimuli? 
    
7. The distinction between experienced stimuli and behavioral actions  seems like an important factor to explore more in a discussion, given that the similarity space in the two could differ in ways that could affect behavior (and appropriate modeling).


</details>

