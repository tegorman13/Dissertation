---
title: "Variability and Generalization"
bibliography: ../Assets/Bib/Dissertation.bib
link-citations: true
keep-md: true
execute: 
  warning: false
  eval: true
  include: true
---






::: {.content-visible when-format="pdf"}
\begin{centering}

\vspace*{1.5cm}

\LARGE
{Thomas E. Gorman}

\vspace{5cm}

\end{centering}

Submitted to the faculty of the University Graduate School in partial fulfillment of the
requirements for the degree Doctor of Philosophy in the Department of Psychology and Brain
Sciences and the Cognitive Science Program, Indiana University
Indiana University

\vspace{6cm}

\pagenumbering{gobble}


\newpage

Accepted by the Graduate Faculty, Indiana University, in partial fulfillment of the
requirements for the degree of Doctor of Philosophy.
\vspace{4cm}

\
\_____________________________  Robert L. Goldstone
\vspace{2.5cm}
\
\
\_____________________________  Robert Nosofsky
\vspace{2.5cm}
\
\_____________________________  Peter Todd
\vspace{2.5cm}
\
\_____________________________  Mike Jones

\newpage

\begin{centering}
\vspace{15cm}
Thomas Gorman
@2023
\end{centering}
\newpage

:::











# Project 1


## Abstract

Exposing learners to variability during training has been demonstrated
to improve performance in subsequent transfer testing. Such variability
benefits are often accounted for by assuming that learners are
developing some general task schema or structure. However much of this
research has neglected to account for differences in similarity between
varied and constant training conditions. In a between-groups
manipulation, we trained participants on a simple projectile launching
task, with either varied or constant conditions. We replicate previous
findings showing a transfer advantage of varied over constant training.
Furthermore, we show that a standard similarity model is insufficient to
account for the benefits of variation, but, if the model is adjusted to
assume that varied learners are tuned towards a broader generalization
gradient, then a similarity-based model is sufficient to explain the
observed benefits of variation. Our results therefore suggest that some
variability benefits can be accommodated within instance-based models
without positing the learning of some schemata or structure.

