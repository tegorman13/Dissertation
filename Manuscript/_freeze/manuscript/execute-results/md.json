{
  "hash": "04e5d8ebe05202fd84abc0fd56af4ca3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ncode-repo: \"Access the code, data, and analysis at <https://github.com/tegorman13/Dissertation>\"\ntoc: false\ntoc-depth: 3\ntoc-location: body\nexecute: \n  warning: false\n  include: true\n---\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n<div style=\"text-align: center; margin-top: 50px; font-size: 24px; font-weight: bold;\">\n  The Role of Variability in Learning Transfer: A Similarity-Based Computational Approach\n</div>\n\n<div style=\"text-align: center; margin-top: 300px; font-size: 32px;\">\n  Thomas E. Gorman\n</div>\n\n<!-- Second Page and so on... -->\n<div style=\"text-align: justify; margin-top: 700px;\">\n  Submitted to the faculty of the University Graduate School in partial fulfillment of the\n  requirements for the degree Doctor of Philosophy in the Department of Psychology and Brain\n  Sciences and the Cognitive Science Program, Indiana University\n  Indiana University\n</div>\n\n<div style=\"margin-top: 400px;\">\n  <!-- Your page numbering if needed -->\n</div>\n\n<div style=\"page-break-before: always;\"></div>\n\n<!-- Committee Members Page -->\n<div style=\"text-align: center; margin-top: 300px;\">\n  Accepted by the Graduate Faculty, Indiana University, in partial fulfillment of the\n  requirements for the degree of Doctor of Philosophy.\n</div>\n\n<div style=\"text-align: center; margin-top: 300px;\">\n  _____________________________  Robert L. Goldstone, PhD<br><br>\n  _____________________________  Robert Nosofsky, PhD<br><br>\n  _____________________________  Peter Todd, PhD<br><br>\n  _____________________________  Mike Jones, PhD\n</div>\n\n<div style=\"page-break-before: always;\"></div>\n\n\n\n<div style=\"page-break-before: always;\"></div>\n\n<!-- Acknowledgements -->\n<div style=\"text-align: center; font-weight: bold; font-size: 24px;\">\n  Acknowledgements\n</div>\n<div style=\"text-align: center; margin-top: 700px; font-size: 24px;\">\n   \n</div>\n\n\n<div style=\"page-break-before: always;\"></div>\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\\begin{centering}\n\\LARGE\n{The Role of Variability in Learning Transfer: A Similarity-Based Computational Approach}\n\n \n\\vspace*{1.5cm}\n\n\\LARGE\n{Thomas E. Gorman}\n\n\\vspace{16.5cm}\n\n\\end{centering}\n\nSubmitted to the faculty of the University Graduate School in partial fulfillment of the\nrequirements for the degree Doctor of Philosophy in the Department of Psychology and Brain\nSciences and the Cognitive Science Program, Indiana University\nIndiana University\n\n\\vspace{6cm}\n\n\\pagenumbering{gobble}\n\n\n\\newpage\n\nAccepted by the Graduate Faculty, Indiana University, in partial fulfillment of the\nrequirements for the degree of Doctor of Philosophy.\n\\vspace{4cm}\n\n\\\n\\_____________________________  Robert L. Goldstone, PhD\n\\vspace{2.5cm}\n\\\n\\\n\\_____________________________  Robert Nosofsky, PhD\n\\vspace{2.5cm}\n\\\n\\_____________________________  Peter Todd, PhD\n\\vspace{2.5cm}\n\\\n\\_____________________________  Mike Jones, PhD\n\n\\newpage\n\n\\begin{centering}\n\n\\vspace*{6.5cm}\n\n@2023 \\\\\n\\vspace{1cm} \n\nThomas E. Gorman\n\\vspace{.2cm}\n\n\\vspace{5cm}\n\n\\end{centering}\n\n\\newpage\n\\begin{center}\n\\textbf{Acknowledgements}\n\\end{center}\n\\newpage\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"docx\"}\n\n# The Role of Variability in Learning Transfer: A Similarity-Based Computational Approach\n\n## Thomas E. Gorman\n\nSubmitted to the faculty of the University Graduate School in partial fulfillment of the\nrequirements for the degree Doctor of Philosophy in the Department of Psychology and Brain\nSciences and the Cognitive Science Program, Indiana University.\n\n---\n\nAccepted by the Graduate Faculty, Indiana University, in partial fulfillment of the\nrequirements for the degree of Doctor of Philosophy.\n\n- Robert L. Goldstone, PhD\n- Robert Nosofsky, PhD\n- Peter Todd, PhD\n- Mike Jones, PhD\n\n---\n\n## @2023\n\n### Thomas E. Gorman\n\n---\n\n## Acknowledgements\n\n---\n\n\n:::\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n# Abstract\n\nThis dissertation seeks to explore the cognitive underpinnings that govern the generalization of learning, focusing specifically on the role of variability during training in shaping subsequent transfer performance. A comprehensive review of the existing literature is presented, emphasizing the methodological complications associated with disentangling the confounding effects of similarity. Through a series of experiments involving several novel visuomotor tasks, this work investigates whether and how variability in training conditions affects performance in novel tasks. To theoretically account for the empirical outcomes, I employ both instance-based and connectionist computational models, both of which incorporate similarity-based mechanisms. These models serve to account for the extent to which variability influences the learners' generalization gradient, and also explain how training variation can produce both beneficial and deleterious outcomes. \n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n\n# Table of contents\n<div id=\"html-toc\"></div>\n\n\n\n```{=html}\n<script>\n<!-- document.addEventListener(\"DOMContentLoaded\", function(){ -->\n<!--   var toc = document.getElementById(\"TOC\"); -->\n<!--   var placeholder = document.getElementById(\"toc-placeholder\"); -->\n<!--   if (toc && placeholder) { -->\n<!--     placeholder.appendChild(toc); -->\n<!--   } -->\n<!-- }); -->\n\n\n<!-- document.addEventListener(\"DOMContentLoaded\", function(){ -->\n<!--     var tocDiv = document.getElementById(\"html-toc\"); -->\n<!--     var headings = document.querySelectorAll('h1, h2, h3'); // adjust if you need more depth -->\n<!--     var tocList = document.createElement(\"ul\"); -->\n\n<!--     headings.forEach(function(heading) { -->\n<!--         var listItem = document.createElement(\"li\"); -->\n<!--         var link = document.createElement(\"a\"); -->\n<!--         link.textContent = heading.textContent; -->\n<!--         link.href = \"#\" + heading.id; -->\n<!--         listItem.appendChild(link); -->\n\n<!--         if (heading.tagName == \"H2\") { -->\n<!--             listItem.style.marginLeft = \"20px\"; -->\n<!--         } else if (heading.tagName == \"H3\") { -->\n<!--             listItem.style.marginLeft = \"40px\"; -->\n<!--         } -->\n<!--         tocList.appendChild(listItem); -->\n<!--     }); -->\n\n<!--     tocDiv.appendChild(tocList); -->\n<!-- }); -->\n\ndocument.addEventListener(\"DOMContentLoaded\", function(){\n    var tocDiv = document.getElementById(\"html-toc\");\n    var headings = document.querySelectorAll('h1, h2, h3'); // adjust if you need more depth\n    var tocList = document.createElement(\"ul\");\n\n    headings.forEach(function(heading, index) {\n        // Create or get the ID for the heading\n        if (!heading.id) {\n            heading.id = heading.textContent.replace(/[^a-z0-9]+/gi, '-').toLowerCase() + \"-\" + index;\n        }\n        var listItem = document.createElement(\"li\");\n        var link = document.createElement(\"a\");\n        link.textContent = heading.textContent;\n        link.href = \"#\" + heading.id;\n        listItem.appendChild(link);\n\n        if (heading.tagName == \"H2\") {\n            listItem.style.marginLeft = \"20px\";\n        } else if (heading.tagName == \"H3\") {\n            listItem.style.marginLeft = \"40px\";\n        }\n        tocList.appendChild(listItem);\n    });\n    \n    tocDiv.appendChild(tocList);\n});\n\n</script>\n```\n\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\\tableofcontents\n\\newpage\n\\listoffigures\n\\newpage\n\\listoftables\n\\newpage\n\n\n\n:::\n\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n\n# Literature Review\n\n\n\n\n---\n#title: Introduction\n---\n\n\n\n\n# Variability and Generalization\n\nThe factors that influence the generalization of learning are of considerable interest to both researchers exploring the human learning system and practitioners aiming to enhance the effectiveness of educational and training interventions. The present effort will focus specifically on the role of variability in the learning input. Variability manipulations typically regulate either the number of distinct instances presented to learners during training, or the dispersion of these instances. Such manipulations have been empirically demonstrated to affect subsequent generalization performance. This essay will offer an in-depth review of the extant literature on the influence of variability, spanning multiple relevant domains.\n\n## The study of variability\n\nStudies investigating the \"benefits of variability\" hypothesis usually assign participants to either a constant or varied group for the training stage of the experiment. Then, subjects in both groups complete an identical testing stage which often consists items/conditions seen during training, and novel items/conditions. If the varied group performs better in the testing stage, this is taken for evidence of the benefits of variability hypothesis. Even within this relatively straightforward between-groups design, researchers must navigate several crucial methodological choices, highlighted below:\n\n1)  Variables Subject to Variation. In multidimensional tasks, researchers have the option to vary numerous variables. The experimenters must decide the specific dimension(s) across which variation will occur. For instance, in a projectile throwing accuracy task -- researchers might vary the distance from the target, the size of the target, the weight of the projectile. They might also vary a contextual variable not directly relevant to the task, but which will still be encoded by the subject on a trial by trial basis, e.g. the background color.\n\n2)  Magnitude of Variation, relative to the control condition. The simplest comparison would be to compare a constant group who trains with 1 example/condition, against a varied group that trains from 2 examples/conditions. However, it is not uncommon in the literature for the varied condition to train from 3 or 4 conditions. For example, @catalanoDistantTransferCoincident1984a train varied subjects from 4 different velocities in their coincident timing task, and [@goodeSuperiorityVariableRepeated2008] have varied subjects' practice with 3 different variants (i.e. different letter scrambles of the same word) of an anagram for a given word, while their constant participants view the same variant 3 different times. Alternatively, rather than a constant vs. varied comparison, subjects in all conditions might experience a variety of training items, but with one group experiencing a greater number of unique items [@nosofskyModelguidedSearchOptimal2018].\n\n3)  Locations within Task-Space. For tasks in which the stimuli or conditions fall within a continuous metric space, the experimenter must decide whether the varied instances are relatively close together (e.g. throwing a ball from a distance of 4 feet and 5 feet), or far apart (throwing from 4 feet and 20 feet). Spreading the varied training items further apart may be beneficial in terms of providing a more representative sample of the task space to the learner, however large distances may also result in significant differences in difficulty between the training examples, which can be a common confound in variability studies.\n\n4)  Proximity of Testing to Training Conditions. Intuitively, the fairest form of comparison is to include testing conditions that are of an equivalent distance from both the varied and constant groups. However researchers might also attempt to demonstrate the benefits of variation as being sufficiently powerful to outperform constant training, even in cases where the constant group trained from a closer proximity to the testing conditions, or whose training conditions are identical to the testing conditions [@goodeSuperiorityVariableRepeated2008; @kerrSpecificVariedPractice1978].\n\n## Variability Literature Review\n\nAn early and influential work on the influence of variability on category learning is that of @posnerGenesisAbstractIdeas1968. In an ambitious attempt to address the question of how category information is represented, the authors trained participants to categorize artificial dot patterns, manipulating whether learners were exposed to examples clustered close to the category prototypes (e.g. a low variability condition), or spread further away from the prototype (the varied-training group). It should be noted that both groups in this study were trained with the same number of unique instances and the manipulated difference was how spread out the instances were. The authors claim based on prior experiments using the same stimuli, that the training stimuli for the varied group were at least as far away from the testing stimuli as the training stimuli of the less-varied group. The authors interpreted their findings as evidence for the extraction of an abstraction or schema that is extracted and stored, and then over time becomes more likely to be the reference point from which generalization occurs, given that specific instances are thought to decay at a faster rate than prototypes or schema. The Posner and Keele study has been extremely influential and continues to be cited in contemporary research as clear evidence that schema abstraction underlies the benefits of varied training. It's also referenced as a key influence in the development of \"Schema Theory of Motor Learning\" [@schmidtSchemaTheoryDiscrete1975], which in turn influenced decades of investigations on the potential benefits of varied training in motor skill learning. However, the classic Posner & Keele study despite being far more carefully designed than many subsequent studies, and despite being a relative rarity in explicitly discussing and attempting to control for potential confounds of similarity between groups, may nevertheless be emblematic of a common issue in many investigations of the effects of varied training on learning. The problem with Posner & Keele's conclusion was demonstrated clearly almost 3 decades later [@palmeriCentralTendenciesExtreme2001], when researchers conducting a near replication of the original study also collected similarity judgements following training and performed multidimensional scaling analysis. Rather than being in the middle of the training stimuli as was the case in the physical stimuli space, the psychological representation of the prototype was shown reside at an extreme point, and generalization patterns by participants that would have seemed to warrant the learning of a prototype were then easily accounted for with only the assumption that the participants encoded instances. One of the primary concerns of the present paper is that many of the studies which purport to explain the benefits of variation via prototypes, schemas, or other abstractions, are often overlooking the potential of instance based similarity accounts.\n\n## Motor skill learning\n\nTraining variation has also been shown to promote transfer in motor learning. Much of this research has been influenced by the work of [@schmidtSchemaTheoryDiscrete1975], who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes a priori that learners possess general motor programs for classes of movements, such as an underhand throw. When called up for use, such programs must be parameterized, as well as schema rules that determine how a motor program is parameterized or scaled for a particular movement. Schema theory predicts that varied training results in the formation of a more general schema-rule, which can allow for transfer to novel movements within a given movement class, such as an underhand throw (though it is agnostic to the development of the movement classes themselves). Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, investigators might train two groups of participants to throw a projectile at a target, with a constant group that throws from a single location, and a varied group that throws from multiple locations. Both groups are then tested from novel locations.\n\nOne of the earliest, and still often cited investigations of Schmidt's benefits of variability hypothesis was the work of @kerrSpecificVariedPractice1978. Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet. Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blindfolded for testing and did not receive trial by trial feedback in this stage. However, at the halfway point of the testing stage they were allowed to see the landing location of the 4 beanbags they had thrown, and then completed the final 4 testing throws. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. Although this design does not directly assess the hypothesis of varied training producing superior generalization to constant training (since the constant group is not tested from a novel position), it nevertheless offers a compelling example of the merits of varied practice.\n\nOn occasion the Kerr and Booth design may be nested within a larger experimental design. One such study that used a movement timing task, wherein subjects had to move their hand from a starting location, to a target location, attempting to arrive at the target location at specific time following the onset of a cue [@wrisbergVariabilityPracticeHypothesis1987]. This study utilized 4 different constant groups, and 3 varied groups, with one of the constant groups training under conditions identical to the testing conditions, and which were not trained on by any of the varied groups, e.g. the design of Kerr and Booth. However, in this case the varied group did not outperform the constant group. A more recent study attempting a slightly more direct replication of the original Kerr & Booth study [@willeyLongtermMotorLearning2018], having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). However, this study diverged from the original in that the participants were adults; they faced away from the target and threw the beanbag backwards over their bodies; they alternated using their right and left hands every 6 trials; and underwent a relatively extreme amount of training (20 sessions with 60 practice trials each, spread out over 5-7 weeks). Like @wrisbergVariabilityPracticeHypothesis1987, this study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups.\n\nSome support for the Kerr and Booth findings was found with a relatively less common experimental task of training participants in hitting a projectile at a target with the use of a racket [@greenPracticeVariabilityTransfer1995a]. Varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket. One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. Of course, this finding is less surprising than that of Kerr & Booth, given that varied subjects did have some prior exposure to the constant groups condition. This highlights an issue rarely discussed in the literature, of how much practice from an additional position might be necessary to induce benefits. Experimenters almost uniformly have varied participants train with an equivalent number of trials from each of their conditions\n\nOne of the few studies that has replicated the surprising result of varied outperforming constant, from the constant training condition, did so in the relatively distant domain of verbal manipulation [@goodeSuperiorityVariableRepeated2008]. All participants trained to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g. LDOOF), whereas for constant group B, the testing phase consisted of a arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had 3 prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and actually achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups -- solving 2 more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution -- further suggesting that there may not have been a strong identity advantage in this task.\n\nPitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this particular contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters [@catalanoDistantTransferCoincident1984a; @chuaPracticeVariabilityPromotes2019;\n@newellVariabilityPracticeTransfer1976; @moxleySchemaVariabilityPractice1979;\n@mccrackenTestSchemaTheory1977], or for a single constant group to train from just one of the conditions experienced by the varied participants [@pigottMotorSchemaStructure1984; @rollerVariablePracticeLenses2001; @wrisbergTrainingProductionNovel1984; @wrisbergDevelopingCoincidentTiming1983]. A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition [@wrisbergVariabilityPracticeHypothesis1987; @wulfVariabilityPracticeImplicit1997].\n\n\n\nOf particular relevant to the current essay is the early work of @catalanoDistantTransferCoincident1984a, as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions -- 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.\n\n\n## Category learning\n\n\n\nIn the category learning literature, the constant vs. varied comparison is much less suitable. Instead, researchers tend to compare a condition with many repetitions of a few items against condition with fewer repetitions of a wider array of exemplars. Much of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns [@homaCategoryBreadthAbstraction1976; @posnerGenesisAbstractIdeas1968], where more varied or distorted training examples were often shown to produce superior generalization when categorizing novel exemplars. More recently, researchers have also begun to utilize more realistic stimuli in their experiments. @wahlheimMetacognitiveJudgmentsRepetition2012 conducted one such study. In a within-participants design, participants were trained on bird categories with either high repetitions of a few exemplars, or few repetitions of many exemplars. Across four different experiments, which were conducted to address an unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization [@nosofskyTestsExemplarmemoryModel2018]. For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions. Moreover, the pattern of results in this study could be nicely accounted for by an extended version of the Generalized Context Model.\n\nThe studies described thus far have studied the benefits of variability by exposing participants to a greater or lesser number of distinct examples during training. A distinct sub-literature within the category learning domain has focused much less on benefits derived from varied training, instead emphasizing how increased variability during the learning of a novel category influences how far the category boundary will then be generalized. The general approach is to train participants on examples from two categories, with the examples from one of the categories being more dispersed than the other. Participants are then tested with novel items located within ambiguous regions of the task space which allow the experimenters to assess whether the difference in variability influences how far participants generalize the category boundaries.\n\n@cohenCategoryVariabilityExemplar2001 trained subjects on two categories, one with much more variability than the other. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into a category with greater variation. Moreover, this effect was at odds with the predications of the baseline version of the GCM, thus providing some evidence that training variation may at least sometimes induce effects that cannot be entirely accounted for by exemplar-similarity accounts.\n\n\nFurther observations consonant with the results of @cohenCategoryVariabilityExemplar2001  have since been observed in numerous investigations [@hahnEffectsCategoryDiversity2005; @perlmanFurtherAttemptsClarify2012;\n@sakamotoPuttingPsychologyBack2008; @hsuEffectsGenerativeDiscriminative2010]. The results of @sakamotoPuttingPsychologyBack2008 are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.\n\nThe work of @hahnEffectsCategoryDiversity2005, is also of particular interest to the present discussion. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with \"neither\", in addition to responses to the two training categories. The \"neither\" response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The authors then used the standard GCM framework to compare a variety of similarity-based models to account for their results. Of particular interest are their evaluations of a category response bias parameter, and a similarity scaling parameter. A model fit improvement when the response bias parameter is allowed to vary between the high-variability and low-variability trained groups is taken to suggest a simple bias for responding with one of the trained categories over the other. Alternatively, an improvement in fit due to a separate similarity scaling parameter may reflect the groups being differentially sensitive to the distances between stimuli. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improvement significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items. This model accounted for both the extended generalization gradients of the varied particpants, and also for their poor performance in a recognition condition. Additional model comparisons suggested that this similarity rescaling applied across the entire stimulus space, rather than to the high variability category in particular.\n\nVariability effects have also been examined in the higher-level domain of how learners acquire novel concepts, and then instantiate (rather than merely recognize) that concept in untrained contexts [@braithwaiteEffectsVariationPrior2015]. This study trained participants on problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g. all involving people selecting objects) or in which the surface features were varied between examples (e.g. people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants -- such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that in order to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples.\n\nVariability has also been examined within the realm of language learning. A particularly impressive study is that of [@perryLearnLocallyThink2010]. In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the tight condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. The varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category, a pattern consistent with much of the existing literature. More interesting was the superior performance of the varied group on a higher order generalization task -- such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.\n\n## Complications to the influence of variability\n\n### Sequence Effects\n\nA necessary consequence of varied training is that participants will have the experience of switching from one task condition to another. The number of switches can vary greatly, with the two extremes being varied participants completing all of their training trials in one before switching to the next condition (blocked sequencing), or if they alternate between conditions on a trial by trial basis (random/intermixed/interleaved sequencing). Not long after the initial influx of schema-theory inspired studies testing the benefits of variability hypothesis was shown that the influence of varied training might interact with the type of training sequence chosen by the experimenter  [@sheaContextualInterferenceEffects1979]. In this seminal study, both groups of training subjects trained with the same number of trials of three separate movement patterns. A blocked group that completed all of their trials with one sequence before beginning the next sequence, and a random group that trained with all three movement patterns interspersed throughout the course of training. Participants were also randomly assigned to retention testing under either blocked or random sequence conditions, thus resulting in all four training-testing combinations of blocked-blocked; blocked-random; random-blocked; random-random. There was some effect of sequence context, such that both groups performed better when the testing sequence matched their training sequence. However, the main finding of interest was the advantage of random-training, which resulted in superior testing performance than blocked training regardless of whether the testing stage had a blocked or random sequence, an effect observed both immediately after training, and in a follow up test ten days after the end of training.\n\nPrior to the influential @sheaContextualInterferenceEffects1979 study, studies investigating the benefits of variability hypothesis had utilized both blocked and random training schedules, often without comment or justification. It was later observed [@leeInfluencePracticeSchedule1985] that positive evidence for benefits of varied training seemed more likely to occur for studies that utilized random schedules. The theoretical basis of such studies was invariably an appeal to Schmidt's schema theory; however schema theory made no clear predictions of an effect of study sequence on retention or generalization, thus prompting the need for alternate accounts. One such account, the elaborative processing account [@sheaContextEffectsMemory1983], draws on the earlier work [@battigFacilitationInterference1966] and argues that randomly sequencing conditions during training promotes comparison and contrastive processes between those conditions, which result in a deeper understanding of the training task than could arise via blocked sequencing. Supporting evidence for elaborative processing comes in the form of random-sequence trained subjects self-reporting more nuanced mental representations of movement patterns following training [@sheaContextEffectsMemory1983], and by manipulating whether subjects are able to perform comparisons during training [@wrightContributionElaborativeProcessing1992]. An alternative, though not incompatible account suggests that the benefits of random-sequencing are a result of such sequences forcing the learner to continually reconstruct the relevant motor task in working memory [@leeLocusContextualInterference]. Blocked training, on the other hand, allows the learner to maintain the same motor task in short term memory without decay for much of the training which facilitates training performance, but hinders ability to retrieve the appropriate motor memory in a later testing context. A much more recent study [@chuaPracticeVariabilityPromotes2019], replicates the standard findings of an advantage of varied training over constant training (expt 1, bean-bag throwing task), and of random training over blocked training (expt 2 & 3, bean-bag throwing & golf putting). The novelty of this study is that the experimenters queried subjects about their attentional focus throughout the training stage. In all three experiments varied or random trained-subjects reported significantly greater external attention (e.g. attending to the target distance), and constant or blocked subjects reported more internal attention (e.g. posture or hand position). The authors argue that the benefits of varied/random training may be mediated by changes in attentional focus, however the claims made in the paper seem to go far beyond what can be justified by the analyses reported -- e.g. the increased external focus could be a simple byproduct of varied training. A stronger form of evidence that was not provided may have been to use multiple regression analyses to show that the testing advantage of the varied/random groups over the constant/blocked groups could be accounted for by the differences in self-reported attentional focus.\n\n## Other task and participant effects\n\nOf course, the effects of varied training, and different training sequences, are likely to be far more complex than simply more varied training being better than less, or random training being better than blocked. Null effects of both manipulations have been reported [see @magillReviewContextualInterference1990; @vanrossumSchmidtSchemaTheory1990 for reviews], and a variety of moderators have emerged. In one of the earlier examples of the complex relationship between study sequence and learning [@delreyEffectsContextualInterference1982], experimenters recruited participants who self-reported either large amounts, or very little experience with athletic actives, and then trained participants on a coincident timing task under with either a single constant training velocity, or with four training velocities under either blocked, or random training sequence conditions - resulting in six experimental conditions: (athlete vs. non-athlete) x (constant vs. blocked vs. random training). Athlete participants had superior performance during training, regardless of sequence condition, and training performance was superior for all subjects in the constant group, followed by blocked training in the middle, and then random training resulting in the worst training performance. Of greater interest is the pattern of testing results for novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received random training, followed by blocked, and then constant training. Non-athletes showed the opposite pattern, with superior performance for those who had constant training. A similar pattern was later observed in a golf-putting training study, wherein participants who had some prior golf experience benefited most from random-sequenced training, and participants with no golf experience benefited most from blocked training  [@guadagnoliRelationshipContextualInterference1999]. More recently, the same pattern was observed in the concept learning literature [@braithwaiteEffectsVariationPrior2015 expt 1.]. This study trained participants on a mathematical concept and found that participants who self-reported some prior experience with the concept improved more from pre-test to post-test after training with varied examples, while participants who reported no prior experience showed greater gains following training with highly similar examples.\n\nIn addition to the influence of prior experiences described above, ample evidence also suggests that numerous aspects of the experiment may also interact with the influence of variation. One important study examined the impact of the amount of training completed in a force production task [@sheaContextualInterferenceContributions1990]. This study employed a typical blocked vs. random training procedure, but with the additional manipulation of separate groups receiving 50, 200, or 400 total training trials. For the group that received only 50 training trials retention was best when training had been blocked. However, for the conditions that received 200 or 400 training trials the pattern was reversed, with random training resulting in superior retention than blocked training. These results were taken to suggest that the benefits of randomization may take time to emerge. Another experimental factor shown to interact with training sequence is the complexity of the training task [@albaretDifferentialEffectsTask1998]. In addition to random or blocked training, participants in this study were assigned to train on a drawing task at one of three different levels of complexity (reproducing from memory shapes composed of two, three or four components). On a transfer task 48 hours after the completion of training, only participants trained at the lower levels of task complexity (2 or 3 components) showed superior performance to the blocked condition. The authors suggest that the benefits of random sequencing, thought to arise from more elaborate cognitive processing, or the necessity of continually recalling task information from long term into short term memory, are more likely to be obscured as the complexity of the task forces the blocked participants to also engage in such processes.\n\nA final important influence of particular relevance to the practice-sequence literature concerns the exact structure of \"random\" sequencing. Although the term random is commonly used for convenience, experimenters do not typically leave the order of training entirely up to chance. Rather, the training sequence is often constrained such that each condition must occur a minimum number of times in each quartile of the training phase, thus resulting in an even distribution the conditions throughout training. While the assurance of the conditions being evenly spread throughout training is consistent across studies, other aspects of the sequence structure are a bit more idiosyncratic. Some researchers report setting a maximum number of consecutive repetitions, e.g. no more than 2 consecutive trials of the same condition [@delreyEffectsContextualInterference1982; @sheaContextualInterferenceEffects1979], or structure the random trials such that the same condition never occurs consecutively [@wulfEffectTypePractice1991]. Also common is to structure experiments such that random condition really consists of many small blocks, where participants do a few trials of one condition consecutively and then switch to another condition [@willeyLongtermMotorLearning2018; @chuaPracticeVariabilityPromotes2019; @wrisbergVariabilityPracticeHypothesis1987], resulting in many more switches than would arise if training was perfectly blocked. The question of whether such differences in the structure of random sequencing are consequential has been addressed experimentally a few times, in all cases consisting of a 1) a no-repeat random condition; 2) a blocked random condition (typically 3 or 4 repeats before a switch); and 3) a standard fully-blocked condition. Blocked-random training resulted in better performance than either repeat-random, or fully - blocked training in both a bean-bag throwing [@pigottMotorSchemaStructure1984], and basketball shot training study [@landinComparisonThreePractice1997], and in a replication plus extension of the seminal [@sheaContextualInterferenceEffects1979] study, blocked-random training was equally effective as no-repeat random training, with both random structures leading to better performance than the fully-blocked training condition. Consequences on different study schedules have also been repeatedly observed in the category learning literature [@carvalhoSequenceStudyChanges2017; @carvalhoPuttingCategoryLearning2014]. This line of research has revealed that the effects of blocking vs. interleaving can depend on the structure of the category being learned, and also that the different schedules can result in the participants requiring different representations. A fruitful line of inquiry in the motor skill learning literature may be to attempt to identify whether structural aspects of the motor task interact with different training sequences in a reliable manner.\n\nNumerous researchers have attempted to provide coherent frameworks to account for the full range of influences of training variation and sequencing described above (along with many other effects not discussed). Such accounts are generally quite similar, invoking ideas of desirable levels of difficulty [@bjorkNewTheoryDisuse1992; @schmidtNewConceptualizationsPractice1992], or optimal challenge points [@guadagnoliChallengePointFramework2004]. They tend to start by describing the dissociation between acquisition performance (performance during training) and testing performance (delayed retention and/or transfer), most strikingly observed as varied/random training participants performing worse than their constant/blocked counterparts during the training stage of the study, but then outperforming the constant/blocked comparisons at a later retention or transfer stage. This observation is then used to justify the idea that the most enduring and generalizable learning occurs by training at an optimal level of training difficulty, with difficulty being some function of the experience of the learner, and the cognitive or visuomotor processing demands of the task. It then follows that the factors that tend to make training more difficult (i.e. increased variability or randomization), are more likely to be beneficial when the learner has some experience, or when the processing demands of the task are not too extreme (which may only occur after some experience with the task). Such frameworks may be helpful heuristics in some cases, but they also seem to be overly flexible such that any null result of some intervention might be accounted for by a suboptimal amount of training trials, or by suggesting the training task was too difficult. The development of computational models that can account for how changes in the parameters of the motor-skill task scale with difficulty, would be a great step forward.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| \nhere::set_here(path='..')\nsource(here::here(\"Functions\", \"packages.R\"))\n\nif (is.null(knitr::pandoc_to())) {\n  fmt_out <- \"interactive\"\n} else {\n  fmt_out <- knitr::pandoc_to()\n}\n\n# knitr::opts_chunk$set(echo = FALSE, include =TRUE, \n#                       warning = FALSE, message = FALSE, eval=TRUE)\n\n# knitr::opts_chunk$set(fig.align = \"center\", fig.retina = 3,\n#                       fig.width = 6, fig.height = (6 * 0.618),\n#                       out.width = \"100%\", collapse = TRUE)\n# \noptions(digits = 3, width = 120,\n        dplyr.summarise.inform = FALSE,\n        knitr.kable.NA = \"\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here('Functions/IGAS_ProcessFunctions.R'))\n\ne1 <- readRDS(here::here(\"data/igas_e1_cleanedData-final.rds\")) %>% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity,stageInt=as.numeric(as.character(experimentStage)))\n\n# load the processed data from experiment 1 and 2\n\ne2<- readRDS(here::here('data/igas_e2_cleanedData-final.rds')) %>% mutate(initialVelocityX=X_Velocity,initialVelocityY=Y_Velocity)\n\n# load subject similarity data - computed with the IGAS model in 'IGAS-SimModel.R'\ne2_sim <- readRDS(here::here('data/IGAS_Similarity-Performance.rds'))\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\ndefaultContrasts = options()$contrasts\n\ndodge <- position_dodge(width = 0.9)\ne2GrpPos <- c(\"400\",\"500\",\"625\",\"675\",\"800\",\"900\")\ne2Grp <- paste(\"Constant\",\"Constant\", \"Constant\",\"Constant\",\"Constant\",\"Constant\", \"Varied\")\ne2Labels <- paste(c(\"400\\n Constant\",\"500\\n Constant\",\"625\\n Constant\",\"675\\n Constant\",\n                   \"800\\n Constant\",\"900\\n Constant\",\"500-800\\n Varied\"),sep=\"\")\n\ne1Pos <- c(\"610\",\"760\",\"835\",\"910\")\ne1Var <- paste(\"Varied Train Position\",\"Constant Train Position\", \"Novel Position\", \"Varied Training Position\")\ne1Labels<- paste(c(\"610\\n Varied Trained\",\"760\\n Constant Trained\",\"835\\n Novel Location\",\"910\\n Varied Trained\"),sep=\"\")\n```\n:::\n\n\n\n\n\n# Project 1\n\n\n## Abstract\n\nExposing learners to variability during training has been demonstrated\nto improve performance in subsequent transfer testing. Such variability\nbenefits are often accounted for by assuming that learners are\ndeveloping some general task schema or structure. However much of this\nresearch has neglected to account for differences in similarity between\nvaried and constant training conditions. In a between-groups\nmanipulation, we trained participants on a simple projectile launching\ntask, with either varied or constant conditions. We replicate previous\nfindings showing a transfer advantage of varied over constant training.\nFurthermore, we show that a standard similarity model is insufficient to\naccount for the benefits of variation, but, if the model is adjusted to\nassume that varied learners are tuned towards a broader generalization\ngradient, then a similarity-based model is sufficient to explain the\nobserved benefits of variation. Our results therefore suggest that some\nvariability benefits can be accommodated within instance-based models\nwithout positing the learning of some schemata or structure.\n\n\n## Introduction\n\nThe past century of research on human learning has produced ample\nevidence that although learners can improve at almost any task, such\nimprovements are often specific to the trained task, with unreliable or\neven nonexistent transfer to novel tasks or conditions\n[@barnettWhenWhereWe2002; @dettermanCaseProsecutionTransfer1993]. Such\ntransfer challenges are of noteworthy practical relevance, given that\neducators, trainers, and rehabilitators typically intend for their\nstudents to be able to apply what they have learned to new situations.\nIt is therefore important to better understand the factors that\ninfluence transfer, and to develop cognitive models that can predict\nwhen transfer is likely to occur. The factor of interest to the present\ninvestigation is variation during training. Our experiments add to the\nlongstanding empirical investigation of the controversial relationship\nbetween training variation, and subsequent transfer. We also offer a\nnovel explanation for such results in the form of an instance-based\nmodel that accounts for the benefits of variation in simple terms of\npsychological similarity. We first review the relevant concepts and\nliterature.\n\n### Similarity and instance-based approaches to transfer of learning\n\nNotions of similarity have long played a central role in many prominent\nmodels of generalization of learning, as well as in the longstanding\ntheoretical issue of whether learners abstract an aggregate, summary\nrepresentation, or if they simply store individual instances. Early\nmodels of learning often assumed that discrete experiences with some\ntask or category were not stored individually in memory, but instead\npromoted the formation of a summary representation, often referred to as\na prototype or schema, and that exposure to novel examples would then\nprompt the retrieval of whichever preexisting prototype was most similar\n[@posnerGenesisAbstractIdeas1968]. Prototype\nmodels were later challenged by the success of instance-based or\nexemplar models -- which were shown to provide an account of\ngeneralization as good or better than prototype models, with the\nadvantage of not assuming the explicit construction of an internal\nprototype [@estesClassificationCognition1994;\n@hintzmanMINERVASimulationModel1984;\n@medinContextTheoryClassification1978;\n@nosofskyAttentionSimilarityIdentificationcategorization1986 ].\nInstance-based models assume that learners encode each experience with a\ntask as a separate instance/exemplar/trace, and that each encoded trace\nis in turn compared against novel stimuli. As the number of stored\ninstances increases, so does the likelihood that some previously stored\ninstance will be retrieved to aid in the performance of a novel task.\nStored instances are retrieved in the context of novel stimuli or tasks\nif they are sufficiently similar, thus suggesting that the process of\ncomputing similarity is of central importance to generalization.\n\nSimilarity, defined in this literature as a function of psychological\ndistance between instances or categories, has provided a successful\naccount of generalization across numerous tasks and domains. In an\ninfluential study demonstrating an ordinal similarity effect,\nexperimenters employed a numerosity judgment task in which participants\nquickly report the number of dots flashed on a screen. Performance (in\nterms of response times to new patterns) on novel dot configurations\nvaried as an inverse function of their similarity to previously trained\ndot configurations @palmeriExemplarSimilarityDevelopment1997. That is, performance was better on\nnovel configurations moderately similar to trained configurations than\nto configurations with low-similarity, and also better on low-similarity\nconfigurations than to even less similar, unrelated configurations.\nInstance-based approaches have had some success accounting for\nperformance in certain sub-domains of motor learning [@cohenWhereGraspsAre2004; @crumpEpisodicContributionsSequential2010; @meighWhatMemoryRepresentation2018; @poldrackRelationshipSkillLearning1999; @wifallReachingResponseSelection2017; @crumpEpisodicContributionsSequential2010] trained participants to type words on an unfamiliar keyboard, while constraining the letters composing the training words to a pre-specified letter set. Following\ntraining, typing speed was tested on previously experienced words\ncomposed of previously experienced letters; novel words composed of\nletters from the trained letter set; and novel words composed of letters\nfrom an untrained letter set. Consistent with an instance-based account,\ntransfer performance was graded such that participants were fastest at\ntyping the words they had previously trained on, followed by novel words\ncomposed of letters they had trained on, and slowest performance for new\nwords composed of untrained letters.\n\n## The effect of training variability on transfer\n\nWhile similarity-based models account for transfer by the degree of\nsimilarity between previous and new experiences, a largely separate body\nof research has focused on improving transfer by manipulating\ncharacteristics of the initial training stage. Such characteristics have\nincluded training difficulty, spacing, temporal order, feedback\nschedules, and the primary focus of the current work -- variability of\ntraining examples.\n\nResearch on the effects of varied training typically compares\nparticipants trained under constant, or minimal variability conditions\nto those trained from a variety of examples or conditions [@czyzVariabilityPracticeInformation2021; @soderstromLearningPerformanceIntegrative2015]. Varied training has been shown to influence\nlearning in myriad domains including categorization of simple stimuli [@hahnEffectsCategoryDiversity2005; @maddoxStimulusRangeDiscontinuity2011; @posnerGenesisAbstractIdeas1968],\ncomplex categorization [@nosofskyModelguidedSearchOptimal2018], language learning [@jonesDensityDistinctivenessEarly2020; @perryLearnLocallyThink2010; @twomeyAllRightNoises2018; @wonnacottInputEffectsAcquisition2012], anagram completion [@goodeSuperiorityVariableRepeated2008], trajectory\nextrapolation [@fulvioTaskSpecificResponseStrategy2014], task switching [@sabahWhenLessMore2019], associative learning [@leeEvidentialDiversityIncreases2019], visual search [@georgeStimulusVariabilityTask2021; @gonzalezDiversityTrainingEnhances2011; @kelleyLearningAttendEffects2009], voice\nidentity learning [@lavanEffectsHighVariability2019], simple motor learning [@braunMotorTaskVariation2009; @kerrSpecificVariedPractice1978; @rollerVariablePracticeLenses2001; @willeyLimitedGeneralizationVaried2018],\nsports training  [@greenPracticeVariabilityTransfer1995a @northEffectConsistentVaried2019], and training\non a complex video game [@seowTransferEffectsVaried2019].\n\nTraining variation has received a particularly large amount of attention\nwithin the domain of visuomotor skill learning. Much of this research\nhas been influenced by the work of @schmidtSchemaTheoryDiscrete1975, who proposed a\nschema-based account of motor learning as an attempt to address the\nlongstanding problem of how novel movements are produced. According to\nSchema Theory, learners possess general motor programs for classes of\nmovements (e.g. throwing a ball with an underhand movement), as well as\nschema rules that determine how a motor program is parameterized or\nscaled for a particular movement. Schema theory predicts that varied\ntraining results in the formation of a more general schema-rule, which\ncan allow for transfer to novel movements within a given movement class.\nExperiments that test this hypothesis are often designed to compare the\ntransfer performance of a constant-trained group against that of a\nvaried-trained group. Both groups train on the same task, but the varied\ngroup practices from multiple levels of a task-relevant dimension that\nremains invariant for the constant group. For example, investigators\nmight train two groups of participants to throw a projectile at a\ntarget, with a constant group that throws from a single location, and a\nvaried group that throws from multiple locations. Both groups are then\ntested from novel locations. Empirically observed benefits of the\nvaried-trained group are then attributed to the variation they received\nduring training, a finding observed in numerous studies [@catalanoDistantTransferCoincident1984a; @chuaPracticeVariabilityPromotes2019; @goodwinEffectDifferentQuantities1998; @kerrSpecificVariedPractice1978; @wulfEffectTypePractice1991], and the benefits of this variation are typically\nthought to be mediated by the development of a more general schema for\nthe throwing motion.\n\nOf course, the relationship between training variability and transfer is\nunlikely to be a simple function wherein increased variation is always\nbeneficial. Numerous studies have found null, or in some cases negative\neffects of training variation [@deloshExtrapolationSineQua1997; @sinkeviciuteRoleInputVariability2019; @wrisbergVariabilityPracticeHypothesis1987], and many more have suggested that the\nbenefits of variability may depend on additional factors such as prior\ntask experience, the order of training trials, or the type of transfer\nbeing measured [@bernikerEffectsTrainingBreadth2014; @braithwaiteEffectsVariationPrior2015; @hahnEffectsCategoryDiversity2005; @lavanEffectsHighVariability2019; @northEffectConsistentVaried2019; @sadakataIndividualAptitudeMandarin2014; @zamanPerceptualVariabilityImplications2021]. \n\n### Issues with Previous Research\n\nAlthough the benefits of training variation in visuomotor skill learning\nhave been observed many times, null findings have also been repeatedly\nfound, leading some researchers to question the veracity of the\nvariability of practice hypothesis [@newellSchemaTheory19752003; @vanrossumSchmidtSchemaTheory1990].\nCritics have also pointed out that investigations of the effects of\ntraining variability, of the sort described above, often fail to control\nfor the effect of similarity between training and testing conditions.\nFor training tasks in which participants have numerous degrees of\nfreedom (e.g. projectile throwing tasks where participants control the x\nand y velocity of the projectile), varied groups are likely to\nexperience a wider range of the task space over the course of their\ntraining (e.g. more unique combinations of x and y velocities).\nExperimenters may attempt to account for this possibility by ensuring\nthat the training location(s) of the varied and constant groups are an\nequal distance away from the eventual transfer locations, such that\ntheir training throws are, on average, equally similar to throws that\nwould lead to good performance at the transfer locations. However, even\nthis level of experimental control may still be insufficient to rule out\nthe effect of similarity on transfer. Given that psychological\nsimilarity is typically best described as either a Gaussian or\nexponentially decaying function of psychological distance [@ennisMultidimensionalStochasticTheory1988; @ghahramaniGeneralizationLocalRemappings1996; @loganInstanceTheoryAutomatization1988; @nosofskySimilarityScalingCognitive1992; @shepardUniversalLawGeneralization1987; @thoroughmanRapidReshapingHuman2005 ], it is plausible that a subset of the\nmost similar training instances could have a disproportionate impact on\ngeneralization to transfer conditions, even if the average distance\nbetween training and transfer conditions is identical between groups.\n@fig-toy-model1 demonstrates the consequences of a generalization gradient that\ndrops off as a Gaussian function of distance from training, as compared\nto a linear drop-off.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np=2\nc<- .0002\nsimdat <- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %>%\n                     mutate(plotjitter=ifelse(condit==\"varied\",0,7),\n                            linScale=ifelse(condit==\"varied\",980,1000),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genLinear=1000-abs(x-train.position)+plotjitter) %>% \n  #group_by(condit) %>% mutate(scaleLinear=(genLinear-min(genLinear))/(max(genLinear)-min(genLinear))) \n  group_by(x,condit) %>%\n  reframe(genGauss=mean(genGauss),genLinear=mean(genLinear)/linScale,.groups = 'keep')\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs <- list(geom_line(alpha=.7,size=.4),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=447,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=450,y=.97,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=659,y=1.05,label=\"Constant\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=657,y=.97,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=847,y=1.05,label=\"Varied\",size=3.1,fontface=\"plain\"),\n                  annotate(geom=\"text\",x=850,y=.97,label=\"Training\",size=3.1,fontface=\"plain\"),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1),\n                        legend.position=\"none\"))\n\nip1 <- simdat  %>% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"\")\nip2 <- simdat %>%  ggplot(aes(x,y=genLinear,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")\n\nplot_grid(ip1,ip2,ncol=2,rel_heights=c(1))\n```\n\n::: {.cell-output-display}\n![Left panel- Generalization predicted from a simple model that assumes a linear generalization function. A varied group (red vertical lines indicate the 2 training locations) trained from positions 400 and 800, and a constant group (blue vertical line), trained from position 600. Right panel- if a Gaussian generalization function is assumed, then varied training (400, 800) is predicted to result in better generalization to positions close to 400 and 800 than does constant training at 600. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)](manuscript.markdown_strict_files/figure-markdown_strict/fig-toy-model1-1.jpeg){#fig-toy-model1}\n:::\n:::\n\n\n\n\n\nIn addition to largely overlooking the potential for non-linear\ngeneralization to confound interpretations of training manipulations,\nthe visuomotor skill learning literature also rarely considers\nalternatives to schema representations [@chamberlinMemoryRepresentationMotor1992].\nAlthough schema-theory remains influential within certain literatures,\ninstance or exemplar-based models have accounted for human behavior\nacross myriad domains [@jamiesonInstanceTheoryDomaingeneral2022; @loganInstanceTheoryAttention2002a]. As mentioned above, instance based accounts have been shown to perform well on a\nvariety of different tasks with motoric components [@crumpEpisodicContributionsSequential2010;@gandolfoMotorLearningField1996a;  @meighWhatMemoryRepresentation2018; @rosenbaumPlanningReachesEvaluating1995; @vandamMappingShapeVisuomotor2015]. However, such accounts have received little\nattention within the subdomain of visuomotor skill learning focused on\nthe benefits of varied training.\n\nThe present work examines whether the commonly observed benefits of\nvaried training can be accounted for by between-group differences in\nsimilarity between training and testing throws. We first attempt to\nreplicate previous work finding an advantage of varied training over\nconstant training in a projectile launching task. We then examine the\nextent to which this advantage can be explained by an instance-based\nsimilarity model.\n\n## Experiment 1\n\n### Methods\n\n#### Sample Size Estimation\n\nTo obtain an independent estimate of effect size, we identified previous\ninvestigations which included between-subjects contrasts of varied and\nconstant conditions following training on an accuracy based projectile\nlaunching task [@chuaPracticeVariabilityPromotes2019; @goodwinEffectDifferentQuantities1998; @kerrSpecificVariedPractice1978; @wulfEffectTypePractice1991]. We then averaged effects across these studies,\nyielding a Cohens f =.43. The GPower 3.1 software package [@faulStatisticalPowerAnalyses2009],\n2009) was then used to determine that a power of 80% requires a sample\nsize of at least 23 participants per condition. All experiments reported\nin the present manuscript exceed this minimum number of participants per\ncondition.\n\n#### Participants\n\nParticipants were recruited from an undergraduate population that is 63%\nfemale and consists almost entirely of individuals aged 18-22 years. A\ntotal of 110 Indiana University psychology students participated in\nExperiment 1. We subsequently excluded 34 participants poor performance\nat one of the dependent measures of the task (2.5-3 standard deviations\nworse than the median subject at the task) or for displaying a pattern\nof responses that was clearly indicative of a lack of engagement with\nthe task (e.g. simply dropping the ball on each trial rather than\nthrowing it at the target), or for reporting that they completed the\nexperiment on a phone or tablet device, despite the instructions not to\nuse one of these devices. A total of 74 participants were retained for\nthe final analyses, 35 in the varied group and 39 in the constant group.\n\n#### Task\n\nThe experimental task was programmed in JavaScript, using packages from\nthe Phaser physics engine (https://phaser.io) and the jsPsych library\n(de Leeuw, 2015). The stimuli, presented on a black background,\nconsisted of a circular blue ball -- controlled by the participant via\nthe mouse or trackpad cursor; a rectangular green target; a red\nrectangular barrier located between the ball and the target; and an\norange square within which the participant could control the ball before\nreleasing it in a throw towards the target. Because the task was\nadministered online, the absolute distance between stimuli could vary\ndepending on the size of the computer monitor being used, but the\nrelative distance between the stimuli was held constant. Likewise, the\ndistance between the center of the target, and the training and testing\nlocations was scaled such that relative distances were preserved\nregardless of screen size. For the sake of brevity, subsequent mentions\nof this relative distance between stimuli, or the position where the\nball landed in relation to the center of the target, will be referred to\nsimply as distance. @fig-igasTask displays the layout of the task, as\nit would appear to a participant at the start of a trial, with the ball\nappearing in the center of the orange square. Using a mouse or trackpad,\nparticipants click down on the ball to take control of the ball,\nconnecting the movement of the ball to the movement of the cursor.\nParticipants can then \"wind up\" the ball by dragging it (within the\nconfines of the orange square) and then launch the ball by releasing the\ncursor. If the ball does not land on the target, participants are\npresented with feedback in red text at the top right of the screen, on\nhow many units away they were from the center of the target. If the ball\nwas thrown outside of the boundary of the screen participants are given\nfeedback as to how far away from the target center the ball would have\nbeen if it had continued its trajectory. If the ball strikes the barrier\n(from the side or by landing on top), feedback is presented telling\nparticipants to avoid hitting the barrier. If participants drag the ball\noutside of the orange square before releasing it, the trial terminates,\nand they are reminded to release the ball within the orange square. If\nthe ball lands on the target, feedback is presented in green text,\nconfirming that the target was hit, and presenting additional feedback\non how many units away the ball was from the exact center of the target.\n\n[Link to abbrevaited example of\ntask](https://pcl.sitehost.iu.edu/tg/demos/igas_expt1_demo.html){target=\"_blank\"}.\n\n![The stimuli of the task consisted of a blue ball, which the participants would launch at the green target, while avoiding the red barrier. On each trial, the ball would appear in the center of the orange square, with the position of the orange square varying between experimental conditions. Participants were constrained to release the ball within the square](Assets/methodsFig1.png){#fig-igasTask}\n\n\n\n\n### Results\n\n#### Data Processing and Statistical Packages\n\nTo prepare the data, we first removed trials that were not easily\ninterpretable as performance indicators in our task. Removed trials\nincluded: 1) those in which participants dragged the ball outside of the\norange starting box without releasing it, 2) trials in which\nparticipants clicked on the ball, and then immediately released it,\ncausing the ball to drop straight down, 3) outlier trials in which the\nball was thrown more than 2.5 standard deviations further than the\naverage throw (calculated separately for each throwing position), and 4)\ntrials in which the ball struck the barrier. The primary measure of\nperformance used in all analyses was the absolute distance away from the\ncenter of the target. The absolute distance was calculated on every\ntrial, and then averaged within each subject to yield a single\nperformance score, for each position. A consistent pattern across\ntraining and testing phases in both experiments was for participants to\nperform worse from throwing positions further away from the target -- a\npattern which we refer to as the difficulty of the positions. However,\nthere were no interactions between throwing position and training\nconditions, allowing us to collapse across positions in cases where\ncontrasts for specific positions were not of interest. All data\nprocessing and statistical analyses were performed in R version 4.03 (R\nCore Team, 2020). ANOVAs for group comparisons were performed using the\nrstatix package (Kassambara, 2021)****.\n\n#### Training Phase\n\n@fig-IGAS-Training1 below shows aggregate training performance binned into three\nstages representing the beginning, middle, and end of the training\nphase. Because the two conditions trained from target distances that\nwere not equally difficult, it was not possible to directly compare\nperformance between conditions in the training phase. Our focus for the\ntraining data analysis was instead to establish that participants did\nimprove their performance over the course of training, and to examine\nwhether there was any interaction between training stage and condition.\nDescriptive statistics for the intermittent testing phase are provided\nin the supplementary materials.\n\nWe performed an ANOVA comparison with stage as a within-group factor and\ncondition as between-group factor. The analysis revealed a significant\neffect of training stage F(2,142)=62.4, p\\<.001, $\\eta^{2}_G$ = .17,\nsuch that performance improved over the course of training There was no\nsignificant effect of condition F(1,71)=1.42, p=.24, $\\eta^{2}_G$ = .02,\nand no significant interaction between condition and training stage,\nF(2,142)=.10, p=.91, $\\eta^{2}_G$ \\< .01.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1TrainPosition <- e1 %>% filter(stage!=\"Transfer\",mode==1) %>%ungroup() %>% \n  group_by(sbjCode,Group,conditType,trainHalf,positionX) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1TrainPosition3 <- e1 %>% filter(stage!=\"Transfer\",mode==1) %>%ungroup() %>% \n  group_by(sbjCode,Group,conditType,stage,positionX) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train <- e1 %>% filter(stage!=\"Transfer\",mode==1)  %>%\n  group_by(sbjCode,Group,conditType,trainHalf) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\nexp1Train3 <- e1 %>% filter(stage!=\"Transfer\",mode==1)  %>%\n  group_by(sbjCode,Group,conditType,stage) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),.groups = 'keep')\n\n\ne1train2 <- exp1TrainPosition3 %>% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",fun=mean,position=dodge)+\n  facet_wrap(~conditType,ncol=2)+\n  stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  xlab(\"Training Location(s)\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)\n\n\n#plot_grid(title,e1train2,capt,ncol=1,rel_heights=c(.18,1,.15))\nplot_grid(e1train2,ncol=1)\n```\n\n::: {.cell-output-display}\n![Training performance for varied and constant participants binned into three stages. Shorter bars indicate better performance (ball landing closer to the center of the target). Error bars indicate standard error of the mean.](manuscript.markdown_strict_files/figure-markdown_strict/fig-IGAS-Training1-1.jpeg){#fig-IGAS-Training1}\n:::\n:::\n\n\n\n\n\n### Testing Phase\n\nIn Experiment 1, a single constant-trained group was compared against a\nsingle varied-trained group. At the transfer phase, all participants\nwere tested from 3 positions: 1) the positions(s) from their own\ntraining, 2) the training position(s) of the other group, and 3) a\nposition novel to both groups. Overall, group performance was compared\nwith a mixed type III ANOVA, with condition (varied vs. constant) as a\nbetween-subject factor and throwing location as a within-subject\nvariable. The effect of throwing position was strong, F(3,213) = 56.12,\np\\<.001, 2G = .23. The effect of training condition was significant\nF(1,71)=8.19, p\\<.01, 2G = .07. There was no significant interaction\nbetween group and position, F(3,213)=1.81, p=.15, 2G = .01.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp1.Test <- e1 %>% filter(stage==\"Transfer\") %>% select(-trainHalf)%>% group_by(positionX) %>% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n  group_by(sbjCode,positionX) %>% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%\n  ungroup() %>% group_by(sbjCode,conditType,positionX,ThrowPosition) %>%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%>% as.data.frame()\n\n#manuscript plot\ne1test1=exp1.Test %>% ggplot(aes(x=positionX,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",fun=mean,position=dodge)+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)+scale_x_discrete(name=\"Testing Location\",labels=e1Labels)\n\ne1test1\n```\n\n::: {.cell-output-display}\n![Testing performance for each of the 4 testing positions, compared between training conditions. Positions 610 and 910 were trained on by the varied group, and novel for the constant group. Position 760 was trained on by the constant group, and novel for the varied group. Position 835 was novel for both groups. Shorter bars are indicative of better performance (the ball landing closer to the center of the target). Error bars indicate standard error of the mean.](manuscript.markdown_strict_files/figure-markdown_strict/fig-IGAS-Testing1-1.jpeg){#fig-IGAS-Testing1}\n:::\n:::\n\n\n\n\n\n\\\n\\\n\n\n\n\n\n::: {#tbl-IGAS-Table1 .cell tab.cap='Testing performance for varied and constant groups in experiment 1. Mean absolute deviation from the center of the target, with standard deviations in parenthesis.'}\n\n```{.r .cell-code}\nexp1.Test <- e1 %>% filter(stage==\"Transfer\") %>% select(-trainHalf)%>% group_by(positionX) %>% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n  group_by(sbjCode,positionX) %>% \n  mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%\n  ungroup() %>% group_by(sbjCode,conditType,positionX,ThrowPosition) %>%\nsummarise(MeanTargetDeviance = mean(AbsDistFromCenter),MeanScaleDev = mean(scaledDev),.groups=\"keep\")%>% as.data.frame()\n\n\ntest= exp1.Test %>% dplyr::rename(Condition=\"conditType\") %>% group_by(Condition,positionX) %>%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n test=test %>% group_by(Condition) %>% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n test = test %>% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %>% select(positionX,Condition,msd,gsd)%>%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n test=test[,1:3]\n\nkable(test,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  \n```\n\n::: {.cell-output-display}\n\n\n|Position |Constant       |Varied         |\n|:--------|:--------------|:--------------|\n|610      |132.48(50.85)  |104.2(38.92)   |\n|760      |207.26(89.19)  |167.12(72.29)  |\n|835      |249.13(105.92) |197.22(109.71) |\n|910      |289.36(122.48) |212.86(113.93) |\n\n\n:::\n\n```{.r .cell-code}\n#%>% kableExtra::kable_styling(position=\"left\") \n# %>%   \n#   kable_classic() #%>% kableExtra::footnote(general=captionText,general_title = \"\")\n```\n:::\n\n\n\n\n\n### Discussion\n\nIn Experiment 1, we found that varied training resulted in superior\ntesting performance than constant training, from both a position novel\nto both groups, and from the position at which the constant group was\ntrained, which was novel to the varied condition. The superiority of\nvaried training over constant training even at the constant training\nposition is of particular note, given that testing at this position\nshould have been highly similar for participants in the constant\ncondition. It should also be noted, though, that testing at the constant\ntrained position is not exactly identical to training from that\nposition, given that the context of testing is different in several ways\nfrom that of training, such as the testing trials from the different\npositions being intermixed, as well as a simple change in context as a\nfunction of time. Such contextual differences will be further considered\nin the General Discussion.\n\nIn addition to the variation of throwing position during training, the\nparticipants in the varied condition of Experiment 1 also received\ntraining practice from the closest/easiest position, as well as from the\nfurthest/most difficult position that would later be encountered by all\nparticipants during testing. The varied condition also had the potential\nadvantage of interpolating both of the novel positions from which they\nwould later be tested. Experiment 2 thus sought to address these issues\nby comparing a varied condition to multiple constant conditions.\n\n## Experiment 2\n\nIn Experiment 2, we sought to replicate our findings from Experiment 1\nwith a new sample of participants, while also addressing the possibility\nof the pattern of results in Experiment 1 being explained by some\nidiosyncrasy of the particular training location of the constant group\nrelative to the varied group. To this end, Experiment 2 employed the\nsame basic procedure as Experiment 1, but was designed with six separate\nconstant groups each trained from one of six different locations (400,\n500, 625, 675, 800, or 900), and a varied group trained from two\nlocations (500 and 800). Participants in all seven groups were then\ntested from each of the 6 unique positions.\n\n## Methods\n\n### Participants\n\nA total of 306 Indiana University psychology students participated in\nExperiment 2, which was also conducted online. As was the case in\nexperiment 1, the undergraduate population from which we recruited\nparticipants was 63% female and primarily composed of 18--22-year-old\nindividuals. Using the same procedure as experiment 1, we excluded 98\nparticipants for exceptionally poor performance at one of the dependent\nmeasures of the task, or for displaying a pattern of responses\nindicative of a lack of engagement with the task. A total of 208\nparticipants were included in the final analyses with 31 in the varied\ngroup and 32, 28, 37, 25, 29, 26 participants in the constant groups\ntraining from location 400, 500, 625, 675, 800, and 900, respectively.\nAll participants were compensated with course credit.\n\n### Task and Procedure\n\nThe task of Experiment 2 was identical to that of Experiment 1, in all\nbut some minor adjustments to the height of the barrier, and the\nrelative distance between the barrier and the target. Additionally, the\nintermittent testing trials featured in experiment 1 were not utilized\nin experiment 2, and all training and testing trials were presented with\nfeedback. An abbreviated demo of the task used for Experiment 2 can be\nfound at (https://pcl.sitehost.iu.edu/tg/demos/igas_expt2_demo.html).\n\nThe procedure for Experiment 2 was also quite similar to experiment 1.\nParticipants completed 140 training trials, all of which were from the\nsame position for the constant groups and split evenly (70 trials each -\nrandomized) for the varied group. In the testing phase, participants\ncompleted 30 trials from each of the six locations that had been used\nseparately across each of the constant groups during training. Each of\nthe constant groups thus experience one trained location and five novel\nthrowing locations in the testing phase, while the varied group\nexperiences 2 previously trained, and 4 novel locations.\n\n## Results\n\n#### Data Processing and Statistical Packages\n\nAfter confirming that condition and throwing position did not have any\nsignificant interactions, we standardized performance within each\nposition, and then average across position to yield a single performance\nmeasure per participant. This standardization did not influence our\npattern of results. As in experiment 1, we performed type III ANOVA's\ndue to our unbalanced design, however the pattern of results presented\nbelow is not altered if type 1 or type III tests are used instead. The\nstatistical software for the primary analyses was the same as for\nexperiment 1. Individual learning rates in the testing phase, compared\nbetween groups in the supplementary analyses, were fit using the TEfit\npackage in R [@cochraneTEfitsNonlinearRegression2020].\n\n#### Training Phase\n\nThe different training conditions trained from positions that were not\nequivalently difficult and are thus not easily amenable to comparison.\nAs previously stated, the primary interest of the training data is\nconfirmation that some learning did occur. @fig-e2train depicts the training\nperformance of the varied group alongside that of the aggregate of the\nsix constant groups (5a), and each of the 6 separate constant groups\n(5b). An ANOVA comparison with training stage (beginning, middle, end)\nas a within-group factor and group (the varied condition vs. the 6\nconstant conditions collapsed together) as a between-subject factor\nrevealed no significant effect of group on training performance,\nF(1,206)=.55,p=.49, $\\eta^{2}_G$ \\<.01, a significant effect of training\nstage F(2,412)=77.91, p\\<.001, $\\eta^{2}_G$ =.05, and no significant\ninteraction between group and training stage, F(2,412)=.489 p=.61,\n$\\eta^{2}_G$ \\<.01. We also tested for a difference in training\nperformance between the varied group and the two constant groups that\ntrained matching throwing positions (i.e., the constant groups training\nfrom position 500, and position 800). The results of our ANOVA on this\nlimited dataset mirrors that of the full-group analysis, with no\nsignificant effect of group F(1,86)=.48, p=.49, $\\eta^{2}_G$ \\<.01, a\nsignificant effect of training stage F(2,172)=56.29, p\\<.001,\n$\\eta^{2}_G$ =.11, and no significant interaction between group and\ntraining stage, F(2,172)=.341 p=.71, $\\eta^{2}_G$ \\<.01.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne2$stage <- factor(e2$stage, levels = c(\"Beginning\", \"Middle\", \"End\",\"Transfer\"),ordered = TRUE)\n\nexp2TrainPosition <- e2  %>% filter(stage!=\"Transfer\") %>%ungroup() %>% \n  group_by(sbjCode,Group2,conditType,trainHalf,positionX) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter))%>% as.data.frame()\n\nexp2TrainPosition3 <- e2  %>% filter(stage!=\"Transfer\") %>%ungroup() %>% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n  group_by(sbjCode,positionX) %>% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%ungroup() %>%\n  group_by(sbjCode,Group2,conditType,stage,positionX) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05))%>% as.data.frame()\n\nexp2Train <- e2  %>% filter(stage!=\"Transfer\")  %>% \n  group_by(sbjCode,Group2,conditType,trainHalf) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter)) %>% as.data.frame()\n\nexp2Train3 <- e2  %>% filter(stage!=\"Transfer\")  %>% ungroup() %>% \n  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n  group_by(sbjCode,positionX) %>% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%ungroup() %>%\n  group_by(sbjCode,Group2,conditType,stage) %>% \n  summarise(MeanTargetDistance=mean(AbsDistFromCenter),MeanScaledDev=mean(scaledDev,trim=.05)) %>% as.data.frame()\n\ntransfer <- filter(e2, stage==\"Transfer\") %>% droplevels() %>% select(-trainHalf,-initialVelocityY,ThrowPosition2)%>% ungroup()\ntransfer <- transfer %>% group_by(positionX) %>% mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n  group_by(sbjCode,positionX) %>% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%ungroup()\n\ntransfer <- transfer %>% group_by(sbjCode,positionX) %>% mutate(ind=1,testPosIndex=cumsum(ind),posN=max(testPosIndex)) %>%\n  select(-ind) %>% mutate(testHalf = case_when(testPosIndex<15 ~\"1st Half\",testPosIndex>=15 ~\"2nd Half\")) %>% convert_as_factor(testHalf)\n\nvariedTest <- transfer %>% filter(condit==7) %>% mutate(extrapolate=ifelse(positionX==\"900\" | positionX==\"400\",\"extrapolation\",\"interpolation\")) \nconstantTest <- transfer %>% filter(condit!=7) %>% mutate(extrapolate=ifelse(distFromTrain==0,\"interpolation\",\"extrapolation\"))\n\ntransfer <- rbind(variedTest,constantTest)\ntransfer<- transfer %>% mutate(novel=ifelse(distFromTrain3==0,\"trainedLocation\",\"novelLocation\"))%>% convert_as_factor(novel,extrapolate)\n\ntransfer <- transfer %>% relocate(sbjCode,condit2,Group,conditType2,stage,trial,novel,extrapolate,positionX,AbsDistFromCenter,globalAvg,globalSd,scaledDev,distFromTrain3) %>% ungroup()\n\n\n# novelAll <- transfer %>% filter(distFromTrain!=0, distFromTrain3!=0) %>% select(-globalAvg,-globalSd,-scaledDev)%>% droplevels() %>% ungroup()\n# novelAll <- novelAll %>% group_by(positionX) %>%\n#  mutate(globalAvg=mean(AbsDistFromCenter),globalSd=sd(AbsDistFromCenter)) %>% \n#   group_by(sbjCode,positionX) %>% mutate(scaledDev = scaleVar(globalAvg,globalSd,AbsDistFromCenter)) %>%ungroup()\n\nnovelAll <- transfer %>% filter(distFromTrain!=0, distFromTrain3!=0)\nnovelAllMatched <- novelAll %>% filter(condit!=5,condit!=2)\n\n\nconstantIden <- transfer %>% filter(condit !=7,distFromTrain==0) # only constant groups from their training position\nvariedTest <- transfer %>% filter(condit==7) # only varied testing\nvariedVsIden <- rbind(constantIden,variedTest) # all varied combined with constant identity\n\n\nvariedNovel <- variedTest %>% filter(distFromTrain3 !=0) # removes 500 and 800 from varied\nconstantIden2 <- transfer %>% filter(condit !=7,condit!=5,condit!=2,distFromTrain==0) # only constant groups from training position 400,625,675,900\nvariedVsNovelIden <- rbind(constantIden2,variedNovel) # novel positions for varied, trained for constant\n\nexp2.Test <- transfer %>%group_by(sbjCode,conditType,positionX,ThrowPosition)%>%\n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %>%ungroup() %>% as.data.frame()\n\nexp2.Test2 <- exp2.Test %>% group_by(sbjCode,conditType)%>%\n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %>%ungroup() %>% as.data.frame()\n\nexp2.Test7 <- transfer %>%group_by(Group2,sbjCode,positionX,Group,conditType,ThrowPosition4) %>% \n  summarise(MeanTargetDeviance = mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05)) %>% as.data.frame()\n\nexp2.Test7.agg <- exp2.Test7  %>%group_by(Group2,sbjCode,Group,conditType) %>% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %>% as.data.frame()\n\nexp2.Test7.agg2 <- exp2.Test7  %>%group_by(sbjCode,conditType) %>% \n  summarise(MeanTargetDeviance = mean(MeanTargetDeviance),MeanScaledDev=mean(MeanScaledDev)) %>% as.data.frame()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### New - 3 stage\ne2train1<-exp2TrainPosition3 %>% ggplot(aes(x=stage,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=stage,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+facet_wrap(~conditType,ncol=2)+\n  ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Stage\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0.0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"A\")\n\ne2train2<-exp2TrainPosition3 %>% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=stage,fill=stage),stat=\"summary\",position=dodge,fun=\"mean\")+\n  facet_wrap(~conditType,ncol=2)+stat_summary(aes(x=positionX,group=stage),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Location(s)\")+\n  theme(plot.title = element_text(face=\"bold\",hjust = 0,size=9),\n        plot.title.position = \"plot\")+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)+ggtitle(\"B\")\n\n#plot_grid(e2train1,e2train2,ncol=1)\n\ne2train1 / e2train2\n```\n\n::: {.cell-output-display}\n![Training performance for the six constant conditions, and the varied condition, binned into three stages. On the left side, the six constant groups are averaged together, as are the two training positions for the varied group. On the right side, the six constant groups are shown separately, with each set of bars representing the beginning, middle, and end of training for a single constant group that trained from the position indicated on the x-axis. Figure 5b also shows training performance separately for both of the throwing locations trained by the varied group. Error bars indicate standard error of the mean.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2train-1.jpeg){#fig-e2train}\n:::\n:::\n\n\n\n\n\n\n\n\n#### Testing Phase\n\nIn Experiment 2, a single varied condition (trained from two positions,\n500 and 800), was compared against six separate constant groups (trained\nfrom a single position, 400, 500, 625, 675, 800 or 900). For the testing\nphase, all participants were tested from all six positions, four of\nwhich were novel for the varied condition, and five of which were novel\nfor each of the constant groups. For a general comparison, we took the\nabsolute deviations for each throwing position and computed standardized\nscores across all participants, and then averaged across throwing\nposition. The six constant groups were then collapsed together allowing\nus to make a simple comparison between training conditions (constant vs.\nvaried). A type III between-subjects ANOVA was performed, yielding a\nsignificant effect of condition F(1,206)=4.33, p=.039, $\\eta^{2}_G$\n=.02. Descriptive statistics for each condition are shown in table 2. In\n@fig-e2testa visualizes the consistent advantage of the varied condition\nover the constant groups across the testing positions. @fig-e2testa shows\nperformance between the varied condition and the individual constant\ngroups.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# manuscript plot\ne2test1<-exp2.Test %>% ggplot(aes(x=ThrowPosition,y=MeanTargetDeviance,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25)+\n  ggtitle(\"A\")\n\n\ne2test2<-exp2.Test7 %>% \n  ggplot(aes(x=Group,y=MeanTargetDeviance,group=conditType,fill=conditType))+\n  geom_bar(stat=\"summary\",position=position_dodge(),fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=position_dodge())+\n  facet_wrap(~ThrowPosition4)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  guides(fill=guide_legend(title=\"Training Condition\"))+\n  theme(plot.title=element_text(face=\"bold\",size=9),\n        plot.title.position = \"plot\",\n        legend.title.align=.25,\n        axis.text.x = element_text(size = 7,angle=45,hjust=1))+\n  scale_x_discrete(name=\" Training Group\",labels=e2Labels)+ggtitle(\"B\")\n\ne2test1 / e2test2\n```\n\n::: {.cell-output-display}\n![Testing phase performance from each of the six testing positions. The six constant conditions are averaged together into a single constant group, compared against the single varied-trained group.B) Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2testa-1.jpeg){#fig-e2testa}\n:::\n:::\n\n\n\n\n\n\\\n\n\n\n\n\n::: {#tbl-e2table1 .cell tbl-cap='Transfer performance from each of the 6 throwing locations from which all participants were tested. Each bar represents performance from one of seven distinct training groups (six constant groups in red, one varied group in blue). The x axis labels indicate the location(s) from which each group trained. Lower values along the y axis reflect better performance at the task (closer distance to target center). Error bars indicate standard error of the mean.'}\n\n```{.r .cell-code}\ntab2= exp2.Test %>% rename(Condition=\"conditType\") %>% group_by(Condition,positionX) %>%\n   summarise(Mean=round(mean(MeanTargetDeviance),2),sd=round(sd(MeanTargetDeviance),2),.groups=\"keep\")\n tab2=tab2 %>% group_by(Condition) %>% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n tab2 = tab2 %>% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %>% \n   select(positionX,Condition,msd,gsd)%>%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab2=tab2[,1:3]\n\n\nkable(tab2,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\")) \n```\n\n::: {.cell-output-display}\n\n\n|Position |Constant       |Varied         |\n|:--------|:--------------|:--------------|\n|400      |100.59(46.3)   |83.92(33.76)   |\n|500      |152.28(69.82)  |134.38(61.38)  |\n|625      |211.21(90.95)  |183.51(75.92)  |\n|675      |233.32(93.35)  |206.32(94.64)  |\n|800      |283.24(102.85) |242.65(89.73)  |\n|900      |343.51(114.33) |289.62(110.07) |\n\n\n:::\n\n```{.r .cell-code}\n#  %>% kableExtra::kable_styling(position=\"left\") %>% \n#   kable_classic() #%>% footnote(general=captionText,general_title = \"\")\n```\n:::\n\n\n\n\n\nNext, we compared the testing performance of constant and varied groups\nfrom only positions that participants had not encountered during\ntraining. Constant participants each had 5 novel positions, whereas\nvaried participants tested from 4 novel positions (400,625,675,900). We\nfirst standardized performance within in each position, and then\naveraged across positions. Here again, we found a significant effect of\ncondition (constant vs. varied): F(1,206)=4.30, p=.039, $\\eta^{2}_G$ =\n.02 .\n\n\n\n\n\n::: {#tbl-e2table2 .cell tbl-cap='Testing performance from novel positions. Includes data only from positions that were not encountered during the training stage (e.g. excludes positions 500 and 800 for the varied group, and one of the six locations for each of the constant groups). Table presents Mean absolute deviations from the center of the target, and standard deviations in parenthesis.'}\n\n```{.r .cell-code}\nsum.novelAll <- novelAll %>% group_by(sbjCode,conditType,positionX) %>% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %>% as.data.frame()\n\ntab3=sum.novelAll %>% rename(Condition=\"conditType\") %>% group_by(Condition,positionX) %>%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\n tab3=tab3 %>% group_by(Condition) %>% mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \n tab3 = tab3 %>% \n   mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %>% select(positionX,Condition,msd,gsd)%>%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab3=tab3[,1:3]\n\n\n\nkable(tab3,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\"))  \n```\n\n::: {.cell-output-display}\n\n\n|Position |Constant       |Varied         |\n|:--------|:--------------|:--------------|\n|400      |98.84(45.31)   |83.92(33.76)   |\n|500      |152.12(69.94)  |               |\n|625      |212.91(92.76)  |183.51(75.92)  |\n|675      |232.9(95.53)   |206.32(94.64)  |\n|800      |285.91(102.81) |               |\n|900      |346.96(111.35) |289.62(110.07) |\n\n\n:::\n\n```{.r .cell-code}\n# %>% kableExtra::kable_styling(position=\"left\") %>% \n#   kable_classic() #%>% footnote(general=captionText,general_title = \"\")\n```\n:::\n\n\n\n\n\nFinally, corresponding to the comparison of position 760 from experiment\n1, we compared the test performance of the varied group against the\nconstant group from only the positions that the constant groups trained.\nSuch positions were novel to the varied group (thus this analysis\nomitted two constant groups that trained from positions 500 or 800 as\nthose positions were not novel to the varied group). @fig-e2test1 displays\nthe particular subset of comparisons utilized for this analysis. Again,\nwe standardized performance within each position before performing the\nanalyses on the aggregated data. In this case, the effect of condition\ndid not reach statistical significance F(1,149)=3.14, p=.079,\n$\\eta^{2}_G$ = .02. Table 4 provides descriptive statistics.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum.variedVsNovelIden <- variedVsNovelIden  %>%\n  group_by(sbjCode,conditType,positionX) %>% \n  summarise(MeanTargetDev=mean(AbsDistFromCenter,trim=.05),MeanScaledDev=mean(scaledDev,trim=.05),.groups=\"keep\") %>% as.data.frame()\n\ne2Test2 <- sum.variedVsNovelIden %>% ggplot(aes(x=positionX,y=MeanTargetDev,group=conditType,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ylab(\"Mean Distance From Center Of Target\") +xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)\n\n\ne2Test2\n```\n\n::: {.cell-output-display}\n![A comparison of throwing location that are identical to those trained by the constant participants (e.g. constant participants trained at position 900, tested from position 900), which are also novel to the varied-trained participants (thus excluding positions 500 and 800). Error bars indicate standard error of the mean.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2test1-1.jpeg){#fig-e2test1}\n:::\n:::\n\n\n\n\n\n\\\n\n\n\n\n\n::: {#tbl-e2tab3 .cell tbl-cap='Testing performance from the locations trained by constant participants and novel to varied participants. Locations 500 and 800 are not included as these were trained by the varied participants. Table presents Mean absolute deviation from the center of the target, and standard deviations in parenthesis.'}\n\n```{.r .cell-code}\ntab4=sum.variedVsNovelIden %>% rename(Condition=\"conditType\") %>% group_by(Condition,positionX) %>%\n  summarise(Mean=round(mean(MeanTargetDev),2),sd=round(sd(MeanTargetDev),2),.groups=\"keep\")\n\ntab4=tab4 %>% group_by(Condition) %>% \n   mutate(GroupAvg=round(mean(Mean),2),groupSd=round(sd(Mean),2))\n \ntab4 = tab4 %>% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\"),gsd=paste(GroupAvg,\"(\",groupSd,\")\",sep=\"\")) %>% select(positionX,Condition,msd,gsd)%>%pivot_wider(names_from = Condition,values_from=c(msd,gsd))\n tab4=tab4[,1:3]\n\nkable(tab4,escape=FALSE,booktabs=TRUE,col.names=c(\"Position\",\"Constant\",\"Varied\"),align=c(\"l\")) \n```\n\n::: {.cell-output-display}\n\n\n|Position |Constant      |Varied         |\n|:--------|:-------------|:--------------|\n|400      |108.85(50.63) |83.92(33.76)   |\n|625      |204.75(84.66) |183.51(75.92)  |\n|675      |235.75(81.15) |206.32(94.64)  |\n|900      |323.5(130.9)  |289.62(110.07) |\n\n\n:::\n\n```{.r .cell-code}\n#  %>% kableExtra::kable_styling(position=\"left\") %>% \n#   kable_classic() #%>% footnote(general=captionText,general_title = \"\")\n```\n:::\n\n\n\n\n\n\n### Discussion\n\nThe results of experiment 2 largely conform to the findings of\nexperiment 1. Participants in both varied and constant conditions\nimproved at the task during the training phase. We did not observe the\ncommon finding of training under varied conditions producing worse\nperformance during acquisition than training under constant conditions [@catalanoDistantTransferCoincident1984a; @wrisbergVariabilityPracticeHypothesis1987], which has been\nsuggested to relate to the subsequent benefits of varied training in\nretention and generalization testing [@soderstromLearningPerformanceIntegrative2015]. However\nour finding of no difference in training performance between constant\nand varied groups has been observed in previous work [@chuaPracticeVariabilityPromotes2019; @moxleySchemaVariabilityPractice1979; @pigottMotorSchemaStructure1984].\n\nIn the testing phase, our varied group significantly outperformed the\nconstant conditions in both a general comparison, and in an analysis\nlimited to novel throwing positions. The observed benefit of varied over\nconstant training echoes the findings of many previous visuomotor skill\nlearning studies that have continued to emerge since the introduction of\nSchmidt's influential Schema Theory [@catalanoDistantTransferCoincident1984a; @chuaPracticeVariabilityPromotes2019; @goodwinEffectDifferentQuantities1998; @mccrackenTestSchemaTheory1977; @moxleySchemaVariabilityPractice1979; @newellVariabilityPracticeTransfer1976; @pigottMotorSchemaStructure1984; @rollerVariablePracticeLenses2001; @schmidtSchemaTheoryDiscrete1975; @willeyLongtermMotorLearning2018; @wrisbergVariabilityPracticeHypothesis1987; @wulfEffectTypePractice1991]. We also join a much smaller set of research to observe this\npattern in a computerized task [@seowTransferEffectsVaried2019]. One departure from\nthe experiment 1 findings concerns the pattern wherein the varied group\noutperformed the constant group even from the training position of the\nconstant group, which was significant in experiment 1, but did not reach\nsignificance in experiment 2. Although this pattern has been observed\nelsewhere in the literature [@goodeSuperiorityVariableRepeated2008; @kerrSpecificVariedPractice1978], \nthe overall evidence for this effect appears to be far weaker than for\nthe more general benefit of varied training in conditions novel to all\ntraining groups.\n\n### Computational Model\n\nControlling for the similarity between training and testing The primary\ngoal of Experiment 2 was to examine whether the benefits of variability\nwould persist after accounting for individual differences in the\nsimilarity between trained and tested throwing locations. To this end,\nwe modelled each throw as a two-dimensional point in the space of x and\ny velocities applied to the projectile at the moment of release. For\neach participant, we took each individual training throw, and computed\nthe similarity between that throw and the entire population of throws\nwithin the solution space for each of the 6 testing positions. We\ndefined the solution space empirically as the set of all combinations of\nx and y throw velocities that resulted in hitting the target. We then\nsummed each of the trial-level similarities to produce a single\nsimilarity for each testing position score relating how the participant\nthrew the ball during training and the solutions that would result in\ntarget hits from each of the six testing positions -- thus resulting in\nsix separate similarity scores for each participant. @fig-taskSpace1\nvisualizes the solution space for each location and illustrates how\ndifferent combinations of x and y velocity result in successfully\nstriking the target from different launching positions. As illustrated\nin @fig-taskSpace1, the solution throws represent just a small fraction of the\nentire space of velocity combinations used by participants throughout\nthe experiment.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaskspace <- e2 %>% filter(AbsDistFromCenter<900)\ntaskspace$hitOrMiss <- ifelse(taskspace$trialType==11,\"Hit Target\",\"Missed Target\")\n\nsolSpace <- e2 %>% filter(trialType==11)\n#solSpace %>% ggplot(aes(x=X_Velocity,y=Y_Velocity)) + geom_point(aes(colour=ThrowPosition),alpha=0.58) + ggtitle(\"\") \n\nsolSpace$Result = ifelse(solSpace$ThrowPosition==400,\"400\",solSpace$ThrowPosition)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==500,\"500\",solSpace$Result)\nsolSpace$Result= ifelse(solSpace$ThrowPosition==625,\"625\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==675,\"675\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==800,\"800\",solSpace$Result)\nsolSpace$Result = ifelse(solSpace$ThrowPosition==900,\"900\",solSpace$Result)\n\n\nmissSpace <- e2 %>% filter(trialType !=11)\nmissSpace$Result = \"Missed Target\"\nsolSpace$Result <- solSpace$Result\n\n# the usual method of changing the legend title does not seem to work after the colours are manually scaled. \n# multiplied velocoties by -1 to make the axes less confusing\nss=solSpace %>% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1)) + \n  geom_point(aes(colour=Result),alpha=0.6) + \n  scale_color_manual(values =brewer.pal(n=6,name=\"Set1\"))+\n  labs(colour=\"Target Hit Thrown from Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\")+ggtitle(\"A\")\n\nfullSpace <- rbind(missSpace,solSpace)\n\nfs<- fullSpace %>% ggplot(aes(x=X_Velocity*-1,y=Y_Velocity*-1,colour=Result)) + \n  geom_point(aes(),alpha=0.6) + scale_color_manual(values =brewer.pal(n=7,name=\"Set1\"))+\n  labs(colour=\"Target Hit or Miss From Position:\") + xlab(\"X Release Velocity\") + ylab(\"Y Release Velocity\") +ggtitle(\"B\")\n\n\nss / fs\n```\n\n::: {.cell-output-display}\n![A visual representation of the combinations of throw parameters (x and y velocities applied to the ball at launch), which resulted in target hits during the testing phase. This empirical solution space was compiled from all of the participants in experiment 2. Figure 8B shows the solution space within the context of all of the throws made throughout the testing phase of the experiment.](manuscript.markdown_strict_files/figure-markdown_strict/fig-taskSpace1-1.jpeg){#fig-taskSpace1}\n:::\n:::\n\n\n\n\n\n\nFor each individual trial, the Euclidean distance (Equation 1) was\ncomputed between the velocity components (x and y) of that trial and the\nvelocity components of each individual solution throw for each of the 6\npositions from which participants would be tested in the final phase of\nthe study. The P parameter in Equation 1 is set equal to 2, reflecting a\nGaussian similarity gradient. Then, as per an instance-based model of\nsimilarity [@loganInstanceTheoryAttention2002a; @nosofskySimilarityScalingCognitive1992], these distances were\nmultiplied by a sensitivity parameter, c, and then exponentiated to\nyield a similarity value. The parameter c controls the rate with which\nsimilarity-based generalization drops off as the Euclidean distance\nbetween two throws in x- and y-velocity space increases. If c has a\nlarge value, then even a small difference between two throws' velocities\ngreatly decreases the extent of generalization from one to the other. A\nsmall value for c produces broad generalization from one throw to\nanother despite relatively large differences in their velocities. The\nsimilarity values for each training individual throw made by a given\nparticipant were then summed to yield a final similarity score, with a\nseparate score computed for each of the 6 testing positions. The final\nsimilarity score is construable as index of how accurate the throws a\nparticipant made during the training phase would be for each of the\ntesting positions.\n\n**Equation 1:** $$ Similarity_{I,J} = \\sum_{i=I}\\sum_{j=J}\n(e^{-c^\\cdot d^{p}_{i,j}}) $$\n\n**Equation 2:**\n$$ d_{i,j} = \\sqrt{(x_{Train_i}-x_{Solution_j})^2 + (y_{Train_i}-y_{Solution_j})^2 } $$\n\nA simple linear regression revealed that these similarity scores were\nsignificantly predictive of performance in the transfer stage, t\n=-15.88, p\\<.01, $r^2$=.17, such that greater similarity between\ntraining throws and solution spaces for each of the test locations\nresulted in better performance. We then repeated the group comparisons\nabove while including similarity as a covariate in the model. Comparing\nthe varied and constant groups in testing performance from all testing\npositions yielded a significant effect of similarity, F(1, 205)=85.66,\np\\<.001, $\\eta^{2}_G$ =.29, and also a significant effect of condition\n(varied vs. constant), F(1, 205)=6.03, p=.015, $\\eta^{2}_G$ =.03. The\ngroup comparison limited to only novel locations for the varied group\npit against trained location for the constant group resulted in a\nsignificant effect of similarity, F(1,148)=31.12, p\\<.001, $\\eta^{2}_G$\n=.18 as well as for condition F(1,148)=11.55, p\\<.001, $\\eta^{2}_G$\n=.07. For all comparisons, the pattern of results was consistent with\nthe initial findings from experiment 2, with the varied group still\nperforming significantly better than the constant group.\n\n#### Fitting model parameters separately by group\n\nTo directly control for similarity in Experiment 2, we developed a\nmodel-based measure of the similarity between training throws and\ntesting conditions. This similarity measure was a significant predictor\nof testing performance, e.g., participants whose training throws were\nmore similar to throws that resulted in target hits from the testing\npositions, tended to perform better during the testing phase.\nImportantly, the similarity measure did not explain away the group-level\nbenefits of varied training, which remained significant in our linear\nmodel predicting testing performance after similarity was added to the\nmodel. However, previous research has suggested that participants may\ndiffer in their level of generalization as a function of prior\nexperience, and that such differences in generalization gradients can be\ncaptured by fitting the generalization parameter of an instance-based\nmodel separately to each group [@hahnEffectsCategoryDiversity2005; @lambertsFlexibleTuningSimilarity1994].\nRelatedly, the influential Bayesian generalization model developed by @tenenbaumGeneralizationSimilarityBayesian2001a predicts that the breadth of generalization\nwill increase when a rational agent encounters a wider variety of\nexamples. Following these leads, we assume that in addition to learning\nthe task itself, participants are also adjusting how generalizable their\nexperience should be. Varied versus constant participants may be\nexpected to learn to generalize their experience to different degrees.\nTo accommodate this difference, the generalization parameter of the\ninstance-based model (in the present case, the c parameter) can be\nallowed to vary between the two groups to reflect the tendency of\nlearners to adaptively tune the extent of their generalization. One\nspecific hypothesis is that people adaptively set a value of c to fit\nthe variability of their training experience [@nosofskyExemplarbasedAccountsMultiplesystem2000; @sakamotoTrackingVariabilityLearning2006]. If one's training experience is relatively\nvariable, as with the variable training condition, then one might infer\nthat future test situations will also be variable, in which case a low\nvalue of c will allow better generalization because generalization will\ndrop off slowly with training-to-testing distance. Conversely, if one's\ntraining experience has little variability, as found in the constant\ntraining conditions, then one might adopt a high value of c so that\ngeneralization falls off rapidly away from the trained positions.\n\nTo address this possibility, we compared the original instance-based\nmodel of similarity fit against a modified model which separately fits\nthe generalization parameter, c, to varied and constant participants. To\nperform this parameter fitting, we used the optim function in R, and fit\nthe model to find the c value(s) that maximized the correlation between\nsimilarity and testing performance.\n\nBoth models generate distinct similarity values between training and\ntesting locations. Much like the analyses in Experiment 2, these\nsimilarity values are regressed against testing performance in models of\nthe form shown below. As was the case previously, testing performance is\ndefined as the mean absolute distance from the center of the target\n(with a separate score for each participant, from each position).\n\nLinear models 1 and 3 both show that similarity is a significant\npredictor of testing performance (p\\<.01). Of greater interest is the\ndifference between linear model 2, in which similarity is computed from\na single c value fit from all participants (Similarity1c), with linear\nmodel 4, which fits the c parameter separately between groups\n(Similarity2c). In linear model 2, the effect of training group remains\nsignificant when controlling for Similarity1c (p\\<.01), with the varied\ngroup still performing significantly better. However, in linear model 4\nthe addition of the Similarity2c predictor results in the effect of\ntraining group becoming nonsignificant (p=.40), suggesting that the\neffect of varied vs. constant training is accounted for by the\nSimilarity2c predictor. Next, to further establish a difference between\nthe models, we performed nested model comparisons using ANOVA, to see if\nthe addition of the training group parameter led to a significant\nimprovement in model performance. In the first comparison, ANOVA(Linear\nModel 1, Linear Model 2), the addition of the training group predictor\nsignificantly improved the performance of the model (F=22.07, p\\<.01).\nHowever, in the second model comparison, ANOVA (Linear model 3, Linear\nModel 4) found no improvement in model performance with the addition of\nthe training group predictor (F=1.61, p=.20).\n\nFinally, we sought to confirm that similarity values generated from the\nadjusted Similarity2c model had more predictive power than those\ngenerated from the original Similarity1c model. Using the BIC function\nin R, we compared BIC values between linear model 1 (BIC=14604.00) and\nlinear model 3 (BIC = 14587.64). The lower BIC value of model 3 suggests\na modest advantage for predicting performance using a similarity measure\ncomputed with two c values over similarity computed with a single c\nvalue. When fit with separate c values, the best fitting c parameters\nfor the model consistently optimized such that the c value for the\nvaried group (c=.00008) was smaller in magnitude than the c value for\nthe constant group(c= .00011). Recall that similarity decreases as a\nGaussian function of distance (equation 1 above), and a smaller value of\nc will result in a more gradual drop-off in similarity as the distance\nbetween training throws and testing solutions increases.\n\nIn summary, our modeling suggests that an instance-based model which\nassumes equivalent generalization gradients between constant and varied\ntrained participants is unable to account for the extent of benefits of\nvaried over constant training observed at testing. The evidence for this\nin the comparative model fits is that when a varied/constant dummy-coded\nvariable for condition is explicitly added to the model, the variable\nadds a significant contribution to the prediction of test performance,\nwith the variable condition yielding better performance than the\nconstant conditions. However, if the instance-based generalization model\nis modified to assume that the training groups can differ in the\nsteepness of their generalization gradient, by incorporating a separate\ngeneralization parameter for each group, then the instance-based model\ncan account for our experimental results without explicitly taking\ntraining group into account. Henceforth this model will be referred to\nas the Instance-based Generalization with Adaptive Similarity (IGAS)\nmodel.\n\n### General Discussion\n\nAcross two experiments, we found evidence in support of the benefits of\nvariability hypothesis in a simple, computerized projectile throwing\ntask. Generalization was observed in both constant and varied\nparticipants, in that both groups tended to perform better at novel\npositions in the testing phase than did participants who started with\nthose positions in the training phase. However, varied trained\nparticipants consistently performed better than constant trained\nparticipants, in terms of both the testing phase in general, and in a\ncomparison that only included untrained positions. We also found some\nevidence for the less commonly observed pattern wherein varied-trained\nparticipants outperform constant-trained participants even from\nconditions identical to the constant group training [@goodeSuperiorityVariableRepeated2008; @greenPracticeVariabilityTransfer1995a; @kerrSpecificVariedPractice1978]. In experiment 1 varied\nparticipants performed significantly better on this identity comparison.\nIn Experiment 2, the comparison was not significant initially, but\nbecame significant after controlling for the similarity measure that\nincorporates only a single value for the steepness of similarity-based\ngeneralization (c). Furthermore, we showed that the general pattern of\nresults from Experiment 2 could be parsimoniously accommodated by an\ninstance-based similarity model, but only with the assumption that\nconstant and varied participants generalize their training experience to\ndifferent degrees. Our results thus suggest that the benefits of\nvariation cannot be explained by the varied-trained participants simply\ncovering a broader range of the task space. Rather, the modeling\nsuggests that varied participants also learn to adaptively tune their\ngeneralization function such that throwing locations generalize more\nbroadly to one another than they do in the constant condition. A\nlearning system could end up adopting a higher c value in the constant\nthan variable training conditions by monitoring the trial-by-trial\nvariability of the training items. The c parameter would be adapted\ndownwards when adjacent training items are dissimilar to each other and\nadapted upwards when adjacent training items are the same. In this\nfashion, contextually appropriate c values could be empirically learned.\nThis learning procedure would capture the insight that if a situation\nhas a high amount variability, then the learner should be predisposed\ntoward thinking that subsequent test items will also show considerable\nvariability, in which case generalization gradients should be broad, as\nis achieved by low values for c.\n\nAlso of interest is whether the IGAS model can predict the pattern of\nresults wherein the varied condition outperforms the constant condition\neven from the position on which the constant condition trained. Although\nour models were fit using all of the Experiment 2 training and testing\ndata, not just that of the identity comparisons, in @fig-Toy-Model2 we\ndemonstrate how a simplified version of the IGAS model could in\nprinciple produce such a pattern. In addition to the assumption of\ndifferential generalization between varied and constant conditions, our\nsimplified model makes explicit an assumption that is incorporated into\nthe full IGAS model -- namely that even when being tested from a\nposition identical to that which was trained, there are always some\npsychological contextual differences between training and testing\nthrows, resulting in a non-zero dissimilarity.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \n\np=2\nc<- .000002\n\ntrainingTestingDifference=2000;\ncvaried=.00002\ncconstant=.0005\nsimdat <- data.frame(x=rep(seq(200,1000),3),condit=c(rep(\"varied\",1602),rep(\"constant\",801)),\n                     train.position=c(rep(400,801),rep(800,801),rep(600,801)),c=.0002,p=2) %>%\n                     mutate(c2=ifelse(condit==\"varied\",cvaried,cconstant),\n                            genGauss=exp(-c*(abs((x-train.position)^p))),\n                            genGaussDist=exp(-c*(trainingTestingDifference+abs((x-train.position)^p))),\n                            genGauss2=exp(-c2*(abs((x-train.position)^p))),\n                            genGaussDist2=exp(-c2*(trainingTestingDifference+abs((x-train.position)^p))),\n                            ) %>% \n  group_by(x,condit) %>%\n  summarise(genGauss=mean(genGauss),genGauss2=mean(genGauss2),genGaussDist=mean(genGaussDist),genGaussDist2=mean(genGaussDist2),.groups='keep')\n\n\n#plot(x,exp(c*(trainingTestingDifference+abs(x-800)))+exp(c*(trainingTestingDifference+abs(x-400)))\n\ncolorVec=c(\"darkblue\",\"darkred\")\nplotSpecs <- list(geom_line(alpha=.7),scale_color_manual(values=colorVec),\n                  geom_vline(alpha=.55,xintercept = c(400,800),color=colorVec[2]),\n                  geom_vline(alpha=.55,xintercept = c(600),color=colorVec[1]),\n                  ylim(c(0,1.05)),\n                  #xlim(c(250,950)),\n                  scale_x_continuous(breaks=seq(200,1000,by=200)),\n                  xlab(\"Test Stimulus\"),\n                  annotate(geom=\"text\",x=455,y=1.05,label=\"Varied\",size=3.0),\n                  annotate(geom=\"text\",x=455,y=.97,label=\"Training\",size=3.0),\n                  annotate(geom=\"text\",x=662,y=1.05,label=\"Constant\",size=3.0),\n                  annotate(geom=\"text\",x=657,y=.97,label=\"Training\",size=3.0),\n                  annotate(geom=\"text\",x=855,y=1.05,label=\"Varied\",size=3.0),\n                  annotate(geom=\"text\",x=855,y=.97,label=\"Training\",size=3.0),\n                  theme(panel.border = element_rect(colour = \"black\", fill=NA, size=1),\n                        legend.position=\"none\"))\n\nip1 <- simdat  %>% ggplot(aes(x,y=genGauss,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")+ggtitle(\"Identical context, 1c\")\nip2 <- simdat %>%  ggplot(aes(x,y=genGauss2,group=condit,col=condit))+plotSpecs+ylab(\"\")+ggtitle(\"Identical context, 2c\")\nip3 <- simdat  %>% ggplot(aes(x,y=genGaussDist,group=condit,col=condit))+plotSpecs+ylab(\"Amount of Generalization\")+\n  ggtitle(\"Added distance due to context, 1c\")+theme(plot.margin = margin(0, 0, 0, 1))\nip4 <- simdat %>%  ggplot(aes(x,y=genGaussDist2,group=condit,col=condit))+plotSpecs+ylab(\"\")+\n  ggtitle(\"Added distance due to context, 2c\")+theme(plot.margin = margin(0, 0, 0, 1))\n# gridExtra::grid.arrange(ip1,ip2,ip3,ip4,ncol=2)\n\ngtitle=\"Figure 9.\"\ntitle = ggdraw()+draw_label(gtitle,fontface = 'bold',x=0,hjust=0,size=11)+theme(plot.margin = margin(0, 0, 0, 1))\nplot_grid(title,NULL,ip1,ip2,ip3,ip4,ncol=2,rel_heights=c(.1,.8,.8))\n```\n\n::: {.cell-output-display}\n![A simple model depicting the necessity of both of two separately fit generalization parameters, c, and a positive distance between training and testing contexts, in order for an instance model to predict a pattern of varied training from stimuli 400 and 800 outperforming constant training from position 600 at a test position of 600. For the top left panel, in which the generalization model assumes a single c value (-.008) for  both varied and constant conditions, and identical contexts across training and testing, the equation which generates the varied condition is - Amount of Generalization = $e^{(c\\cdot|x-800|)} + e^{(c\\cdot|x-400|)}$, whereas the constant group generalization is generated from $2\\cdot e^{(c\\cdot|x-600|)}$. For the top right panel, the c constants in the original equations are different for the 2 conditions, with $c=-.002$ for the varied condition, and $c=-.008$ for the constant condition. The bottom two panels are generated from identical equations to those immediately above, except for the addition of extra distance (100 units) to reflect the assumption of some change in context between training and testing conditions. Thus, the generalization model for the varied condition in the bottom-right panel is of the form - Amount of Generalization = $e^{(c_{varied}\\cdot|x-800|)}+e^{(c_{varied}\\cdot|x-400|)}$ .](manuscript.markdown_strict_files/figure-markdown_strict/fig-Toy-Model2-1.jpeg){#fig-Toy-Model2}\n:::\n:::\n\n\n\n\n\n\nAs mentioned above, the idea that learners flexibly adjust their\ngeneralization gradient based on prior experience does have precedent in\nthe domains of category learning [@ahaConceptLearningFlexible1992; @briscoeConceptualComplexityBias2011; @hahnEffectsCategoryDiversity2005; @lambertsFlexibleTuningSimilarity1994; @opdebeeckRepresentationPerceivedShape2008], and sensorimotor adaptation [@marongelliAdvantageFlexibleNeuronal2013; @taylorContextdependentGeneralization2013; @thoroughmanRapidReshapingHuman2005]. @lambertsFlexibleTuningSimilarity1994 showed\nthat a simple manipulation of background knowledge during a\ncategorization test resulted in participants generalizing their training\nexperience more or less broadly, and moreover that such a pattern could\nbe captured by allowing the generalization parameter of an\ninstance-based similarity model to be fit separately between conditions.\nThe flexible generalization parameter has also successfully accounted\nfor generalization behavior in cases where participants have been\ntrained on categories that differ in their relative variability [@hahnEffectsCategoryDiversity2005; @sakamotoTrackingVariabilityLearning2006]. However, to the best of our\nknowledge, IGAS is the first instance-based similarity model that has\nbeen put forward to account for the effect of varied training in a\nvisuomotor skill task. Although IGAS was inspired by work in the domain\nof category learning, its success in a distinct domain may not be\nsurprising in light of the numerous prior observations that at least\ncertain aspects of learning and generalization may operate under common\nprinciples across different tasks and domains [@censorCommonMechanismsHuman2012; @hillsCentralExecutiveSearch2010; @jamiesonInstanceTheoryDomaingeneral2022; @lawSharedMechanismsPerceptual2010; @roarkComparingPerceptualCategory2021; @rosenbaumAcquisitionIntellectualPerceptualMotor2001; @vigoLearningDifficultyVisual2018; @wallIdentifyingRelationshipsCognitive2021; @wuSimilaritiesDifferencesSpatial2020; @yangGeneralLearningAbility2020].\n\n\nOur modelling approach does differ from category learning\nimplementations of instance-based models in several ways. One such\ndifference is the nature of the training instances that are assumed to\nbe stored. In category learning studies, instances are represented as\npoints in a multidimensional space of all of the attributes that define\na category item (e.g. size/color/shape). Rather than defining instances\nin terms of what stimuli learners experience, our approach assumes that\nstored, motor instances reflect how they act, in terms of the velocity\napplied to the ball on each throw. An advantage of many motor learning\ntasks is the relative ease with which task execution variables can be\ndirectly measured (e.g. movement force, velocity, angle, posture) in\naddition to the decision and response time measures that typically\nexhaust the data generated from more classical cognitive tasks. Of\ncourse, whether learners actually are storing each individual motor\ninstance is a fundamental question beyond the scope of the current work\n-- though as described in the introduction there is some evidence in\nsupport of this idea [@chamberlinNoteSchemaExemplar1992; @crumpEpisodicContributionsSequential2010; @hommelEventFilesEvidence1998; @meighWhatMemoryRepresentation2018; @poldrackRelationshipSkillLearning1999]. A particularly\nnoteworthy instance-based model of sensory-motor behavior is the\nKnowledge II model of Rosenbaum and colleagues [@cohenWhereGraspsAre2004; @rosenbaumPlanningReachesEvaluating1995]. Knowledge II explicitly defines instances as\npostures (joint combinations), and is thus far more detailed than IGAS\nin regards to the contents of stored instances. Knowledge II also\ndiffers from IGAS in that learning is accounted for by both the\nretrieval of stored postures, and the generation of novel postures via\nthe modification of retrieved postures. A promising avenue for future\nresearch would be to combine the adaptive similarity mechanism of IGAS\nwith the novel instance generation mechanisms of Knowledge II.\n\nOur findings also have some conceptual overlap with an earlier study on\nthe effects of varied training in a coincident timing task [@catalanoDistantTransferCoincident1984a].  In this task, participants observe a series of lamps\nlighting up consecutively, and attempt to time a button press with the\nonset of the final lamp. The design consisted of four separate constant\ngroups, each training from a single lighting velocity, and a single\nvaried group training with all four of the lighting velocities used by\nthe individual constant groups. Participants were then split into four\nseparate testing conditions, each of which were tested from a single\nnovel lighting velocity of varying distance from the training\nconditions. The result of primary interest was that all participants\nperformed worse as the distance between training and testing velocity\nincreased -- a typical generalization decrement. However, varied\nparticipants showed less of a decrement than did constant participants.\nThe authors take this result as evidence that varied training results in\na less-steep generalization gradient than does constant training.\nAlthough the experimental conclusions of Catalano and Kleiner are\nsimilar to our own, our work is novel in that we account for our results\nwith a cognitive model, and without assuming the formation of a schema.\nAdditionally, the way in which Catalano and Kleiner collapse their\nseparate constant groups together may result in similarity confounds\nbetween varied and constant conditions that leaves their study open to\nmethodological criticisms, especially in light of related work which\ndemonstrated that the extent to which varied training may be beneficial\ncan depend on whether the constant group they are compared against\ntrained from similar conditions to those later tested [@wrisbergVariabilityPracticeHypothesis1987]. Our study alleviates such concerns by explicitly controlling for\nsimilarity.\n\n## Limitations\n\nA limitation of this study concerns the ordering of the testing/transfer\ntrials at the conclusion of both experiments. Participants were tested\nfrom each separate position (4 in Experiment 1, 6 in Experiment 2) in a\nrandom, intermixed order. Because the varied group was trained from two\npositions that were also randomly ordered, they may have benefited from\nexperience with this type of sequencing, whereas the constant groups had\nno experience with switching between positions trial to trial. This\nconcern is somewhat ameliorated by the fact that the testing phase\nperformance of the constant groups from their trained position was not\nsignificantly worse than their level of performance at the end of the\ntraining phase, suggesting that they were not harmed by random ordering\nof positions during testing. It should also be noted that the\ncomputerized task utilized in the present work is relatively simple\ncompared to many of the real-world tasks utilized in prior research. It\nis thus conceivable that the effect of variability in more complex tasks\nis distinct from the process put forward in the present work. An\nimportant challenge for future work will be to assess the extent to\nwhich IGAS can account for generalization in relatively complex tasks\nwith far more degrees of freedom.\n\nIt is common for psychological process models of categorization learning\nto use an approach such as multidimensional scaling so as to transform\nthe stimuli from the physical dimensions used in the particular task\ninto the psychological dimensions more reflective of the actual human\nrepresentations [@nosofskySimilarityScalingCognitive1992; @shepardUniversalLawGeneralization1987]. Such scaling typically entails having participants rate the similarity between individual items\nand using these similarity judgements to then compute the psychological\ndistances between stimuli, which can then be fed into a subsequent\nmodel. In the present investigation, there was no such way to scale the\nx and y velocity components in terms of the psychological similarity,\nand thus our modelling does rely on the assumption that the\npsychological distances between the different throwing positions are\nproportional to absolute distances in the metric space of the task (e.g.\nthe relative distance between positions 400 and 500 is equivalent to\nthat between 800 and 900). However, an advantage of our approach is that\nwe are measuring similarity in terms of how participants behave\n(applying a velocity to the ball), rather than the metric features of\nthe task stimuli.\n\n## Conclusion\n\nOur experiments demonstrate a reliable benefit of varied training in a\nsimple projectile launching task. Such results were accounted for by an\ninstance-based model that assumes that varied training results in the\ncomputation of a broader similarity-based generalization gradient.\nInstance-based models augmented with this assumption may be a valuable\napproach towards better understanding skill generalization and transfer.\n\n\n\n\n\n# Project 2\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- readRDS(here::here(\"data/e1_08-21-23.rds\")) |> filter(expMode2 == \"Test\") \n#options(brms.backend=\"cmdstanr\",mc.cores=4)\ne1Sbjs <- test |> group_by(id,condit) |> summarise(n=n())\ntestAvg <- test %>% group_by(id, condit, vb, bandInt,bandType,tOrder) %>%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n```\n:::\n\n\n\n\n\n\n\n# Introduction\n\nIn project 1, I applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, we will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. Project 2 also places a greater emphasis on extrapolation performance following training. Although varied training has often been purported to be particularly beneficial for generalization or transfer, few experiments have compared varied and constant training in contexts with unambiguous extrapolation testing. \n\n## Function Learning and Extrapolation\n\nThe study of human function learning investigates how people learn relationships between continuous input and output values.  Function learning is studied both in tasks where individuals are exposed to a sequence of input/output pairs [@deloshExtrapolationSineQua1997; @mcdanielEffectsSpacedMassed2013], or situations where observers are presented with a an incomplete scatterplot or line graph and make predictions about regions of the plot that don't contain data [@ciccioneCanHumansPerform2021a; @courrieuQuickApproximationBivariate2012; @saidExtrapolationAccuracyUnderestimates2021;@schulzCommunicatingCompositionalPatterns2020].\n\n@carrollFunctionalLearningLearning1963 conducted the earliest work on function learning. Input stimuli and output responses were both lines of varying length. The correct output response was related to the length of the input line by a linear, quadratic, or random function.  Participants in the linear and quadratic performed above chance levels during extrapolation testing, with those in the linear condition performing the best overall. Carroll argued that these results were best explained by a ruled based model wherein learners form an abstract representation of the underlying function. Subsequent work by @brehmerHypothesesRelationsScaled1974,testing a wider array of functional forms, provided further evidence for superior extrapolation in tasks with linear functions. Brehmer argued that individuals start out with an assumption of a linear function, but  given sufficient error will progressively test alternative hypothesis with polynomials of greater degree. @kohFunctionLearningInduction1991 employed a visuomotor function learning task, wherein participants were trained on examples from an unknown function relating the length of an input line to the duration of a response (time between keystrokes). In this domain, participants performed best when the relation between line length and response duration was determined by a power, as opposed to linear function. Koh & Meyer developed the log-polynomial adaptive-regression model to account for their results. \n\nThe first significant challenge to the rule-based accounts of function learning was put forth by @deloshExtrapolationSineQua1997 .  In their task, participants learned to associate stimulus magnitudes with response magnitudes that were related via either linear, exponential, or quadratic function. Participants approached ceiling performance by the end of training in each function condition, and were able to correctly respond in interpolation testing trials. All three conditions demonstrated some capacity for extrapolation, however participants in the linear condition tended to underestimate the true function, while exponential and quadratic participants reliably overestimated the true function on extrapolation trials. Extrapolation and interpolation performance are depicted in @fig-delosh-extrap.\n\nThe authors evaluated both of the rule-based models introduced in earlier research (with some modifications enabling trial-by-trial learning). The polynomial hypothesis testing model [@carrollFunctionalLearningLearning1963; @brehmerHypothesesRelationsScaled1974] tended to mimic the true function closely in extrapolation, and thus offered a poor account of the human data.  The log-polynomial adaptive regression model [@kohFunctionLearningInduction1991] was able to mimic some of the  systematic deviations produced by human subjects, but also predicted overestimation in cases where underestimation occurred. \n\nThe authors also introduced two new function-learning models. The Associative Learning Model (ALM) and the extrapolation-association model (EXAM). ALM is a two layer connectionist model adapted from the ALCOVE model in the category learning literature [@kruschkeALCOVEExemplarbasedConnectionist1992]. ALM belongs to the general class of radial-basis function neural networks, and can be considered a similarity-based model in the sense that the nodes in the input layer of the network are activated as a function of distance. The EXAM model retains the same similarity based activation and associative learning mechanisms as ALM, while being augmented with a linear rule response mechanism. When presented with novel  stimuli, EXAM will retrieve the most similar input-output examples encountered during training, and from those examples compute a local slope. ALM was able to provide a good account of participant training and interpolation data in all three function conditions, however it was unable to extrapolate. EXAM, on the other hand, was able to reproduce both the extrapolation underestimation, as well as the quadratic and exponential overestimation patterns exhibited by the human participants. Subsequent research identified some limitations in EXAM's ability to account for cases where human participants learn and extrapolate sinusoidal function @bottNonmonotonicExtrapolationFunction2004 or to scenarios where different functions apply to different regions of the input space @kalishPopulationLinearExperts2004, though EXAM has been shown to provide a good account of human learning and extrapolation in tasks with bi-linear, V shaped input spaces @mcdanielPredictingTransferPerformance2009.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"Functions\",\"deLosh_data.R\"))\n\nextrapLines <- list(geom_vline(xintercept=30,color=\"black\",alpha=.4,linetype=\"dashed\"),\n                    geom_vline(xintercept=70,color=\"black\",alpha=.4,linetype=\"dashed\"))\n  \n\nlinear_function <- function(x) 2.2 * x + 30\nexponential_function <- function(x) 200 * (1 - exp(-x/25))\nquadratic_function <- function(x) 210 - (x - 50)^2 / 12\n\nlinear_plot <- ggplot(deLosh_data$human_data_linear, aes(x, y)) +\n    geom_point(shape=1) + stat_function(fun = linear_function, color = \"black\") +\n  labs(y=\"Response Magnitude\", title=\"Linear Function\",x=\"\") + extrapLines\n\nexponential_plot <- ggplot(deLosh_data$human_data_exp, aes(x, y)) +\n  geom_point(aes(shape = \"Observed\", color = \"Observed\"),shape=1) + \n  stat_function(aes(color = \"True Function\"),fun = exponential_function, geom=\"line\")+\n  labs(x=\"Stimulus Magnitude\", title=\"Exponential Function\",y=\"\")  +\n  extrapLines +\n  scale_shape_manual(values = c(1)) +\n  scale_color_manual(values = c(\"Observed\" = \"black\", \"True Function\" = \"black\")) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +\n  guides(color = guide_legend(override.aes = list(shape = c(1, NA), \n                                                  linetype = c(0, 1))))\n\nquadratic_plot <- ggplot(deLosh_data$human_data_quad, aes(x = x, y = y)) +\n  geom_point( shape = 1) +\n  stat_function( fun = quadratic_function, geom = \"line\") +\n  labs(title=\"Quadratic Function\",x=\"\",y=\"\") + extrapLines\n\nlinear_plot + exponential_plot + quadratic_plot\n```\n\n::: {.cell-output-display}\n![Generalization reproduced patterns from DeLosh et al. (1997) Figure 3. Stimulii that fall within the dashed lines are interpolations of the training examples.](manuscript.markdown_strict_files/figure-markdown_strict/fig-delosh-extrap-1.jpeg){#fig-delosh-extrap}\n:::\n:::\n\n\n\n\n\n\n\n# Methods\n\n## Participants\n\nData was collected from 647 participants (after exclusions). The results shown below consider data from subjects in our initial experiment, which consisted of 196 participants (106 constant, 90 varied). The follow-up experiments entailed minor manipulations: 1) reversing the velocity bands that were trained on vs. novel during testing; 2) providing ordinal rather than numerical feedback during training (e.g. correct, too low, too high). The data from these subsequent experiments are largely consistently with our initial results shown below.\n\n## Task\n\nWe developed a novel visuomotor extrapolation task, termed the Hit The Wall task, wherein participants learned to launch a projectile such that it hit a rectangle at the far end of the screen with an appropriate amount of force. Although the projectile had both x and y velocity components, only the x-dimension was relevant for the task. [Link to task demo](https://pcl.sitehost.iu.edu/tg/HTW/HTW_Index.html?sonaid=){target=\"_blank\"}\n\n## Procedure\n\nThe HTW task involved launching projectiles to hit a target displayed on the computer screen. Participants completed a total of 90 trials during the training stage. In the varied training condition, participants encountered three velocity bands (800-1000, 1000-1200, and 1200-1400). In contrast, participants in the constant training condition encountered only one velocity band (800-1000).\n\nDuring the training stage, participants in both conditions also completed \"no feedback\" trials, where they received no information about their performance. These trials were randomly interleaved with the regular training trials.\n\nFollowing the training stage, participants proceeded to the testing stage, which consisted of three phases. In the first phase, participants completed \"no-feedback\" testing from three novel extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 15 trials.\n\nIn the second phase of testing, participants completed \"no-feedback\" testing from the three velocity bands used during the training stage (800-1000, 1000-1200, and 1200-1400). In the constant training condition, two of these bands were novel, while in the varied training condition, all three bands were encountered during training.\n\nThe third and final phase of testing involved \"feedback\" testing for each of the three extrapolation bands (100-300, 350-550, and 600-800), with each band consisting of 10 trials. Participants received feedback on their performance during this phase.\n\nThroughout the experiment, participants' performance was measured by calculating the distance between the produced x-velocity of the projectiles and the closest edge of the current velocity band. Lower distances indicated better performance.\n\nAfter completing the experiment, participants were debriefed and provided with an opportunity to ask questions about the study.\n\n\n\n\n\n```{dot}\n//| label: fig-design-e1\n//| fig-cap: \"Experiment 1 Design. Constant and Varied participants complete different training conditions.\"\n//| fig-width: 6.0\n//| fig-height: 2.5\n//| fig-responsive: false\n\ndigraph {\n  graph [layout = dot, rankdir = LR]\n\n  // define the global styles of the nodes\n  node [shape = rectangle, style = filled]\n\n  data1 [label = \" Varied Training \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#FF0000\"]\n  data2 [label = \" Constant Training \\n800-1000\", fillcolor = \"#00A08A\"]\n  Test3 [label = \"    Final Test \\n  Novel With Feedback  \\n100-300\\n350-550\\n600-800\", fillcolor = \"#ECCBAE\"]\n\n  // edge definitions with the node IDs\n  data1 -> Test1\n  data2 -> Test1\n\n  subgraph cluster {\n    label = \"Test Phase \\n(Counterbalanced Order)\"\n    Test1 [label = \"Test  \\nNovel Bands \\n100-300\\n350-550\\n600-800\", fillcolor = \"#ECCBAE\"]\n    Test2 [label = \"  Test \\n  Varied Training Bands  \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#ECCBAE\"]\n    Test1 -> Test2\n  }\n\n  Test2 -> Test3\n}\n\n```\n\n\n\n\n\n\n\n## Analyses Strategy\n\nAll data processing and statistical analyses were performed in R version 4.31 @rcoreteamLanguageEnvironmentStatistical2020. To assess differences between groups, we used Bayesian Mixed Effects Regression. Model fitting was performed with the brms package in R @burknerBrmsPackageBayesian2017, and descriptive stats and tables were extracted with the BayestestR package @makowskiBayestestRDescribingEffects2019. Mixed effects regression enables us to take advantage of partial pooling, simultaneously estimating parameters at the individual and group level. Our use of Bayesian, rather than frequentist methods allows us to directly quantify the uncertainty in our parameter estimates, as well as circumventing convergence issues common to the frequentist analogues of our mixed models. For each model, we report the median values of the posterior distribution, and 95% credible intervals.\n\nEach model was set to run with 4 chains, 5000 iterations per chain, with the first 2500 of which were discarded as warmup chains. Rhat values were generally within an acceptable range, with values \\<=1.02 (see appendix for diagnostic plots). We used uninformative priors for the fixed effects of the model (condition and velocity band), and weakly informative Student T distributions for for the random effects.\n\nWe compared varied and constant performance across two measures, deviation and discrimination. Deviation was quantified as the absolute deviation from the nearest boundary of the velocity band, or set to 0 if the throw velocity fell anywhere inside the target band. Thus, when the target band was 600-800, throws of 400, 650, and 1100 would result in deviation values of 200, 0, and 300, respectively. Discrimination was measured by fitting a linear model to the testing throws of each subjects, with the lower end of the target velocity band as the predicted variable, and the x velocity produced by the participants as the predictor variable. Participants who reliably discriminated between velocity bands tended to have positive slopes with values \\~1, while participants who made throws irrespective of the current target band would have slopes \\~0.\n\n\n\n\n\n\n\n\n::: {#tbl-e1-test-nf-deviation .cell layout-ncol=\"1\" tbl-cap='Testing Deviation - Empirical Summary' tbl-subcap='[\"Full datasets\",\"Intersection of samples with all labels available\"]'}\n\n```{.r .cell-code}\nresult <- test_summary_table(test, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\n\n\nresult$constant |>kable(booktabs = TRUE,\n                        linesep = \"\\\\addlinespace[0.5em]\",\n                        caption = paste(\"Summary of Deviation- Constant\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Summary of Deviation- Constant\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  254|    148| 298|\n|350-550   |Extrapolation |  191|    110| 229|\n|600-800   |Extrapolation |  150|     84| 184|\n|800-1000  |Trained       |  184|    106| 242|\n|1000-1200 |Extrapolation |  233|    157| 282|\n|1200-1400 |Extrapolation |  287|    214| 290|\n\n\n:::\n\n```{.r .cell-code}\n# |>\n#   kable_styling(font_size = ifelse(fmt_out == \"latex\", 8.5, NA))\n\nresult$varied |> kable(booktabs = TRUE,\n                        linesep = \"\\\\addlinespace[0.5em]\",\n                        caption = paste(\"Summary of Deviation- Varied\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Summary of Deviation- Varied\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  386|    233| 426|\n|350-550   |Extrapolation |  285|    149| 340|\n|600-800   |Extrapolation |  234|    144| 270|\n|800-1000  |Trained       |  221|    149| 248|\n|1000-1200 |Trained       |  208|    142| 226|\n|1200-1400 |Trained       |  242|    182| 235|\n\n\n:::\n:::\n\n\n\n\n\n## Results\n\n### Testing Phase - No feedback.\n\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw.\n\n#### Deviation From Target Band\n\nDescriptive summaries testing deviation data are provided in @tbl-e1-test-nf-deviation and @fig-e1-test-dev. To model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id).\n\n\n\n\n\n```{=tex}\n\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\ntest |>  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n```\n\n::: {.cell-output-display}\n![E1. Deviations from target band during testing without feedback stage.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-test-dev-1.jpeg){#fig-e1-test-dev}\n:::\n:::\n\n::: {#tbl-e1-bmm-dist .cell tbl-cap='Experiment 1. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band' tbl-subcap='[\"Constant Testing1 - Deviation\",\"Varied Testing - Deviation\"]'}\n\n```{.r .cell-code}\nmodelName <- \"e1_testDistBand_RF_5K\"\ne1_distBMM <- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=test,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nGetModelStats(e1_distBMM) |> kable(booktabs = TRUE,caption = paste(\"Coefficients\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Coefficients\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   205.09|        136.86|        274.06| 1.00|\n|conditVaried |   157.44|         60.53|        254.90| 1.00|\n|Band         |     0.01|         -0.07|          0.08| 0.57|\n|condit*Band  |    -0.16|         -0.26|         -0.06| 1.00|\n\n\n:::\n\n```{.r .cell-code}\ne1_distBMM |> \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |> \n  pairs() |> gather_emmeans_draws()  |> \n   summarize(median_qi(.value),pd=sum(.value>0)/n()) |>\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |> \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value<0,1-pd,pd)) |> kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\n\n|contrast          | Band|   value|  lower|  upper|   pd|\n|:-----------------|----:|-------:|------:|------:|----:|\n|Constant - Varied |  100| -141.49| -229.2| -53.83| 1.00|\n|Constant - Varied |  350| -101.79| -165.6| -36.32| 1.00|\n|Constant - Varied |  600|  -62.02| -106.2| -14.77| 1.00|\n|Constant - Varied |  800|  -30.11|  -65.1|   6.98| 0.94|\n|Constant - Varied | 1000|    2.05|  -33.5|  38.41| 0.54|\n|Constant - Varied | 1200|   33.96|  -11.9|  81.01| 0.92|\n\n\n:::\n\n```{.r .cell-code}\n# |> \n#   kable(caption=\"Contrasts\")\n\ncoef_details <- get_coef_details(e1_distBMM, \"conditVaried\")\n```\n:::\n\n\n\n\n\n\nThe model predicting absolute deviation (dist) showed clear effects of both training condition and target velocity band (Table X). Overall, the varied training group showed a larger deviation relative to the constant training group ( = 157.44, 95% CI \\[60.53, 254.9\\]). Deviation also depended on target velocity band, with lower bands showing less deviation. See @tbl-e1-bmm-dist for full model output.\n\n\n#### Discrimination between bands\n\nIn addition to accuracy/deviation, we also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). @tbl-e1-test-nf-vx shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants on each testing trial.\n\n\n\n\n\n```{=tex}\n\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\ntest %>% group_by(id,vb,condit) |> plot_distByCondit()\n```\n\n::: {.cell-output-display}\n![E1 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-test-vx-1.jpeg){#fig-e1-test-vx}\n:::\n:::\n\n::: {#tbl-e1-test-nf-vx .cell tbl-cap='Testing vx - Empirical Summary' tbl-subcap='[\"Constant\",\"Varied\"]'}\n\n```{.r .cell-code}\nresult <- test_summary_table(test, \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |> kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  524|    448| 327|\n|350-550   |Extrapolation |  659|    624| 303|\n|600-800   |Extrapolation |  770|    724| 300|\n|800-1000  |Trained       | 1001|    940| 357|\n|1000-1200 |Extrapolation | 1167|   1104| 430|\n|1200-1400 |Extrapolation | 1283|   1225| 483|\n\n\n:::\n\n```{.r .cell-code}\nresult$varied |> kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  664|    533| 448|\n|350-550   |Extrapolation |  768|    677| 402|\n|600-800   |Extrapolation |  876|    813| 390|\n|800-1000  |Trained       | 1064|   1029| 370|\n|1000-1200 |Trained       | 1180|   1179| 372|\n|1200-1400 |Trained       | 1265|   1249| 412|\n\n\n:::\n:::\n\n::: {#tbl-e1-bmm-vx .cell tbl-cap='Experiment 1. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band' tbl-subcap='[\"Model fit to all 6 bands\",\"Model fit to 3 extrapolation bands\"]'}\n\n```{.r .cell-code}\ne1_vxBMM <- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=test,file=paste0(here::here(\"data/model_cache\", \"e1_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nGetModelStats(e1_vxBMM ) |> kable(booktabs=T, caption=\"Fit to all 6 bands\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Fit to all 6 bands\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   408.55|        327.00|        490.61| 1.00|\n|conditVaried |   164.05|         45.50|        278.85| 1.00|\n|Band         |     0.71|          0.62|          0.80| 1.00|\n|condit*Band  |    -0.14|         -0.26|         -0.01| 0.98|\n\n\n:::\n\n```{.r .cell-code}\ncd1 <- get_coef_details(e1_vxBMM, \"conditVaried\")\nsc1 <- get_coef_details(e1_vxBMM, \"bandInt\")\nintCoef1 <- get_coef_details(e1_vxBMM, \"conditVaried:bandInt\")\n\n\nmodelName <- \"e1_extrap_testVxBand\"\ne1_extrap_VxBMM <- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                  data=test |>\n                    filter(expMode==\"test-Nf\"),file=paste0(here::here(\"data/model_cache\",modelName)),\n                  iter=5000,chains=4)\nGetModelStats(e1_extrap_VxBMM ) |> kable(booktabs=T, caption=\"Fit to 3 extrapolation bands\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Fit to 3 extrapolation bands\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   478.47|        404.00|        551.45| 1.00|\n|conditVaried |   142.04|         37.17|        247.59| 1.00|\n|Band         |     0.50|          0.42|          0.57| 1.00|\n|condit*Band  |    -0.07|         -0.17|          0.04| 0.89|\n\n\n:::\n\n```{.r .cell-code}\nsc2 <- get_coef_details(e1_extrap_VxBMM, \"bandInt\")\nintCoef2 <- get_coef_details(e1_extrap_VxBMM, \"conditVaried:bandInt\")\n```\n:::\n\n\n\n\n\nSee @tbl-e1-bmm-vx for the full model results. The estimated coefficient for training condition ($B$ = 164.05, 95% CrI \\[45.5, 278.85\\]) suggests that the varied group tends to produce harder throws than the constant group, but is not in and of itself useful for assessing discrimination. Most relevant to the issue of discrimination is the slope on Velocity Band ($B$ = 0.71, 95% CrI \\[0.62, 0.8\\]). Although the median slope does fall underneath the ideal of value of 1, the fact that the 95% credible interval does not contain 0 provides strong evidence that participants exhibited some discrimination between bands. The estimate for the interaction between slope and condition ($B$ = -0.14, 95% CrI \\[-0.26, -0.01\\]), suggests that the discrimination was somewhat modulated by training condition, with the varied participants showing less senitivity between vands than the constant condition. This difference is depicted visually in @fig-e1-bmm-vx.Slope coefficients are broken down by quartile in @tbl-e1-slope-quartile. The constant participant participants appear to have larger slopes across quartiles, but the difference between conditions may be less pronounced for the top quartiles of subjects who show the strongest discrimination. Figure @fig-e1-bmm-bx2 shows the distributions of slope values for each participant, and the compares the probability density of slope coefficients between training conditions. @fig-e1-indv-slopes \n\nThe second model, which focused solely on extrapolation bands, revealed similar patterns. The Velocity Band term ($B$ = 0.5, 95% CrI \\[0.42, 0.57\\]) still demonstrates a high degree of discrimination ability. However, the posterior distribution for interaction term ($B$ = -0.07, 95% CrI \\[-0.17, 0.04\\] ) does across over 0, suggesting that the evidence for decreased discrimination ability for the varied participants is not as strong when considering only the three extrapolation bands.\n\n\n\n\n\n\n\n::: {#fig-e1-bmm-vx .cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\ne1_vxBMM |> emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |>\n  gather_emmeans_draws() |> \n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(test$vb), \n                     limits = c(0, 1400))\n\ne1_extrap_VxBMM |> emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600))) |>\n  gather_emmeans_draws() |>\n  condEffects() +\n  scale_x_continuous(breaks = c(100, 350, 600), \n                     labels = levels(test$vb)[1:3], \n                     limits = c(0, 1000)) \n```\n\n::: {.cell-output-display}\n![Model fit to all 6 bands](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-bmm-vx-1.jpeg){#fig-e1-bmm-vx-1}\n:::\n\n::: {.cell-output-display}\n![Model fit to only 3 extrapolation bands](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-bmm-vx-2.jpeg){#fig-e1-bmm-vx-2}\n:::\n\nConditional effect of training condition and Band. Ribbons indicate 95% HDI. The steepness of the lines serves as an indicator of how well participants discriminated between velocity bands.\n:::\n\n::: {#tbl-e1-slope-quartile .cell tbl-cap='Slope coefficients by quartile, per condition'}\n\n```{.r .cell-code}\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")]))) |> \n  dplyr::arrange(id,bandInt) |> \n  mutate(condit_dummy = ifelse(condit == \"Varied\", 1, 0)) \n\nindv_coefs <- as_tibble(coef(e1_vxBMM)$id, rownames=\"id\")|> \n  select(id, starts_with(\"Est\")) |>\n  left_join(e1Sbjs, by=join_by(id) ) \n\n\nfixed_effects <- e1_vxBMM |> \n  spread_draws(`^b_.*`,regex=TRUE) |> arrange(.chain,.draw,.iteration)\n\n\nrandom_effects <- e1_vxBMM |> \n  gather_draws(`^r_id.*$`, regex = TRUE, ndraws = 1500) |> \n  separate(.variable, into = c(\"effect\", \"id\", \"term\"), sep = \"\\\\[|,|\\\\]\") |> \n  mutate(id = factor(id,levels=levels(test$id))) |> \n  pivot_wider(names_from = term, values_from = .value) |> arrange(id,.chain,.draw,.iteration)\n\n\n indvDraws <- left_join(random_effects, fixed_effects, by = join_by(\".chain\", \".iteration\", \".draw\")) |> \n  rename(bandInt_RF = bandInt,RF_Intercept=Intercept) |>\n  right_join(new_data_grid, by = join_by(\"id\")) |> \n  mutate(\n    Slope = bandInt_RF+b_bandInt,\n    Intercept= RF_Intercept + b_Intercept,\n    estimate = (b_Intercept + RF_Intercept) + (bandInt*(b_bandInt+bandInt_RF)) + (bandInt * condit_dummy) * `b_conditVaried:bandInt`,\n    SlopeInt = Slope + (`b_conditVaried:bandInt`*condit_dummy)\n  ) \n\n  indvSlopes <- indvDraws |> group_by(id) |> median_qi(Slope,SlopeInt, Intercept,b_Intercept,b_bandInt) |>\n  left_join(e1Sbjs, by=join_by(id)) |> group_by(condit) |>\n    select(id,condit,Intercept,b_Intercept,starts_with(\"Slope\"),b_bandInt, n) |>\n  mutate(rankSlope=rank(Slope)) |> arrange(rankSlope)   |> ungroup()\n \n  \n  indvSlopes |> mutate(Condition=condit) |>  group_by(Condition) |> \n    reframe(enframe(quantile(SlopeInt, c(0.0,0.25, 0.5, 0.75,1)), \"quantile\", \"SlopeInt\")) |> \n  pivot_wider(names_from=quantile,values_from=SlopeInt,names_prefix=\"Q_\") |>\n  group_by(Condition) |>\n  summarise(across(starts_with(\"Q\"), list(mean = mean))) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Condition | Q_0%_mean| Q_25%_mean| Q_50%_mean| Q_75%_mean| Q_100%_mean|\n|:---------|---------:|----------:|----------:|----------:|-----------:|\n|Constant  |    -0.104|      0.481|      0.690|      0.928|         1.4|\n|Varied    |    -0.203|      0.267|      0.589|      0.900|         1.3|\n\n\n:::\n:::\n\n::: {#fig-e1-bmm-bx2 .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nindvSlopes |> ggplot(aes(y=rankSlope, x=SlopeInt,fill=condit,color=condit)) + \n  geom_pointrange(aes(xmin=SlopeInt.lower , xmax=SlopeInt.upper)) + \n  labs(x=\"Estimated Slope\", y=\"Participant\")  + facet_wrap(~condit)\n\n   ggplot(indvSlopes, aes(x = SlopeInt, color = condit)) + \n  geom_density() + labs(x=\"Slope Coefficient\",y=\"Density\")\n```\n\n::: {.cell-output-display}\n![Slope estimates by participant - ordered from lowest to highest within each condition. ](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-bmm-bx2-1.jpeg){#fig-e1-bmm-bx2-1}\n:::\n\n::: {.cell-output-display}\n![Destiny of slope coefficients by training group](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-bmm-bx2-2.jpeg){#fig-e1-bmm-bx2-2}\n:::\n\nSlope distributions between condition\n:::\n\n::: {#fig-e1-indv-slopes .cell}\n\n```{.r .cell-code}\nnSbj <- 3\nindvDraws  |> indv_model_plot(indvSlopes, testAvg, SlopeInt,rank_variable=Slope,n_sbj=nSbj,\"max\")\nindvDraws |> indv_model_plot(indvSlopes, testAvg,SlopeInt, rank_variable=Slope,n_sbj=nSbj,\"min\")\n```\n\n::: {.cell-output-display}\n![subset with largest slopes](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-indv-slopes-1.jpeg){#fig-e1-indv-slopes-1}\n:::\n\n::: {.cell-output-display}\n![subset with smallest slopes](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-indv-slopes-2.jpeg){#fig-e1-indv-slopes-2}\n:::\n\nSubset of Varied and Constant Participants with the smallest and largest estimated slope values. Red lines represent the best fitting line for each participant, gray lines are 200 random samples from the posterior distribution. Black points and intervals at each band represent the estimated median and 95% HDI. Blue points are empirical means.\n:::\n\n\n\n\n\n\n# Experiment 2\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestE2 <- readRDS(here::here(\"data/e2_08-21-23.rds\")) |> filter(expMode2 == \"Test\") \ne2Sbjs <- testE2 |> group_by(id,condit) |> summarise(n=n())\ntestE2Avg <- testE2 %>% group_by(id, condit, vb, bandInt,bandType,tOrder) %>%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n```\n:::\n\n\n\n\n\n@fig-design-e2 illustrates the design of Experiment 2. The stages of the experiment (i.e. training, testing no-feedback, test with feedback), are identical to that of Experiment 1. The only change is that Experiment 2 participants train, and then test, on bands in the reverse order of Experiment 1 (i.e. training on the softer bands; and testing on the harder bands). \n\n\n\n\n\n```{dot}\n//| label: fig-design-e2\n//| fig-cap: \"Experiment 2 Design. Constant and Varied participants complete different training conditions. The training and testing bands are the reverse of Experiment 1. \"\n//| fig-width: 6.0\n//| fig-height: 2.5\n//| fig-responsive: false\n//| fig-align: left\ndigraph {\n  graph [layout = dot, rankdir = LR]\n\n  // define the global styles of the nodes\n  node [shape = rectangle, style = filled]\n\n  data1 [label = \" Varied Training \\n100-300\\n350-550\\n600-800\", fillcolor = \"#FF0000\"]\n  data2 [label = \" Constant Training \\n600-800\", fillcolor = \"#00A08A\"]\n  Test3 [label = \"    Final Test \\n  Novel With Feedback  \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#ECCBAE\"]\n\n  // edge definitions with the node IDs\n  data1 -> Test1\n  data2 -> Test1\n  subgraph cluster {\n    label = \"Test Phase \\n(Counterbalanced Order)\"\n    Test1 [label = \"Test  \\nNovel Bands  \\n800-1000\\n1000-1200\\n1200-1400\", fillcolor = \"#ECCBAE\"]\n    Test2 [label = \"  Test \\n  Varied Training Bands  \\n100-300\\n350-550\\n600-800\", fillcolor = \"#ECCBAE\"]\n    Test1 -> Test2\n  }\n\n  Test2 -> Test3\n}\n\n```\n\n\n\n\n\n\n\n\n\n\n\n## E2 Results\n\n### Testing Phase - No feedback. \n\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw. \n\n\n#### Deviation From Target Band\n\nDescriptive summaries testing deviation data are provided in @tbl-e2-test-nf-deviation and @fig-e2-test-dev. \nTo model differences in accuracy between groups, we used Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id). \n\n\\begin{equation}\ndist_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot band_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot band_{ij} + b_{0i} + b_{1i} \\cdot band_{ij} + \\epsilon_{ij}\n\\end{equation}\n\n\n\n\n\n::: {#tbl-e2-test-nf-deviation .cell tbl-cap='Testing Deviation - Empirical Summary' tbl-subcap='[\"Constant Testing - Deviation\",\"Varied Testing - Deviation\"]'}\n\n```{.r .cell-code}\nresult <- test_summary_table(testE2, \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  206|     48| 317|\n|350-550   |Extrapolation |  194|     86| 268|\n|600-800   |Trained       |  182|    112| 240|\n|800-1000  |Extrapolation |  200|    129| 233|\n|1000-1200 |Extrapolation |  238|    190| 234|\n|1200-1400 |Extrapolation |  311|    254| 288|\n\n\n:::\n\n```{.r .cell-code}\nresult$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Trained       |  153|     25| 266|\n|350-550   |Trained       |  138|     53| 233|\n|600-800   |Trained       |  160|    120| 183|\n|800-1000  |Extrapolation |  261|    207| 257|\n|1000-1200 |Extrapolation |  305|    258| 273|\n|1200-1400 |Extrapolation |  363|    314| 297|\n\n\n:::\n\n```{.r .cell-code}\n# make kable table with smaller font size\n# result$constant |> kable(caption=\"Constant Testing - Deviation\",booktabs=T,escape=F) |> kable_styling(font_size = 7)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntestE2 |>  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\")\n```\n\n::: {.cell-output-display}\n![E2. Deviations from target band during testing without feedback stage.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2-test-dev-1.jpeg){#fig-e2-test-dev}\n:::\n:::\n\n::: {#tbl-e2-bmm-dist .cell tbl-cap='Experiment 2. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band'}\n\n```{.r .cell-code}\nmodelName <- \"e2_testDistBand_RF_5K\"\ne2_distBMM <- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=testE2,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp2 <- GetModelStats(e2_distBMM) |> kable(booktabs=T)\nmp2\n```\n\n::: {.cell-output-display}\n\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   151.71|         90.51|        215.86| 1.00|\n|conditVaried |   -70.33|       -156.87|         16.66| 0.94|\n|Band         |     0.10|          0.02|          0.18| 1.00|\n|condit*Band  |     0.12|          0.02|          0.23| 0.99|\n\n\n:::\n\n```{.r .cell-code}\ne2_distBMM |> \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |> \n  pairs() |> gather_emmeans_draws()  |> \n   summarize(median_qi(.value),pd=sum(.value>0)/n()) |>\n   select(contrast,Band=bandInt,value=y,lower=ymin,upper=ymax,pd) |> \n   mutate(across(where(is.numeric), \\(x) round(x, 2)),\n          pd=ifelse(value<0,1-pd,pd)) |>\n   kable(caption=\"Contrasts\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Contrasts\n\n|contrast          | Band| value|  lower|  upper|   pd|\n|:-----------------|----:|-----:|------:|------:|----:|\n|Constant - Varied |  100|  57.6|  -20.5| 135.32| 0.93|\n|Constant - Varied |  350|  26.6|  -30.9|  83.84| 0.83|\n|Constant - Varied |  600|  -4.3|  -46.7|  38.52| 0.58|\n|Constant - Varied |  800| -29.3|  -69.4|  11.29| 0.92|\n|Constant - Varied | 1000| -54.6| -101.1|  -5.32| 0.98|\n|Constant - Varied | 1200| -79.6| -139.5| -15.45| 0.99|\n\n\n:::\n\n```{.r .cell-code}\ncoef_details <- get_coef_details(e2_distBMM, \"conditVaried\")\n```\n:::\n\n\n\n\n\n\n\n\n\nThe model predicting absolute deviation showed a modest tendency for the varied training group to have lower deviation compared to the constant training group ( = -70.33, 95% CI \\[-156.87, 16.66\\]),with 94% of the posterior distribution being less than 0. This suggests a potential benefit of training with variation, though the evidence is not definitive.\n\n\n#### Discrimination between Velocity Bands\n\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). @tbl-e2-test-nf-vx shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants. \n\n\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestE2 %>% group_by(id,vb,condit) |> plot_distByCondit()\n```\n\n::: {.cell-output-display}\n![E2 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2-test-vx-1.jpeg){#fig-e2-test-vx}\n:::\n:::\n\n::: {#tbl-e2-test-nf-vx .cell layout-ncol=\"1\" tbl-cap='Testing vx - Empirical Summary' tbl-subcap='[\"Constant Testing - vx\",\"Varied Testing - vx\"]'}\n\n```{.r .cell-code}\nresult <- test_summary_table(testE2, \"vx\",\"X Velocity\" ,mfun = list(mean = mean, median = median, sd = sd))\nresult$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  457|    346| 354|\n|350-550   |Extrapolation |  597|    485| 368|\n|600-800   |Trained       |  728|    673| 367|\n|800-1000  |Extrapolation |  953|    913| 375|\n|1000-1200 |Extrapolation | 1064|   1012| 408|\n|1200-1400 |Extrapolation | 1213|   1139| 493|\n\n\n:::\n\n```{.r .cell-code}\nresult$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Trained       |  410|    323| 297|\n|350-550   |Trained       |  582|    530| 303|\n|600-800   |Trained       |  696|    641| 316|\n|800-1000  |Extrapolation |  910|    848| 443|\n|1000-1200 |Extrapolation | 1028|    962| 482|\n|1200-1400 |Extrapolation | 1095|   1051| 510|\n\n\n:::\n:::\n\n::: {#tbl-e2-bmm-vx .cell tbl-cap='Experiment 2. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band'}\n\n```{.r .cell-code}\ne2_vxBMM <- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=testE2,file=paste0(here::here(\"data/model_cache\", \"e2_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nmt3 <-GetModelStats(e2_vxBMM ) |> kable(escape=F,booktabs=T)\nmt3\n```\n\n::: {.cell-output-display}\n\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   362.64|        274.85|        450.02| 1.00|\n|conditVaried |    -8.56|       -133.97|        113.98| 0.55|\n|Band         |     0.71|          0.58|          0.84| 1.00|\n|condit*Band  |    -0.06|         -0.24|          0.13| 0.73|\n\n\n:::\n\n```{.r .cell-code}\ncd1 <- get_coef_details(e2_vxBMM, \"conditVaried\")\nsc1 <- get_coef_details(e2_vxBMM, \"bandInt\")\nintCoef1 <- get_coef_details(e2_vxBMM, \"conditVaried:bandInt\")\n```\n:::\n\n\n\n\n\n\n\nSee @tbl-e2-bmm-vx for the full model results. \n\nWhen examining discrimination ability using the model predicting raw x-velocity, the results were less clear than those of the absolute deviation analysis. The slope on Velocity Band ( = 0.71, 95% CrI \\[0.58, 0.84\\]) indicates that participants showed good discrimination between bands overall. However, the interaction term suggested this effect was not modulated by training condition ( = -0.06, 95% CrI \\[-0.24, 0.13\\]) Thus, while varied training may provide some advantage for accuracy, both training conditions seem to have similar abilities to discriminate between velocity bands.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne2_vxBMM |> emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |>\n  gather_emmeans_draws() |>\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n  ylab(\"Predicted X Velocity\") + xlab(\"Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE2$vb), \n                     limits = c(0, 1400)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![Conditional effect of training condition and Band. Ribbons indicate 95% HDI.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e2-bmm-vx-1.jpeg){#fig-e2-bmm-vx}\n:::\n:::\n\n\n\n\n\n\n# Experiment 3\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestE3 <- readRDS(here::here(\"data/e3_08-21-23.rds\")) |> filter(expMode2 == \"Test\") \ne3Sbjs <- testE3 |> group_by(id,condit) |> summarise(n=n())\ntestE3Avg <- testE3 %>% group_by(id, condit, vb, bandInt,bandType,tOrder) %>%\n  summarise(nHits=sum(dist==0),vx=mean(vx),dist=mean(dist),sdist=mean(sdist),n=n(),Percent_Hit=nHits/n)\n```\n:::\n\n\n\n\n\nThe major manipulation adjustment of experiment 3 is for participants to receive ordinal feedback during training, in contrast to the continuous feedback of the earlier experiments. Ordinal feedback informs participants whether a throw was too soft, too hard, or fell within the target velocity range. Experiment 3 participants were randomly assigned to both a training condition (Constant vs. Varied) and a Band Order condition (original order used in Experiment 1, or the Reverse order of Experiment 2). \n\n\n## Results\n\n### Testing Phase - No feedback. \n\nIn the first part of the testing phase, participants are tested from each of the velocity bands, and receive no feedback after each throw. Note that these no-feedback testing trials are identical to those of Experiment 1 and 2, as the ordinal feedback only occurs during the training phase, and final testing phase, of Experiment 3. \n\n\n#### Deviation From Target Band\n\nDescriptive summaries testing deviation data are provided in @tbl-e3-test-nf-deviation and @fig-e3-test-dev. \nTo model differences in accuracy between groups, we fit Bayesian mixed effects regression models to the trial level data from the testing phase. The primary model predicted the absolute deviation from the target velocity band (dist) as a function of training condition (condit), target velocity band (band), and their interaction, with random intercepts and slopes for each participant (id). \n\n\n\n\n\n\n::: {#tbl-e3-test-nf-deviation .cell tbl-cap='Testing Deviation - Empirical Summary' tbl-subcap='[\"Constant Testing - Deviation\",\"Varied Testing - Deviation\"]'}\n\n```{.r .cell-code}\nresultOrig <- test_summary_table(testE3 |> filter(bandOrder==\"Original\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  396|    325| 350|\n|350-550   |Extrapolation |  278|    176| 299|\n|600-800   |Extrapolation |  173|    102| 215|\n|800-1000  |Trained       |  225|    126| 284|\n|1000-1200 |Extrapolation |  253|    192| 271|\n|1200-1400 |Extrapolation |  277|    210| 262|\n\n\n:::\n\n```{.r .cell-code}\nresultOrig$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  383|    254| 385|\n|350-550   |Extrapolation |  287|    154| 318|\n|600-800   |Extrapolation |  213|    140| 244|\n|800-1000  |Trained       |  199|    142| 209|\n|1000-1200 |Trained       |  222|    163| 221|\n|1200-1400 |Trained       |  281|    227| 246|\n\n\n:::\n\n```{.r .cell-code}\nresultRev <- test_summary_table(testE3 |> filter(bandOrder==\"Reverse\"), \"dist\",\"Deviation\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  403|    334| 383|\n|350-550   |Extrapolation |  246|    149| 287|\n|600-800   |Trained       |  155|     82| 209|\n|800-1000  |Extrapolation |  207|    151| 241|\n|1000-1200 |Extrapolation |  248|    220| 222|\n|1200-1400 |Extrapolation |  322|    281| 264|\n\n\n:::\n\n```{.r .cell-code}\nresultRev$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Trained       |  153|      0| 307|\n|350-550   |Trained       |  147|     55| 258|\n|600-800   |Trained       |  159|    107| 192|\n|800-1000  |Extrapolation |  221|    160| 235|\n|1000-1200 |Extrapolation |  244|    185| 235|\n|1200-1400 |Extrapolation |  324|    264| 291|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntestE3 |>  ggplot(aes(x = vb, y = dist,fill=condit)) +\n    stat_summary(geom = \"bar\", position=position_dodge(), fun = mean) +\n    stat_summary(geom = \"errorbar\", position=position_dodge(.9), fun.data = mean_se, width = .4, alpha = .7) + \n  labs(x=\"Band\", y=\"Deviation From Target\") + facet_wrap(~bandOrder) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![e3. Deviations from target band during testing without feedback stage.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e3-test-dev-1.jpeg){#fig-e3-test-dev}\n:::\n:::\n\n::: {#tbl-e3-bmm-dist .cell tbl-cap='Experiment 3. Bayesian Mixed Model predicting absolute deviation as a function of condition (Constant vs. Varied) and Velocity Band'}\n\n```{.r .cell-code}\nmodelName <- \"e3_testDistBand_RF_5K\"\ne3_distBMM <- brm(dist ~ condit * bandInt + (1 + bandInt|id),\n                      data=testE3,file=paste0(here::here(\"data/model_cache\",modelName)),\n                      iter=5000,chains=4)\nmp3 <- GetModelStats(e3_distBMM) |> kable(booktabs=T)\nmp3\n```\n\n::: {.cell-output-display}\n\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper|   pd|\n|:------------|--------:|-------------:|-------------:|----:|\n|Intercept    |   306.47|        243.89|        368.75| 1.00|\n|conditVaried |   -90.65|       -182.79|          3.75| 0.97|\n|Band         |    -0.07|         -0.13|          0.00| 0.97|\n|condit*Band  |     0.09|         -0.01|          0.19| 0.96|\n\n\n:::\n\n```{.r .cell-code}\ncd1 <- get_coef_details(e3_distBMM, \"conditVaried\")\nsc1 <- get_coef_details(e3_distBMM, \"bandInt\")\nintCoef1 <- get_coef_details(e3_distBMM, \"conditVaried:bandInt\")\n```\n:::\n\n\n\n\n\n\nThe effect of training condition in Experiment 3 showed a similar pattern to Experiment 2, with the varied group tending to have lower deviation than the constant group ( = -90.65, 95% CrI \\[-182.79, 3.75\\]), with 97% of the posterior distribution falling under 0. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne3_distBMM |> emmeans( ~condit + bandInt, \n                       at = list(bandInt = c(100, 350, 600, 800, 1000, 1200))) |>\n  gather_emmeans_draws() |>\n  ggplot(aes(x = bandInt, y = .value, color = condit, fill = condit)) + \n  stat_dist_pointinterval() +\n  stat_lineribbon(alpha = .25, size = 1, .width = c(.95)) +\n    ylab(\"Predicted Deviation\") + xlab(\"Velocity Band\")+\n  scale_x_continuous(breaks = c(100, 350, 600, 800, 1000, 1200), \n                     labels = levels(testE3$vb), \n                     limits = c(0, 1400)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![e3. Conditioinal Effect of Training Condition and Band. Ribbon indicated 95% Credible Intervals.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e3-bmm-dist-1.jpeg){#fig-e3-bmm-dist}\n:::\n:::\n\n\n\n\n\n\n\n#### Discrimination between Velocity Bands\n\nIn addition to accuracy/deviation. We also assessed the ability of participants to reliably discriminate between the velocity bands (i.e. responding differently when prompted for band 600-800 than when prompted for band 150-350). @tbl-e3-test-nf-vx shows descriptive statistics of this measure, and Figure 1 visualizes the full distributions of throws for each combination of condition and velocity band. To quantify discrimination, we again fit Bayesian Mixed Models as above, but this time the dependent variable was the raw x velocity generated by participants. \n\n\\begin{equation}\nvx_{ij} = \\beta_0 + \\beta_1 \\cdot condit_{ij} + \\beta_2 \\cdot bandInt_{ij} + \\beta_3 \\cdot condit_{ij} \\cdot bandInt_{ij} + b_{0i} + b_{1i} \\cdot bandInt_{ij} + \\epsilon_{ij}\n\\end{equation}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# testE3 |> filter(bandOrder==\"Original\")|> group_by(id,vb,condit) |> plot_distByCondit()\n# testE3 |> filter(bandOrder==\"Reverse\")|> group_by(id,vb,condit) |> plot_distByCondit() +ggtitle(\"test\")\n\ntestE3 |> group_by(id,vb,condit,bandOrder) |> plot_distByCondit() + \n  facet_wrap(bandOrder~condit,scale=\"free_x\") \n```\n\n::: {.cell-output-display}\n![e3 testing x velocities. Translucent bands with dash lines indicate the correct range for each velocity band.](manuscript.markdown_strict_files/figure-markdown_strict/fig-e3-test-vx-1.jpeg){#fig-e3-test-vx}\n:::\n:::\n\n::: {#tbl-e3-test-nf-vx .cell tbl-cap='Testing vx - Empirical Summary' tbl-subcap='[\"Constant Testing - vx\",\"Varied Testing - vx\"]'}\n\n```{.r .cell-code}\nresultOrig <- test_summary_table(testE3 |> filter(bandOrder==\"Original\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultOrig$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  680|    625| 370|\n|350-550   |Extrapolation |  771|    716| 357|\n|600-800   |Extrapolation |  832|    786| 318|\n|800-1000  |Trained       | 1006|    916| 417|\n|1000-1200 |Extrapolation | 1149|   1105| 441|\n|1200-1400 |Extrapolation | 1180|   1112| 443|\n\n\n:::\n\n```{.r .cell-code}\nresultOrig$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  667|    554| 403|\n|350-550   |Extrapolation |  770|    688| 383|\n|600-800   |Extrapolation |  869|    814| 358|\n|800-1000  |Trained       |  953|    928| 359|\n|1000-1200 |Trained       | 1072|   1066| 388|\n|1200-1400 |Trained       | 1144|   1093| 426|\n\n\n:::\n\n```{.r .cell-code}\nresultRev <- test_summary_table(testE3 |> filter(bandOrder==\"Reverse\"), \"vx\",\"X Velocity\", mfun = list(mean = mean, median = median, sd = sd))\nresultRev$constant |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Extrapolation |  684|    634| 406|\n|350-550   |Extrapolation |  729|    679| 350|\n|600-800   |Trained       |  776|    721| 318|\n|800-1000  |Extrapolation |  941|    883| 387|\n|1000-1200 |Extrapolation | 1014|    956| 403|\n|1200-1400 |Extrapolation | 1072|   1014| 442|\n\n\n:::\n\n```{.r .cell-code}\nresultRev$varied |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Band      |Band Type     | Mean| Median|  Sd|\n|:---------|:-------------|----:|------:|---:|\n|100-300   |Trained       |  392|    270| 343|\n|350-550   |Trained       |  540|    442| 343|\n|600-800   |Trained       |  642|    588| 315|\n|800-1000  |Extrapolation |  943|    899| 394|\n|1000-1200 |Extrapolation | 1081|   1048| 415|\n|1200-1400 |Extrapolation | 1185|   1129| 500|\n\n\n:::\n:::\n\n::: {#tbl-e3-bmm-vx .cell tbl-cap='Experiment 3. Bayesian Mixed Model Predicting Vx as a function of condition (Constant vs. Varied) and Velocity Band'}\n\n```{.r .cell-code}\ne3_vxBMM <- brm(vx ~ condit * bandInt + (1 + bandInt|id),\n                        data=testE3,file=paste0(here::here(\"data/model_cache\", \"e3_testVxBand_RF_5k\")),\n                        iter=5000,chains=4,silent=0,\n                        control=list(adapt_delta=0.94, max_treedepth=13))\nmt4 <-GetModelStats(e3_vxBMM ) |> kable(booktabs=T)\nmt4\n```\n\n::: {.cell-output-display}\n\n\n|Term         | Estimate| 95% CrI Lower| 95% CrI Upper| pd|\n|:------------|--------:|-------------:|-------------:|--:|\n|Intercept    |   607.67|        536.02|        679.87|  1|\n|conditVaried |  -167.76|       -277.14|        -64.08|  1|\n|Band         |     0.44|          0.35|          0.52|  1|\n|condit*Band  |     0.18|          0.06|          0.31|  1|\n\n\n:::\n\n```{.r .cell-code}\ncd1 <- get_coef_details(e3_vxBMM, \"conditVaried\")\nsc1 <- get_coef_details(e3_vxBMM, \"bandInt\")\nintCoef1 <- get_coef_details(e3_vxBMM, \"conditVaried:bandInt\")\n```\n:::\n\n\n\n\n\n\nSee @tbl-e3-bmm-vx for the full model results. \n\nSlope estimates for experiment 3 suggest that participants were capable of distinguishing between velocity bands even when provided only ordinal feedback during training ( = 0.44, 95% CrI \\[0.35, 0.52\\]). Unlike the previous two experiments, the posterior distribution for the interaction between condition and band was consistently positive, suggestive of superior discrimination for the varied participants \n = 0.18, 95% CrI \\[0.06, 0.31\\]. \n\n### Computational Modelling \n\n\n\n\n---\n# title: EXAM Fits and Predictions\n# date: last-modified\n# categories: [Simulation, ALM, EXAM, R]\ncode-fold: true\ncode-tools: true\nexecute: \n  warning: false\n  eval: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# load and view data\npacman::p_load(tidyverse,patchwork,here, pander, latex2exp, flextable)\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\", \"Functions/alm_core.R\",\"Functions/misc_model_funs.R\")),source)\n\npurrr::walk(here::here(c(\"Functions/Display_Functions.R\")),source)\n\nselect <- dplyr::select; mutate <- dplyr::mutate \n\nds <- readRDS(here::here(\"data/e1_md_11-06-23.rds\"))\ndsAvg <- ds |> group_by(condit,expMode2,tr, x) |> \n  summarise(y=mean(y),.groups=\"keep\") \n\nvAvg <- dsAvg |> filter(condit==\"Varied\")\ncAvg <- dsAvg |> filter(condit==\"Constant\")\n\n#i1 <- ds |> filter(id==\"3\")\n\ninput.layer <- c(100,350,600,800,1000,1200)\noutput.layer <- c(100,350,600,800,1000,1200)\n\n\npurrr::walk(c(\"con_group_exam_fits\", \"var_group_exam_fits\", \"hybrid_group_exam_fits\"), \n            ~ list2env(readRDS(here::here(paste0(\"data/model_cache/\", .x, \".rds\"))), \n            envir = .GlobalEnv))\n\n# pluck(ex_te_v, \"Fit\") |> mutate(w= ifelse(exists(\"w\"), round(w,2),NA))\n# pluck(hybrid_te_v, \"Fit\") |> mutate(w= ifelse(exists(\"w\"), round(w,2), NA))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n alm_plot()\n```\n\n::: {.cell-output-display}\n![The basic structure of the ALM model.](manuscript.markdown_strict_files/figure-markdown_strict/fig-alm-diagram-1.jpeg){#fig-alm-diagram}\n:::\n:::\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n::: column-page-inset-right\n\n\n|                    | **ALM Response Generation**                                            |                                                                                               |\n|------------------|------------------------------|-------------------------|\n| Input Activation   | $a_i(X) = \\frac{e^{-c(X-X_i)^2}}{\\sum_{k=1}^M e^{-c(X-X_k)^2}}$    | Input nodes activate as a function of Gaussian similarity to stimulus                         |\n| Output Activation  | $O_j(X) = \\sum_{k=1}^M w_{ji} \\cdot a_i(X)$                        | Output unit $O_j$ activation is the weighted sum of input activations and association weights |\n| Output Probability | $P[Y_j|X] = \\frac{O_j(X)}{\\sum_{k=1}^M O_k(X)}$                    | The response, $Y_j$ probabilites computed via Luce's choice rule                              |\n| Mean Output        | $m(X) = \\sum_{j=1}^L Y_j \\cdot \\frac{O_j(x)}{\\sum_{k=1}^M O_k(X)}$ | Weighted average of probabilities determines response to X                                    |\n|                    | **ALM Learning**                                                   |                                                                                               |\n| Feedback           | $f_j(Z) = e^{-c(Z-Y_j)^2}$                                         | feedback signal Z computed as similarity between ideal response and observed response         |\n| magnitude of error | $\\Delta_{ji}=(f_{j}(Z)-o_{j}(X))a_{i}(X)$                          | Delta rule to update weights.                                                                 |\n| Update Weights     | $w_{ji}^{new}=w_{ji}+\\eta\\Delta_{ji}$                              | Updates scaled by learning rate parameter $\\eta$.                                             |\n|                    | **EXAM Extrapolation**                                             |                                                                                               |\n| Instance Retrieval | $P[X_i|X] = \\frac{a_i(X)}{\\sum_{k=1}^M a_k(X)}$                    | Novel test stimulus $X$ activates input nodes $X_i$                                           |\n| Slope Computation  | $S =$ $\\frac{m(X_{1})-m(X_{2})}{X_{1}-X_{2}}$                      | Slope value, $S$ computed from nearest training instances                                              |\n| Response           | $E[Y|X_i] = m(X_i) + S \\cdot [X - X_i]$                          | ALM response $m(X_i)$ adjusted by slope.                                                      |\n\n: ALM & EXAM Equations {#tbl-alm-exam}\n:::\n\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n\n# Modeling\n\nIn project 1, I applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, I will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. For this purpose, we will apply the associative learning model (ALM) and the EXAM model of function learning (DeLosh 1997). ALM is a simple connectionist learning model which closely resembles Kruschke's ALCOVE model (Kruscke 1992), with modifications to allow for the generation of continuous responses.\n\n## ALM & Exam Description\n\n@deloshExtrapolationSineQua1997 introduced the associative learning model (ALM), a connectionist model within the popular class of radial-basis networks. ALM was inspired by, and closely resembles Kruschke's influential ALCOVE model of categorization [@kruschkeALCOVEExemplarbasedConnectionist1992]. \n\nALM is a localist neural network model, with each input node corresponding to a particular stimulus, and each output node corresponding to a particular response value. The units in the input layer activate as a function of their Gaussian similarity to the input stimulus. So, for example, an input stimulus of value 55 would induce maximal activation of the input unit tuned to 55. Depending on thevalue of the generalization parameter, the nearby units (e.g. 54 and 56; 53 and 57) may also activate to some degree. ALM is structured with input and output nodes that correspond to regions of the stimulus space, and response space, respectively. The units in the input layer activate as a function of their similarity to a presented stimulus. As was the case with the exemplar-based models, similarity in ALM is exponentially decaying function of distance. The input layer is fully connected to the output layer, and the activation for any particular output node is simply the weighted sum of the connection weights between that node and the input activations. The network then produces a response by taking the weighted average of the output units (recall that each output unit has a value corresponding to a particular response). During training, the network receives feedback which activates each output unit as a function of its distance from the ideal level of activation necessary to produce the correct response. The connection weights between input and output units are then updated via the standard delta learning rule, where the magnitude of weight changes are controlled by a learning rate parameter.\n\nSee @tbl-alm-exam for a full specification of the equations that define ALM and EXAM.\n\n\n## Model Fitting and Comparison\n\nFollowing the procedure used by @mcdanielPredictingTransferPerformance2009, we will assess the ability of both ALM and EXAM to account for the empirical data when fitting the models to 1) only the training data, and 2) both training and testing data. Models were fit to the aggregated participant data by minimizing the root-mean squared deviation (RMSE). Because ALM has been shown to do poorly at accounting for human patterns extrapolation [@deloshExtrapolationSineQua1997], we will also generate predictions from the EXAM model for the testing stage. EXAM which operates identically to ALM during training, but includes a linear extrapolation mechanism for generating novel responses during testing.\n\nFor the hybrid model, predictions are computed by first generating separate predictions from ALM and EXAM, and then combining them using the following equation: $\\hat{y} = (1 - w) \\cdot alm.pred + w \\cdot exam.pred$. For the grid search, the weight parameter is varied from 0 to 1, and the resulting RMSE is recorded. \n\nEach model was fit to the data in 3 different ways. 1) To just the testing data, 2) Both the training and testing data, 3) Only the training data. In all cases, the model only updates its weights during the training phase, and the weights are frozen during the testing phase. In all cases, only the ALM model generates predictions during the training phase. For the testing phase, all 3 models are used to generate predictions. \n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-e1-cogmodel .cell tbl-cap='Fit Parameters and Model RMSE. The Test_RMSE column is the main performance indicator of interest, and represents the RMSE for just the testing data. The Fit_Method column indicates the data used to fit the model. The $w$ parameter determines the balance between the ALM and EXAM response generation processes, and is only included for the hybrid model. A weight of .5 would indicate equal contribution from both models. $w$ values approaching 1 indicate stronger weight for EXAM.'}\n\n```{.r .cell-code}\n##| column: body-outset-right\n\n\nreshaped_df <- all_combined_params %>%\n  select(-Value,-Test_RMSE) |>\n  rename(\"Fit Method\" = Fit_Method) |>\n  pivot_longer(cols=c(c,lr,w),names_to=\"Parameter\") %>%\n  unite(Group, Group, Parameter) %>%\n  pivot_wider(names_from = Group, values_from = value)\n\nheader_df <- data.frame(\n  col_keys = c(\"Model\", \"Fit Method\",\"Constant_c\", \"Constant_lr\", \"Constant_w\", \"Varied_c\", \"Varied_lr\", \"Varied_w\"),\n  line1 = c(\"\", \"\", \"Constant\", \"\", \"\", \"Varied\", \"\",\"\"),\n  line2 = c(\"Model\", \"Fit Method\", \"c\", \"lr\", \"w\", \"c\", \"lr\", \"w\")\n)\n\nft <- flextable(reshaped_df) %>% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %>% add_header_lines(values = \" \") %>%\n  theme_booktabs() %>% \n  merge_v(part = \"header\") %>% \n  merge_h(part = \"header\") %>%\n  merge_h(part = \"header\") %>%\n  align(align = \"center\", part = \"all\") %>% \n  #autofit() %>% \n  empty_blanks() %>% \n  fix_border_issues() %>% \n  hline(part = \"header\", i = 2, j=3:5) %>% \n  hline(part = \"header\", i = 2, j=6:8)\n\nft\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-9a16e216{}.cl-9a129594{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9a145226{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a145d7a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145d84{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145d85{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145d8e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145dac{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145db6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a145db7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-9a16e216'><thead><tr style=\"overflow-wrap:break-word;\"><th  colspan=\"8\"class=\"cl-9a145d7a\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"> </span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th  colspan=\"2\"class=\"cl-9a145d84\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></th><th class=\"cl-9a145d85\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Constant</span></p></th><th  colspan=\"2\"class=\"cl-9a145d85\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></th><th class=\"cl-9a145d85\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Varied</span></p></th><th  colspan=\"2\"class=\"cl-9a145d85\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-9a145d8e\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Model</span></p></th><th class=\"cl-9a145d8e\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Fit Method</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">c</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">lr</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">w</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">c</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">lr</span></p></th><th class=\"cl-9a145dac\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">w</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">ALM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test Only</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.000</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.100</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.134</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">2.030</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">ALM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test &amp; Train</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.047</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.080</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.067</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.100</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">ALM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Train Only</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.060</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.100</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.047</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.080</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">EXAM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test Only</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.007</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">1.327</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.409</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">1.910</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">EXAM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test &amp; Train</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.081</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.161</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.074</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.100</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">EXAM</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Train Only</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.060</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.100</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.047</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.080</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\"></span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Hybrid</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test Only</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.008</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">1.580</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">1</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.395</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">2.017</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.64</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Hybrid</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Test &amp; Train</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.067</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.134</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">1</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.134</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">2.017</span></p></td><td class=\"cl-9a145db6\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Hybrid</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">Train Only</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.042</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.067</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.042</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.067</span></p></td><td class=\"cl-9a145db7\"><p class=\"cl-9a145226\"><span class=\"cl-9a129594\">0.00</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\n\n\n\n### Testing Observations vs. Predictions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntvte<- pluck(a_te_v, \"test\") |> \n  mutate(Fit_Method=\"Test Only\") |>\n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |> pull(pred))\n\ntvtetr<-pluck(a_tetr_v, \"test\") |> \n  mutate(Fit_Method=\"Test & Train\") |> \n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |> pull(pred))\n\ntvtr<- pluck(a_tr_v, \"test\")|> \n  mutate(Fit_Method=\"Train Only\") |> \n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |> pull(pred))\n\ntcte<- pluck(a_te_c, \"test\") |> \n  mutate(Fit_Method=\"Test Only\") |> \n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |> pull(pred))\n\ntctetr<-pluck(a_tetr_c, \"test\") |> \n  mutate(Fit_Method=\"Test & Train\") |>  \n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |> pull(pred))\n\ntctr<- pluck(a_tr_c, \"test\")|> \n  mutate(Fit_Method=\"Train Only\") |>  \n  rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |> pull(pred))\n\nvPreds <- rbind(tvte,tvtetr, tvtr) |> relocate(Fit_Method,.before=x) |> \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\ncPreds <- rbind(tcte,tctetr, tctr) |> relocate(Fit_Method,.before=x) |> \n   mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\nallPreds <- rbind(vPreds |> mutate(Group=\"Varied\"), cPreds |> mutate(Group=\"Constant\")) |>\n  pivot_longer(cols=c(\"ALM\",\"EXAM\",\"Hybrid\"), names_to=\"Model\",values_to = \"Prediction\") |> \n  mutate(Error=Observed-Prediction, Abs_Error=((Error)^2)) |> \n  group_by(Group,Fit_Method, Model) #|> summarise(Mean_Error=mean(Error), Abs_Error=mean(Abs_Error))\n```\n:::\n\n::: {#tbl-e1-meanPreds .cell tbl-cap='Model Perforamnce - averaged over all X values/Bands. ME=Mean Average Error, RMSE = Root mean squared error.'}\n\n```{.r .cell-code}\nallPreds |> summarise(Error=mean(Error), Abs_Error=sqrt(mean(Abs_Error))) |> \n  mutate(Fit_Method=factor(Fit_Method, levels=c(\"Test Only\", \"Test & Train\", \"Train Only\"))) |>\n  tabulator(rows=c(\"Fit_Method\", \"Model\"), columns=c(\"Group\"), \n             `ME` = as_paragraph(Error), \n            `RMSE` = as_paragraph(Abs_Error)) |> as_flextable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-9a3509c6{}.cl-9a31a178{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9a3302fc{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a330306{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-9a330307{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a330310{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-9a330311{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a33031a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-9a33031b{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a330c70{width:1.096in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c71{width:0.705in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c7a{width:0.079in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c7b{width:0.646in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c84{width:0.705in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c85{width:0.561in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c86{width:1.096in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c87{width:0.705in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c8e{width:0.079in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c8f{width:0.646in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c90{width:0.705in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c98{width:0.561in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330c99{width:1.096in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330ca2{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330ca3{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cac{width:0.646in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cad{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cb6{width:0.561in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cb7{width:1.096in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cc0{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cca{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330ccb{width:0.646in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cd4{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cd5{width:0.561in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cd6{width:1.096in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cde{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cdf{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330ce8{width:0.646in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330ce9{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cf2{width:0.561in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cf3{width:1.096in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cfc{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cfd{width:0.646in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330cfe{width:0.705in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a330d06{width:0.561in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-9a3509c6'><thead><tr style=\"overflow-wrap:break-word;\"><th  rowspan=\"2\"class=\"cl-9a330c70\"><p class=\"cl-9a3302fc\"><span class=\"cl-9a31a178\">Fit_Method</span></p></th><th  rowspan=\"2\"class=\"cl-9a330c71\"><p class=\"cl-9a3302fc\"><span class=\"cl-9a31a178\">Model</span></p></th><th class=\"cl-9a330c7a\"><p class=\"cl-9a330306\"><span class=\"cl-9a31a178\"></span></p></th><th  colspan=\"2\"class=\"cl-9a330c7b\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">Constant</span></p></th><th class=\"cl-9a330c7a\"><p class=\"cl-9a330306\"><span class=\"cl-9a31a178\"></span></p></th><th  colspan=\"2\"class=\"cl-9a330c85\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">Varied</span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-9a330c8e\"><p class=\"cl-9a330310\"><span class=\"cl-9a31a178\"></span></p></th><th class=\"cl-9a330c8f\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">ME</span></p></th><th class=\"cl-9a330c90\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">RMSE</span></p></th><th class=\"cl-9a330c8e\"><p class=\"cl-9a330310\"><span class=\"cl-9a31a178\"></span></p></th><th class=\"cl-9a330c98\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">ME</span></p></th><th class=\"cl-9a330c90\"><p class=\"cl-9a330307\"><span class=\"cl-9a31a178\">RMSE</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"3\"class=\"cl-9a330c99\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Test Only</span></p></td><td class=\"cl-9a330ca2\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">ALM</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cac\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">223.8</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">348.0</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cb6\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">56.3</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">95.4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330ca2\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">EXAM</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cac\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-59.2</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">127.5</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cb6\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-6.0</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">45.9</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330ca2\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Hybrid</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cac\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-58.2</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">127.4</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cb6\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-3.0</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">33.8</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"3\"class=\"cl-9a330cb7\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Test &amp; Train</span></p></td><td class=\"cl-9a330cc0\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">ALM</span></p></td><td class=\"cl-9a330cca\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330ccb\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">193.2</span></p></td><td class=\"cl-9a330cd4\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">328.7</span></p></td><td class=\"cl-9a330cca\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cd5\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">82.3</span></p></td><td class=\"cl-9a330cd4\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">106.6</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330cc0\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">EXAM</span></p></td><td class=\"cl-9a330cca\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330ccb\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-28.8</span></p></td><td class=\"cl-9a330cd4\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">132.1</span></p></td><td class=\"cl-9a330cca\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cd5\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">13.2</span></p></td><td class=\"cl-9a330cd4\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">60.2</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330cde\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Hybrid</span></p></td><td class=\"cl-9a330cdf\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330ce8\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">-16.7</span></p></td><td class=\"cl-9a330ce9\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">136.7</span></p></td><td class=\"cl-9a330cdf\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cf2\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">16.7</span></p></td><td class=\"cl-9a330ce9\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">46.5</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td  rowspan=\"3\"class=\"cl-9a330cf3\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Train Only</span></p></td><td class=\"cl-9a330ca2\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">ALM</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cac\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">194.5</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">329.2</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cb6\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">86.3</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">109.1</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330ca2\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">EXAM</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cac\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">75.3</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">199.9</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cb6\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">17.5</span></p></td><td class=\"cl-9a330cad\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">65.4</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a330cfc\"><p class=\"cl-9a330311\"><span class=\"cl-9a31a178\">Hybrid</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330cfd\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">197.5</span></p></td><td class=\"cl-9a330cfe\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">330.4</span></p></td><td class=\"cl-9a330ca3\"><p class=\"cl-9a33031a\"><span class=\"cl-9a31a178\"></span></p></td><td class=\"cl-9a330d06\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">88.3</span></p></td><td class=\"cl-9a330cfe\"><p class=\"cl-9a33031b\"><span class=\"cl-9a31a178\">110.3</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\n\n\n\n## Varied Testing Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##| column: screen-inset-right\n\n####\n\nvte <-  pluck(a_te_v, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_te_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_te_v, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nvtetr <-  pluck(a_tetr_v, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_tetr_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tetr_v, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nvtr <-  pluck(a_tr_v, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex_tr_v, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tr_v, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n\n vte/vtetr/vtr\n```\n\n::: {.cell-output-display}\n![Varied Group - Mean Model predictions vs. observations](manuscript.markdown_strict_files/figure-markdown_strict/fig-model-preds-varied-1.jpeg){#fig-model-preds-varied}\n:::\n:::\n\n::: {#tbl-e1-predsV .cell tbl-cap='Varied group - mean model predictions vs. observations. Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).'}\n\n```{.r .cell-code}\n##| column: screen-inset-right\n\n\n# Create a custom header dataframe\nheader_df <- data.frame(\n  col_keys = c(\"Fit_Method\", \"x\",\"Observed\" ,\"ALM_Predicted\", \"ALM_Residual\", \"EXAM_Predicted\",\"EXAM_Residual\", \"Hybrid_Predicted\",\"Hybrid_Residual\"),\n  line1 = c(\"\",\"\",\"\", \"ALM\", \"\", \"EXAM\", \"\", \"Hybrid\",\"\"),\n  line2 = c(\"Fit Method\", \"X\", \"Observed\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\", \"Predicted\",\"Residual\")\n)\n\n\nbest_vPreds <- vPreds %>%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |>\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |> group_by(Fit_Method,x) |>\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |> select(-abs_res)\n\nlong_vPreds <- best_vPreds |> select(-best) |>\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |>\n  relocate(Model, .after=Fit_Method) |> \n  unite(Model,Model,Model_Perf) |>\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wide <- best_vPreds |> select(-Residual,-Predicted,-Observed) |> ungroup() |>\n  pivot_wider(names_from=Model,values_from=best) |> select(ALM,EXAM,Hybrid)\n\nbest_indexV <- row_indices <- apply(best_wide, 1, function(row) {\n which(row == 1)\n})\n\n\napply_best_formatting <- function(ft, best_index) {\n  for (i in 1:length(best_index)) {\n      #ft <- ft %>% surround(i=i,j=best_index[i],border=fp_border_default(color=\"red\",width=1))\n      ind = best_index[[i]]\n      ind <- ind  %>% map_dbl(~ .x*2+3)\n      ft <- ft %>% highlight(i=i,j=ind,color=\"wheat\")\n      }\n  return(ft)\n}\n\nft <- flextable(long_vPreds) %>% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %>% \n  theme_booktabs() %>% \n  merge_v(part = \"header\") %>% \n  merge_h(part = \"header\") %>%\n  align(align = \"center\", part = \"all\") %>% \n  #autofit() %>% \n  empty_blanks() %>% \n  fix_border_issues() %>%\n  hline(part = \"header\", i = 1, j=4:9) %>%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %>%\n  hline(part = \"body\", i=c(6,12)) |> \n  bold(i=long_vPreds$x %in% c(100,350,600), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\nft <- apply_best_formatting(ft, best_indexV)\nft\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-9a77c19e{}.cl-9a7417ec{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9a741800{font-family:'Helvetica';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9a741801{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:rgba(245, 222, 179, 1.00);}.cl-9a758ac8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9a759496{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a759497{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594a0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594a1{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594a2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594aa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594ab{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594ac{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594b4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594b5{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594b6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594be{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594bf{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594c0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594c8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594c9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594ca{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594d2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594d3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594dc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594dd{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9a7594de{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-9a77c19e'><thead><tr style=\"overflow-wrap:break-word;\"><th  colspan=\"3\"class=\"cl-9a759496\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\"></span></p></th><th class=\"cl-9a7594a0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">ALM</span></p></th><th class=\"cl-9a7594a1\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\"></span></p></th><th class=\"cl-9a7594a0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">EXAM</span></p></th><th class=\"cl-9a7594a1\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\"></span></p></th><th class=\"cl-9a7594a0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Hybrid</span></p></th><th class=\"cl-9a7594a2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\"></span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-9a7594aa\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Fit Method</span></p></th><th class=\"cl-9a7594aa\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">X</span></p></th><th class=\"cl-9a7594ab\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Observed</span></p></th><th class=\"cl-9a7594ac\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Predicted</span></p></th><th class=\"cl-9a7594b4\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Residual</span></p></th><th class=\"cl-9a7594ac\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Predicted</span></p></th><th class=\"cl-9a7594b4\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Residual</span></p></th><th class=\"cl-9a7594ac\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Predicted</span></p></th><th class=\"cl-9a7594b5\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Residual</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">100</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">663</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-12</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">716</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-53</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">708</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-45</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">350</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">764</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">89</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">817</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-53</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">792</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-28</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">600</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">884</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">209</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">895</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-11</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">875</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">9</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">800</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,083</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,078</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">5</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">83</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,091</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-8</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,196</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,202</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-6</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,199</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-3</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,204</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-8</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test Only</span></p></td><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,200</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,283</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,230</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">53</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,282</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">1</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,221</span></p></td><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">62</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">100</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">663</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-12</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">716</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-53</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">707</span></p></td><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-44</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">350</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">764</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">89</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">817</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-53</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">788</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-24</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">600</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">884</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">209</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">902</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-18</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">851</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">33</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">800</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,083</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">83</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">83</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,004</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">79</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,196</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,163</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">33</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,165</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">31</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,196</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">0</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Test &amp; Train</span></p></td><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,200</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,283</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,191</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">92</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,194</span></p></td><td class=\"cl-9a7594c8\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">89</span></p></td><td class=\"cl-9a7594c9\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,227</span></p></td><td class=\"cl-9a7594c0\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">56</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">100</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">663</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-12</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">716</span></p></td><td class=\"cl-9a7594d2\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">-53</span></p></td><td class=\"cl-9a7594d3\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594ca\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-12</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">350</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">764</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">89</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">817</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-53</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">89</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741800\">600</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">884</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">209</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">905</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">-21</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">675</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">209</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">800</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,083</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">83</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">83</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">999</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">84</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,000</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,196</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,150</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">46</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,150</span></p></td><td class=\"cl-9a7594be\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">46</span></p></td><td class=\"cl-9a7594bf\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,143</span></p></td><td class=\"cl-9a7594b6\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">53</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9a7594dc\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">Train Only</span></p></td><td class=\"cl-9a7594dc\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,200</span></p></td><td class=\"cl-9a7594dd\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,283</span></p></td><td class=\"cl-9a7594de\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,180</span></p></td><td class=\"cl-9a7594dd\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">103</span></p></td><td class=\"cl-9a7594de\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,180</span></p></td><td class=\"cl-9a7594dd\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a741801\">103</span></p></td><td class=\"cl-9a7594de\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">1,176</span></p></td><td class=\"cl-9a7594dc\"><p class=\"cl-9a758ac8\"><span class=\"cl-9a7417ec\">107</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npander(tvte, caption=\"Varied fit to test only\")\npander(tvtetr,caption=\"Varied fit to train and test\")\npander(tvtr,caption=\"Varied fit to train only\")\n```\n:::\n\n\n\n\n\n\n\n\n## Constant Testing Predictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##| column: screen-inset-right\n\n####\n\ncte <-  pluck(a_te_c, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_te_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_te_c, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test Only\")\n\nctetr <-  pluck(a_tetr_c, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_tetr_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tetr_c, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") + \n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Test and Train\")\n\nctr <-  pluck(a_tr_c, \"test\") |> rename(ALM=pred,Observed=y) %>% \n  cbind(.,EXAM=pluck(ex0_tr_c, \"test\") |> pull(pred)) %>%\n  cbind(., Hybrid=pluck(hybrid_tr_c, \"test\") |> pull(pred)) |>  \n  pivot_longer(Observed:Hybrid, names_to=\"Model\", values_to = \"vx\") |> \n  ggplot(aes(x,vx,fill=Model, group=Model)) +geom_bar(position=\"dodge\",stat=\"identity\") +\n  scale_fill_manual(values=col_themes$wes2)+\n  scale_x_continuous(breaks=sort(unique(ds$x)), labels=sort(unique(ds$x)))+ylim(0,1500) +\n  theme(legend.title = element_blank(), legend.position=\"top\") +ggtitle(\"Fit to Train Only\")\n  \ncte/ctetr/ctr\n```\n\n::: {.cell-output-display}\n![Constant Group - Mean Model predictions vs. observations](manuscript.markdown_strict_files/figure-markdown_strict/fig-model-preds-constant-1.jpeg){#fig-model-preds-constant}\n:::\n:::\n\n::: {#tbl-e1-predsC .cell tbl-cap='Constant group - mean model predictions vs. observations. The X values of Extrapolation Bands are bolded. For each Modelling fitting and band combination, the model with the smallest residual is highlighted. Only the lower bound of each velocity band is shown (bands are all 200 units).'}\n\n```{.r .cell-code}\n##| column: screen-inset-right\n\n\n\nbest_cPreds <- cPreds %>%\n  pivot_longer(cols = c(ALM, EXAM, Hybrid), names_to = \"Model\", values_to = \"Predicted\") |>\n  mutate(Residual=(Observed-Predicted), abs_res =abs(Residual)) |> group_by(Fit_Method,x) |>\n  mutate(best=if_else(abs_res==min(abs_res),1,0)) |> select(-abs_res)\n\nlong_cPreds <- best_cPreds |> select(-best) |>\n  pivot_longer(cols=c(Predicted,Residual), names_to=\"Model_Perf\") |>\n  relocate(Model, .after=Fit_Method) |> \n  unite(Model,Model,Model_Perf) |>\n  pivot_wider(names_from=Model,values_from=value)\n\nbest_wideC <- best_cPreds |> select(-Residual,-Predicted,-Observed) |> ungroup() |>\n  pivot_wider(names_from=Model,values_from=best) |> select(ALM,EXAM,Hybrid)\n\nbest_indexC <- row_indices <- apply(best_wideC, 1, function(row) {\n which(row == 1)\n})\n\n\nft <- flextable(long_cPreds) %>% \n  set_header_df(\n    mapping = header_df,\n    key = \"col_keys\"\n  ) %>% \n  theme_booktabs() %>% \n  merge_v(part = \"header\") %>% \n  merge_h(part = \"header\") %>%\n  align(align = \"center\", part = \"all\") %>% \n  #autofit() %>% \n  empty_blanks() %>% \n  fix_border_issues() %>%\n  hline(part = \"header\", i = 1, j=4:9) %>%\n  vline(j=c(\"Observed\",\"ALM_Residual\",\"EXAM_Residual\")) %>%\n  hline(part = \"body\", i=c(6,12)) |> \n  bold(i=long_cPreds$x %in% c(100,350,600, 1000,1200), j=2) \n\n  # bold the cell with the lowest residual, based on best_wide df\n  # for each row, the cell that should be bolded matches which column in best_wide==1 at that row\n\nft <- apply_best_formatting(ft, best_indexC)\nft\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-9ab83a94{}.cl-9ab458a2{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9ab458b6{font-family:'Helvetica';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9ab458c0{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:rgba(245, 222, 179, 1.00);}.cl-9ab5e596{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9ab5f022{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f02c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f036{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f037{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f038{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f040{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f041{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f042{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f04a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f04b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f054{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f055{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f056{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f05e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f068{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f069{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f072{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f073{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f074{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f07c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f07d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9ab5f07e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-9ab83a94'><thead><tr style=\"overflow-wrap:break-word;\"><th  colspan=\"3\"class=\"cl-9ab5f022\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\"></span></p></th><th class=\"cl-9ab5f036\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">ALM</span></p></th><th class=\"cl-9ab5f037\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\"></span></p></th><th class=\"cl-9ab5f036\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">EXAM</span></p></th><th class=\"cl-9ab5f037\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\"></span></p></th><th class=\"cl-9ab5f036\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Hybrid</span></p></th><th class=\"cl-9ab5f038\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\"></span></p></th></tr><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-9ab5f040\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Fit Method</span></p></th><th class=\"cl-9ab5f040\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">X</span></p></th><th class=\"cl-9ab5f041\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Observed</span></p></th><th class=\"cl-9ab5f042\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Predicted</span></p></th><th class=\"cl-9ab5f04a\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Residual</span></p></th><th class=\"cl-9ab5f042\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Predicted</span></p></th><th class=\"cl-9ab5f04a\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Residual</span></p></th><th class=\"cl-9ab5f042\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Predicted</span></p></th><th class=\"cl-9ab5f04b\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Residual</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">100</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">527</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-148</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">717</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-190</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">717</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-190</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">350</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">666</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-9</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">822</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-156</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">821</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-155</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">600</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">780</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">105</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">927</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-147</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">926</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-146</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">800</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">980</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">305</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,010</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-30</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,009</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-29</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,000</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,163</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">488</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,094</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">69</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,093</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">70</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test Only</span></p></td><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,200</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,277</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">602</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,178</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">99</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,176</span></p></td><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">101</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">100</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">527</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-148</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">712</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-185</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">711</span></p></td><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-184</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">350</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">666</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-9</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">806</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-140</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">800</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-134</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">600</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">780</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">105</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">900</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-120</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">889</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-109</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">800</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">980</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">859</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">121</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">975</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">5</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">960</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">20</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,000</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,163</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">488</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,049</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">114</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,031</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">132</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Test &amp; Train</span></p></td><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,200</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,277</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">602</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,124</span></p></td><td class=\"cl-9ab5f068\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">153</span></p></td><td class=\"cl-9ab5f069\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,102</span></p></td><td class=\"cl-9ab5f05e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">175</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">100</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">527</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-148</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">697</span></p></td><td class=\"cl-9ab5f073\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-170</span></p></td><td class=\"cl-9ab5f074\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f072\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-148</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">350</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">666</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-9</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">752</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">-86</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-9</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">600</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">780</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">105</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">807</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">-27</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">105</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">800</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">980</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">851</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">129</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">851</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">129</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">833</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">147</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,000</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,163</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">488</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">895</span></p></td><td class=\"cl-9ab5f055\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">268</span></p></td><td class=\"cl-9ab5f056\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f054\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">488</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-9ab5f07c\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">Train Only</span></p></td><td class=\"cl-9ab5f07c\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458b6\">1,200</span></p></td><td class=\"cl-9ab5f07d\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">1,277</span></p></td><td class=\"cl-9ab5f07e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f07d\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">602</span></p></td><td class=\"cl-9ab5f07e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">939</span></p></td><td class=\"cl-9ab5f07d\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458c0\">338</span></p></td><td class=\"cl-9ab5f07e\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">675</span></p></td><td class=\"cl-9ab5f07c\"><p class=\"cl-9ab5e596\"><span class=\"cl-9ab458a2\">602</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n# EXAM fit learning curves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.content-visible unless-format=\"pdf\"}\n# References\n\n::: {#refs}\n:::\n\n:::\n\n# Appendix\n\n\n### Appendix - Project 1 \n\n\n\n\n---\n# title: \"R Notebook\"\n# output: html_notebook\nexecute: \n  warning: false\n  eval: true\n---\n\n\n\n\n\n\n\n\n\nReviewer #2: This study addresses a question that is important both theoretically and practically. However, the authors need to rule out the following, less interesting alternative. Namely, the results could be due to task practice effect, as follows.\n\nSince there was no pre-training test, and no practice trials (as far as I can tell), and since the task was an online motor task that participants could not rely on their prior motor experience, trying to launch the ball to the target could only be done via trial and error. For the varied training group, they got to practice at two distances. Therefore, they had a better \"calibration\" in terms of the relationship between launching speed and target distance. This was likely beneficial both in Exp.1 when both transfer distances were interpolations from the two trained distances, and in Exp.2 when two transfer distances were interpolations and two were extrapolations but the latter two were immediately next to the training distances.\n\nIn comparison, since the constant group trained at only a single distance, any transfer distance (or at least the first transfer distance tested) was extrapolation even if this transfer distance was shorter than the trained, because the participants did not know beforehand how to shoot the ball to the shortest distance due to the existence of the barrier. If the transfer distance was longer, for sure that was extrapolation.\n\n\nRegardless, the above analysis suggests that the constant group would always be a step behind the varied group. The number of trials at each transfer distance may not be sufficient for them to catch up the varied group either (whether there was learning during testing should be checked). If such disadvantage for the constant group is indeed due to the lack of tryout opportunities, then the authors should verify whether the same results still hold if all groups were provided opportunities to practice, or if pre-training tests across all distances were offered.\n\n\n### exponential learning models fit to individual subjects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne2TestFits <- readRDS(here::here('data/IGAS-e2TestFits-April4.rds'))\ne2TestFits <- e2TestFits %>% mutate(Asymptote.Minus.Start=pAsym-pStart)\nexp2.fit2 <- e2TestFits %>% ungroup() %>% group_by(sbjCode,conditType) %>%\n  summarise(MeanAsym=mean(pAsym),MeanStart=mean(pStart),\n            MeanRate=mean(pRate),\n            asymMinusStart=mean(Asymptote.Minus.Start),.groups=\"keep\") %>% \n  ungroup() %>% as.data.frame()\n```\n:::\n\n\n\n\n### Group comparison of learning rate fits\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mr1=e2TestFits %>% anova_test(dv=pRate,between=conditType,wid=sbjCode,within=positionX,type=3);show(mr1)\n# mr2=exp2.fit2 %>% anova_test(dv=MeanRate,between=conditType,wid=sbjCode,type=3);show(mr2) \n\n# h4<-e2TestFits %>% ggplot(aes(x=positionX,y=pRate,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ggtitle(\"Learning rates \")+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)+ylab(\"Inverse Learning Rate\")+xlab(\"Testing Location\")\n# h4\n\n\n# mr1=e2TestFits %>% filter(converged==TRUE)%>% anova_test(dv=pRate,between=conditType,wid=sbjCode,within=positionX,type=3);show(mr1)\n# mr2=exp2.fit2 %>% anova_test(dv=MeanRate,between=conditType,wid=sbjCode,type=3);show(mr2) \n\nh4<-e2TestFits %>%filter(Pval<.4)%>% ggplot(aes(x=positionX,y=pRate,fill=conditType))+geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+ggtitle(\"Learning rates \")+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)+ylab(\"Inverse Learning Rate\")+xlab(\"Testing Location\")\nh4\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-59-1.jpeg)\n:::\n:::\n\n\n\n\n\n### First vs. second half of testing stage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntestSplit <- transfer %>%\n  group_by(sbjCode, conditType, positionX, testHalfSbj) %>%\n  summarise(\n    MeanTargetDistance = mean(AbsDistFromCenter),\n    MeanScaledDev =mean(scaledDev,trim=.05),\n    .groups = \"keep\"\n  ) %>% \n  as.data.frame()\n\ntestSplit2 <- transfer %>%\n  group_by(sbjCode, conditType, testHalfSbj) %>%\n  summarise(\n    MeanTargetDistance = mean(AbsDistFromCenter, trim = .01),\n    MeanScaledDev =\n      mean(scaledDev, trim = .05),\n    .groups = \"keep\"\n  ) %>% as.data.frame()\n\n\ntsw <- testSplit %>% ungroup() %>% \n  pivot_wider(names_from = testHalfSbj,values_from=c(MeanTargetDistance,MeanScaledDev)) %>%\n  mutate(endMinusStart = `MeanTargetDistance_2nd-Half` - `MeanTargetDistance_1st-Half`,\n    endMinusStartScaled = `MeanScaledDev_2nd-Half` - `MeanScaledDev_1st-Half`) %>% \n    as.data.frame()\n\ntsw2 <- tsw %>% \n  group_by(sbjCode,conditType) %>% summarise(endMinusStart=mean(endMinusStart),endMinusStartScaled=mean(endMinusStartScaled)) %>% as.data.frame()\n\n# testSplit %>% ggplot(aes(x=testHalfSbj,y=MeanTargetDistance))+\n#   geom_bar(aes(group=conditType,fill=conditType),stat=\"summary\",position=dodge)+\n#   facet_wrap(~positionX,ncol=2)+\n#   stat_summary(aes(x=testHalfSbj,group=conditType),fun.data=mean_se,geom=\"errorbar\",position=dodge)\n\n\nh1=testSplit %>% filter(testHalfSbj==\"1st-Half\") %>% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n geom_bar(aes(group=conditType,fill=conditType),stat=\"summary\",position=dodge,fun=\"mean\")+stat_summary(aes(x=positionX,group=conditType),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n scale_y_continuous(name=\"Mean Absolute Deviation From Target\",limits=c(0,400))+ \n ggtitle(\"Testing - 1st half\")+\n  ylab(\"Mean Absolute Deviation From Target\")+xlab(\"Testing Location\")+theme(legend.position=\"top\")\n\nh2=testSplit %>% filter(testHalfSbj==\"2nd-Half\") %>% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n geom_bar(aes(group=conditType,fill=conditType),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=positionX,group=conditType),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8) + \n scale_y_continuous(name=\"\",limits=c(0,400))+ \n ggtitle(\"Testing - 2nd half\")+\n  xlab(\"Testing Location\")+\n  theme(plot.title = element_text(hjust = 0.5))+\n  theme(legend.position=\"top\") +\n  guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.25)\n\n#egg::ggarrange(h1,h2,ncol=2)\n\n\nh3 <- tsw %>% ggplot(aes(x=positionX,y=endMinusStart))+\n  geom_bar(aes(group=conditType,fill=conditType),stat=\"summary\",position=dodge,fun=\"mean\")+\n  stat_summary(aes(x=positionX,group=conditType),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"2nd Half Deviation - 1st Half Deviation\")+ \n  ggtitle(\"Improvement Per Location\")+\n  xlab(\"Testing Location\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Testing Location\")) +theme(legend.position=\"none\")\n\n#ggarrange(h1,h2,h3,h4,ncol=2)\n\n#(h1 + h2) / (h3)\n\n(h1+h2)/h3\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-60-1.jpeg)\n:::\n:::\n\n\n\n\n\n\n\n\n\n### Group Comparison for asymptote-starting performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ma1=e2TestFits %>% filter()%>% anova_test(dv=Asymptote.Minus.Start,between=conditType,wid=sbjCode,within=positionX,type=3);show(ma1)\n# ma2=exp2.fit2 %>% anova_test(dv=asymMinusStart,between=conditType,wid=sbjCode,type=3);show(ma2) \n\ne2TestFits %>% ggplot(aes(x=conditType,y=Asymptote.Minus.Start,fill=conditType))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  facet_grid(~positionX)+ggtitle(\"e2 testing (asymptote - start performance)\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-61-1.jpeg)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n### Relative distance and under/overshooting\n\nReviewer 3\nAbsolute versus relative distance: From a methodological standpoint, I understand the need to differentiate these two types of distance. However, from a theoretical perspective there may be some issue in differentiating these two concepts. Schema theory relies on relative (or invariant) information to inform the motor program. However, both distances would be important to an instance or exemplar representation. You may want to consider commenting on this issue. \n\n\nReviewer 2\nFor the same reason, the plots showing improvement during training could be due to participants learning the task, rather than fine motor skills. Although task learning and motor learning are impossible to separate cleanly, the common practice in the field is indeed to offer practice trials to reduce the task learning aspects. The authors should address this.\n\nIn addition to absolute errors (which is related to variance), the authors should also provide other measures of performance, e.g., the mean of the signed errors, so that readers have a better idea whether there was any meaningful over- or undershooting.\n\n\n#### experiment 1 training - relative distances\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-62-1.jpeg)\n:::\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-62-2.jpeg)\n:::\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-62-3.jpeg)\n:::\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-62-4.jpeg)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=========================================================================\nconditType devianceDirection      610            760            910      \n-------------------------------------------------------------------------\nconstant       Overshoot                    311.84(307.92)               \nconstant      Undershoot                    188.05(163.62)               \nvaried         Overshoot     211.69(234.97)                360.14(322.01)\nvaried        Undershoot     107.35(81.21)                 244.85(196.47)\n-------------------------------------------------------------------------\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======================================================\nconditType      610           760            910      \n------------------------------------------------------\nconstant                 121.03(269.17)               \nvaried     39.91(178.12)                150.53(290.04)\n------------------------------------------------------\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n====================================================================\nconditType     610           760            835            910      \n--------------------------------------------------------------------\nconstant   7.13(124.02) 107.02(218.49) 142.42(252.34) 122.92(282.58)\nvaried     3.19(96.67)   92.1(173.9)   103.84(214.4)  108.12(234.59)\n--------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n#### experiment 2 training - relative distances\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-63-1.jpeg)\n:::\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-63-2.jpeg)\n:::\n:::\n\n\n\n\n#### Experiment 1 Testing - relative distances\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-64-1.jpeg)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n====================================================================================================================================\nconditType2         msdu_610       msdu_760       msdu_835       msdu_910      msds_610      msds_760      msds_835      msds_910   \n------------------------------------------------------------------------------------------------------------------------------------\nConstant Training 136.27(84.29) 191.65(112.65) 219.46(139.91) 276.75(153.09) 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   105.12(51.39)  149.37(93.4)  180.54(129.52) 198.64(137.84) 13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n------------------------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=========================================================================\nCondition              610           760           835           910     \n-------------------------------------------------------------------------\nConstant Training 25.28(158.98) 50.82(217.48) 73.14(250.93) 50.76(313.77)\nVaried Training   13.85(116.87) 50.59(169.59) 50.52(217.39) 49.94(237.71)\n-------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Experiment 2 Testing - relative distances\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-65-1.jpeg)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n#### Experimenet 1 - intermittent testing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintTest.half <- readRDS(here::here(\"data/e1_intTest.rds\"))\n\nintTest.half %>% ggplot(aes(x=positionX,y=MeanTargetDistance))+\n  geom_bar(aes(group=trainHalf,fill=trainHalf),stat=\"summary\",fun=mean,position=dodge)+\n  facet_wrap(~conditType,ncol=2)+\n  stat_summary(aes(x=positionX,group=trainHalf),fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.8)+\n  ylab(\"Mean Distance From Center Of Target\")+\n  xlab(\"Intermittent Testing Throw Location\")+theme(plot.title = element_text(hjust = 0.5))+\n  guides(fill=guide_legend(title=\"Training Stage\"))+theme(legend.title.align=.25)\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-66-1.jpeg)\n:::\n\n```{.r .cell-code}\ncnames=c(\"Condition\",\"610_First Half\",\"760_First Half\",\"910_First Half\",\"610_Second Half\",\"760_Second Half\",\"910_Second Half\")\ntest= intTest.half %>% rename(Condition=\"conditType\") %>% group_by(Condition,trainHalf,positionX) %>% \n  summarise(Mean=round(mean(MeanTargetDistance),2),sd=round(sd(MeanTargetDistance),2)) \ntest=test %>% group_by(Condition) %>% mutate(msd=paste(Mean,\"(\",sd,\")\",sep=\"\")) %>%\n select(Condition,positionX,trainHalf,msd)%>% pivot_wider(names_from = c(positionX,trainHalf),values_from=c(msd))\ntest=test %>% as.data.frame()\ncolnames(test) <- cnames\nstargazer(test,type=\"text\",summary=FALSE,rownames=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======================================================================================================\nCondition 610_First Half 760_First Half 910_First Half 610_Second Half 760_Second Half 910_Second Half\n------------------------------------------------------------------------------------------------------\nconstant  206.64(82.08)  286.51(121.07) 406.93(145.2)   187.2(55.24)    238.21(95.16)  313.27(114.86) \nvaried    195.68(78.58)  278.9(105.37)  318.53(134.81)  177.79(70.82)  224.98(108.04)   276.86(110.5) \n------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Training plots - Experiment 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# possible that scaling required loading special package from devtools\nexp1Train <- e1 %>% filter(stage!=\"Transfer\",mode==1) %>% group_by(Group,sbjCode) %>%mutate(scaleDev=scale_this(AbsDistFromCenter)) %>%ungroup() %>% group_by(Group,sbjCode,stage,conditType)\nexp1Train = exp1Train %>% summarise(MeanTargetDistance=mean(AbsDistFromCenter),scaledDist=mean(scaleDev,trim=.05))\nexp1Train$stage <- factor(exp1Train$stage, levels = c(\"Beginning\", \"Middle\", \"End\")) #in case the levels get out of order\nexp1TrainTrials <- e1 %>% filter(stage!=\"Transfer\",mode==1,trialType!=44) %>% group_by(Group,sbjCode,positionX) %>% mutate(scaleDev=scale_this(AbsDistFromCenter),ind=1,trainIndex=cumsum(ind)) %>%ungroup() %>% group_by(Group,sbjCode,stage,conditType)\n\n\n\n# manuscript plot - original\nggplot(data = exp1Train, aes(x=stage, y=MeanTargetDistance)) + geom_boxplot(aes(fill=conditType),position=position_dodge(1))+stat_summary(fun=\"mean\",aes(group=conditType),position=position_dodge(1))+\nylab(\"Mean Distance From Center Of Target\") +xlab(\"Training Stage\")+theme(plot.title = element_text(hjust = 0.5))+guides(fill=guide_legend(title=\"Training Condition\"))+theme(legend.title.align=.5)+theme_classic()\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-67-1.jpeg)\n:::\n\n```{.r .cell-code}\nlineplot.CI(data=exp1Train,x.factor=stage,group=conditType,response=scaledDist,xlab=\"Training Stage\",x.leg=2,legend=TRUE,ylab=\"Distance from Target (scaled)\",main=\"Training Performance - Experiment 1\",col=c(\"red\",\"black\"))\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-67-2.jpeg)\n:::\n\n```{.r .cell-code}\nlineplot.CI(data=exp1Train,x.factor=stage,group=conditType,response=MeanTargetDistance,xlab=\"Training Stage\",x.leg=2,legend=TRUE,ylab=\"Distance From Target\",main=\"Training Performance - Experiment 1\",col=c(\"red\",\"blue\"))\n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-67-3.jpeg)\n:::\n:::\n\n\n\n\n\n\n\n\n#### Not in manuscript #####\n\n\n#### fit to testing performance averaged across positions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne2Fits.AggPos <- readRDS(here::here('data/IGAS-e2Fits.AggPos-April_12.rds'))\n\ne2Fits.AggPos %>% ggplot(aes(x=Group,y=pStart,fill=Group))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  ggtitle(\"experiment 2 - starting performance per position\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-68-1.jpeg)\n:::\n\n```{.r .cell-code}\ne2Fits.AggPos %>% ggplot(aes(x=Group,y=pAsym,fill=Group))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  ggtitle(\"e2 testing performance asymptote per position \") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-68-2.jpeg)\n:::\n\n```{.r .cell-code}\ne2Fits.AggPos %>% ggplot(aes(x=Group,y=pRate,fill=Group))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  ggtitle(\"e2 testing performance asymptote per position \") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-68-3.jpeg)\n:::\n\n```{.r .cell-code}\n#mr1=e2Fits.AggPos %>% anova_test(dv=pRate,between=conditType,wid=sbjCode,type=3);show(mr1)\n```\n:::\n\n\n\n\n\n\n#### statistical tests for starting performance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nms1=e2TestFits %>% filter(converged==TRUE) %>% anova_test(dv=pStart,between=conditType,wid=sbjCode,within=positionX,type=3);\nms2=exp2.fit2 %>% anova_test(dv=MeanStart,between=conditType,wid=sbjCode,type=3);\n#ms1\nms2 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p<.05   ges\n1 conditType   1 206 3.04 0.083       0.015\n```\n\n\n:::\n\n```{.r .cell-code}\ne2TestFits %>% ggplot(aes(x=conditType,y=pStart,fill=conditType))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  facet_grid(~positionX)+ggtitle(\"experiment 2 - starting performance per position\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-69-1.jpeg)\n:::\n:::\n\n\n\n\n\n\n#### statistical tests for asymptote\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma1=e2TestFits %>% filter(converged==TRUE)%>% anova_test(dv=pAsym,between=conditType,wid=sbjCode,within=positionX,type=3);\nma2=exp2.fit2 %>% anova_test(dv=MeanAsym,between=conditType,wid=sbjCode,type=3);\nma2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nANOVA Table (type III tests)\n\n      Effect DFn DFd    F     p p<.05   ges\n1 conditType   1 206 3.38 0.067       0.016\n```\n\n\n:::\n\n```{.r .cell-code}\ne2TestFits %>% ggplot(aes(x=conditType,y=pAsym,fill=conditType))+\n  geom_bar(stat=\"summary\",position=dodge,fun=\"mean\")+ \n  stat_summary(fun.data=mean_se,geom=\"errorbar\",position=dodge,width=.5)+\n  facet_grid(~positionX)+ggtitle(\"e2 testing performance asymptote per position \") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) \n```\n\n::: {.cell-output-display}\n![](manuscript.markdown_strict_files/figure-markdown_strict/unnamed-chunk-70-1.jpeg)\n:::\n:::\n\n{{< pagebreak >}}\n\n\n\n\n\n\n\n\n\n### Appendix - Project 2 - Experiment 1 \n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# print(getwd())\n# here::set_here(path='..')\n# print(getwd())\nsource(here::here(\"Functions\", \"packages.R\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- readRDS(here::here(\"data/e1_08-21-23.rds\")) |> filter(expMode2 == \"Test\")  |>\n  select(id,condit,bandInt,vb,vx,dist,sdist,bandType,tOrder)\n```\n:::\n\n\n\n\n\n\n\n\n\n#### Posterior Predictive Distributions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_pred <- \n  posterior_predict(e1_distBMM, ndraws = 500) |> \n  array_branch(margin=1) |> \n   map_dfr( \n    function(yrep_iter) {\n      test  |>\n        mutate(dist_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) |>\n  mutate(iter = as.numeric(iter))\n\n\n\ndist_pred  |>\n  filter(iter < 100) %>%\n  ggplot(aes(dist_pred, group = iter)) +\n  geom_line(alpha = .03, stat = 'density', color = 'blue') +\n  geom_density(data = test,\n               aes(dist,col=vb),\n               inherit.aes = FALSE,\n               size = 0.7) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('Deviation')\n```\n\n::: {.cell-output-display}\n![Posterior Predictive distributions for Absolute Deviance. Posterior Draws in Blue, colored lines are empirical data.](manuscript.markdown_strict_files/figure-markdown_strict/fig-post-pred-dist-1.jpeg){#fig-post-pred-dist}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvx_pred <- \n  posterior_predict(e1_vxBMM, ndraws = 500) |> \n  array_branch(margin=1) |> \n   map_dfr( \n    function(yrep_iter) {\n      test  |>\n        mutate(vx_pred = yrep_iter)\n    },\n    .id = 'iter'\n  ) |>\n  mutate(iter = as.numeric(iter))\n\n\n\nvx_pred  |>\n  filter(iter < 100) %>%\n  ggplot(aes(vx_pred, group = iter)) +\n  geom_line(alpha = .03, stat = 'density', color = 'blue') +\n  geom_density(data = test,\n               aes(vx,col=vb),\n               inherit.aes = FALSE,\n               size = 0.7) + # 1\n  facet_grid(condit ~ vb) +\n  xlab('Vx')\n```\n\n::: {.cell-output-display}\n![Posterior Predictive distributions for Vx. Posterior Draws in Blue, colored lines are empirical data.](manuscript.markdown_strict_files/figure-markdown_strict/fig-post-pred-vx-1.jpeg){#fig-post-pred-vx}\n:::\n:::\n\n\n\n\n\n#### Empirical vs. Predicted\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n{\nvx_pred  |>\n  filter(iter < 100)  |> group_by(id,condit,vb,iter) |>\n  summarise(vx_pred=mean(vx_pred)) %>%\n  ggplot(aes(x=vb,y=vx_pred,fill=condit)) + \n  geom_flat_violin( position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,\n                   trim = FALSE, alpha = .5, colour = NA) +\n  # geom_point(aes(x = as.numeric(vb) - 0.15, y = vx_pred, colour = vb),\n  #            position = position_jitter(width = 0.05, height = 0),\n  #            size = 1, shape = 20) +\n  geom_boxplot(aes(x = vb, y = vx_pred, fill = condit),\n               outlier.shape = NA,\n               alpha = 0.5,\n               width = 0.1,\n               colour = \"black\") +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Predicted Vx\")  \n} / {\nvx_pred  |>\n  filter(iter < 2)  |> group_by(id,condit,vb) |>\n  summarise(vx=mean(vx)) %>%\n  ggplot(aes(x=vb,y=vx,fill=condit)) + \n  geom_flat_violin( position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,\n                   trim = FALSE,\n                   alpha = .5,\n                   colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15,col=condit),\n             # position = position_jitter(width = 0.05),\n             position = position_jitter(width = 0.05, height = 0),\n             size = 1,\n             shape = 20) +\n  geom_boxplot(\n               outlier.shape = NA,\n               alpha = 0.5,\n               width = 0.1,\n               colour = \"black\") +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Vx\") \n}\n```\n\n::: {.cell-output-display}\n![Bayesian Mixed Model predictions vs. Empirical Predictions - X velocity](manuscript.markdown_strict_files/figure-markdown_strict/fig-empVsPred-1.jpeg){#fig-empVsPred}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n#### Different Aggregations\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepId <- dist_pred  |>\n  filter(iter < 2)  |> group_by(id,condit,vb) |>\n  summarise(dist=median(dist)) |>\n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  geom_flat_violin(aes(fill=condit), position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,trim = FALSE, alpha = .5, colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15, col=condit),\n             position = position_jitter(width = 0.05, height = 0),\n             size = 1, shape = 20, alpha=.7) +\n  geom_boxplot(aes(x=vb,y=dist,fill=condit),\n               outlier.shape = NA,\n               alpha = 0.5, width = 0.1) +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Deviation - Subject level averaging\") \n\n\n\nepId \n```\n\n::: {.cell-output-display}\n![E1. Distribution of Vx  at Participant level](manuscript.markdown_strict_files/figure-markdown_strict/fig-empirical-distGrp1-1.jpeg){#fig-empirical-distGrp1}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nepTrial <- dist_pred  |>\n  filter(iter < 2)  |> group_by(id,condit,vb) |>\n  ggplot(aes(x=vb,y=dist,fill=condit)) + \n  geom_flat_violin(aes(fill=condit), position = position_nudge(x = 0.1, y = 0),\n                   adjust = 1.5,trim = FALSE, alpha = .5, colour = NA) +\n  geom_point(aes(x = as.numeric(vb) - 0.15, col=condit),\n             position = position_jitter(width = 0.05, height = 0),\n             size = .5, shape = 20, alpha=.7) +\n  geom_boxplot(aes(x=vb,y=dist,fill=condit),\n               outlier.shape = NA,\n               alpha = 0.5, width = 0.1) +\n  geom_hline(yintercept = 0,\n             linetype = 'dashed',\n             color = 'red',\n             size = 0.4) + \n  coord_flip() + ggtitle(\"Empirical Deviation - Raw Trial\") +\n   theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank())\n\nepTrial\n```\n\n::: {.cell-output-display}\n![E1. Distribution of Vx at Trial level](manuscript.markdown_strict_files/figure-markdown_strict/fig-empirical-distGrp2-1.jpeg){#fig-empirical-distGrp2}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data_grid=map_dfr(1, ~data.frame(unique(test[,c(\"id\",\"condit\",\"bandInt\")])))\n\ncSamp <- e1_distBMM  |> \n  emmeans(\"condit\",by=\"bandInt\",at=list(bandInt=c(100,350,600,800,1000,1200)),\n          epred = TRUE, re_formula = NA) |> \n  pairs() |> gather_emmeans_draws()  |>\n  group_by(contrast, .draw,bandInt) |> summarise(value=mean(.value), n=n())\n\n\n ameBand <- cSamp |> ggplot(aes(x=value,y=\"\")) + \n  stat_halfeye() + \n  geom_vline(xintercept=0,alpha=.4)+\n  facet_wrap(~bandInt,ncol=1) + labs(x=\"Marginal Effect (Constant - Varied)\", y= NULL)+\n  ggtitle(\"Average Marginal Effect\")\n\nbothConditGM <- e1_distBMM %>%\n  epred_draws(newdata = new_data_grid,ndraws = 2000, re_formula = NA) |>\n  ggplot(aes(x=.epred,y=\"Mean\",fill=condit)) + \n  stat_halfeye() +facet_wrap(~bandInt, ncol = 1) +\n  labs(x=\"Predicted Deviation\", y=NULL)+\n  ggtitle(\"Grand Means\") +theme(legend.position = \"bottom\")\n\n(bothConditGM | ameBand) + plot_layout(widths=c(2,1.0))\n```\n\n::: {.cell-output-display}\n![E1. Predicted Means Per Condition and Band, and Average Marginal Effect (Constant - Varied)](manuscript.markdown_strict_files/figure-markdown_strict/fig-e1-ame-1.jpeg){#fig-e1-ame}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n# References\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"manuscript_files/libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"manuscript_files/libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}