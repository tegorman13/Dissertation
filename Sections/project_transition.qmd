

In project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model.

Optional: - emphasize that both tasks involve extrapolation - it's just emphasized more in Project 2, and project 2 is designed to more closely mirror tasks used in the function learning literature.


The HTT and HTW tasks also differed in terms of general task complexity. The HTT task was designed to mimic projectile launching tasks commonly employed in visuomotor learning studies, and the parabolic trajectories necessary to strike the target in HTT were sensitive to both the x and y dimensions of the projectile's velocity (and to a lesser extent, the position within the launching box at which the ball was released). Conversely, the HTW task was influenced to a greater extent by the tasks commonly utilized in the function learning literature, wherein the correct output responses are determined by a single input dimension. In HTW, the task space is also almost perfectly smooth, at least for the continuous feedback subjects; if they throw 100 units too hard, they'll be told that they were 100 units too hard. In contrast, in HTT, it was possible to produce xy velocity combinations that were technically closer to the empirical solution space than other throws but resulted in worse feedback due to striking the barrier.


In Project 2, a modified version of the task from Project 1 is used in conjunction with a testing procedure that challenges participants to extrapolate well beyond their training experience. In line with previous research in the function learning literature, participants show evidence of successful extrapolation in our linear task environment. Surprisingly though, the constant training group outperforms the varied training group consistently across numerous variants of the task. Such a pattern is far from unheard of in the vast literature on training variability, and it is therefore remains a worthwhile challenge to evaluate the ability of similarity-based models to account for the observed effects. Additionally, the cognitive process models implemented for project 2 will go beyond the modelling efforts of the previous project in two respects. 1) Extensions that enable the model to produce predictions of participant responses, and 2) fitting and attempting to account for behavior in both training AND testing phases of the experiment.

In project 1, we applied model-based techniques to quantify and control for the similarity between training and testing experience, which in turn enabled us to account for the difference between varied and constant training via an extended version of a similarity based generalization model. In project 2, we will go a step further, implementing a full process model capable of both 1) producing novel responses and 2) modeling behavior in both the learning and testing stages of the experiment. Project 2 also places a greater emphasis on extrapolation performance following training - as varied training has often been purported to be particularly beneficial in such situations. Extrapolation has long been a focus of the literature on function learning (Brehmer 1974; Carroll 1963). Central questions of the function learning literature have included the relative difficulties of learning various functional forms (e.g. linear vs.bilinear vs. quadratic), and the relative effectiveness of rule-based vs. association-based exemplar models vs. various hybrid models (Bott and Heit 2004; DeLosh, McDaniel, and Busemeyer 1997; Jones et al. 2018; Kalish, Lewandowsky, and Kruschke 2004; M. A. Mcdaniel and Busemeyer 2005; M. Mcdaniel et al. 2009). However the issue of training variation has received surprisingly little attention in this area.
