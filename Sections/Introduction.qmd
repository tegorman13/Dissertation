---
title: Introduction
author:
- name: Thomas E. Gorman
  affiliations: 
  - name:  Department of Psychological and Brain Sciences, Indiana University-Bloomington, USA
    affiliation-url: https://psych.indiana.edu/
  - name:  Program in Cognitive Science, Indiana University-Bloomington, USA
    affiliation-url: https://cogs.indiana.edu/index.html
  url: https://tegorman13.github.io/
floatsintext: true
suppress-author-note: true
format:
  apaquarto-html: default
  hugo-md:
    echo: false
    output: false
    html-math-method: mathjax
    output-file: d_intro_hugo.md
  gfm:
    echo: false
    output-file: d_into_gfm.md
  apaquarto-pdf: default
  # hikmah-manuscript-pdf:
  #   echo: false
  #   output-file: "intro_pdf.pdf"
  #   mainfont: "Linux Libertine O"
  #   mainfontoptions:
  #     - "Numbers=Proportional"
  #     - "Numbers=OldStyle"
  #   mathfont: "Libertinus Math"
---


The impact of training variability on generalization has been a long-standing topic in the study of human learning, with conflicting evidence about its potential benefits. This dissertation addresses these ambiguities by examining the effects of varied versus constant training in visuomotor skill learning through a combination of experimental and computational modeling approaches. Across two projects, we systematically compare varied training (multiple items) to constant training (single item) in a projectile-throwing task. Empirical findings reveal both positive and negative impacts of variability, highlighting the complex interplay between training conditions and generalization performance. To provide a theoretical account of these findings, this dissertation employs both instance-based and connectionist computational modeling approaches. The instance-based modeling approach introduced in project 1 provides a theoretically justifiable method of quantifying/controlling for similarity between training and testing conditions, while also demonstrating that varied training may induce broader generalization in the similarity function relating training and test items. In project 2, the Extrapolation-Association Model (EXAM) provided the best account of the testing data across all experiments, capturing the
constant groups' ability to extrapolate to novel regions despite limited training experience, while also revealing potential detriments of varied training for simple extrapolation tasks. These results challenge simplistic notions about the universality of variability benefits in training and emphasize the need for tailored approaches that consider both the structure of the task environment and the prior knowledge of the learners.


## Varied Training and Generalization

Varied training has been shown to influence learning in a wide array of different tasks and domains, including categorization [@hahnEffectsCategoryDiversity2005; @maddoxStimulusRangeDiscontinuity2011; @posnerGenesisAbstractIdeas1968; @nosofskyModelguidedSearchOptimal2019; @morgensternOneshotCategorizationNovel2019; @plebanekEffectsFrequencyVariability2021], language learning [@jonesDensityDistinctivenessEarly2020; @perryLearnLocallyThink2010; @twomeyAllRightNoises2018; @wonnacottInputEffectsAcquisition2012; @brekelmansDoesHighVariability2022], pattern and anagram completion tasks [@goodeSuperiorityVariableRepeated2008; @zhangHighVariabilityLearning2024], perceptual learning [@manentiVariabilityTrainingUnlocks2023; @lovibondStimulusDiscriminabilityInduction2020; @robsonSpecificVariedPractice2022a; @zamanPerceptualVariabilityImplications2021], trajectory extrapolation [@fulvioTaskSpecificResponseStrategy2014], cognitive control tasks [@sabahWhenLessMore2019; @moshon-cohenStimulusVariabilityImproves2024], associative learning [@leeEvidentialDiversityIncreases2019; @reichmannVariabilityAbstractionEvaluative2023; @fanStimulusDiversityIncreases2022; @pradaExperiencedCategoryVariability2020; @liveseyRevisitingPeakShift2019; @ramAnticipatedVariabilityIncreases2024], visual search [@georgeStimulusVariabilityTask2021; @gonzalezDiversityTrainingEnhances2011; @kelleyLearningAttendEffects2009], voice identity learning [@lavanEffectsHighVariability2019], face recognition [@honigPerceptualSimilarityModulates2022; @burtonIdentityVariationRepresentations2016; @menonVariationPhotosSame2015], the perception of social group heterogeneity [@linvilleExemplarAbstractionModels1993; @konovalovaInformationSamplingExplanation2020; @parkPerceptionVariabilityCategory1987; @gershmanStructureLearningPrinciples2023], simple motor learning [@braunMotorTaskVariation2009; @velazquez-vargasRoleTrainingVariability2024; @rollerVariablePracticeLenses2001; @willeyLimitedGeneralizationVaried2018], sports training [@greenPracticeVariabilityTransfer1995a; @northEffectConsistentVaried2019; @breslinConstantVariablePractice2012], and complex skill learning [@seowTransferEffectsVaried2019; @huetEducationAttentionExplanation2011; @hacquesVisualControlClimbing2022]. See @czyzVariabilityPracticeInformation2021 and @ravivHowVariabilityShapes2022 for more detailed reviews.

Research on the effects of varied training typically manipulates variability in one of two ways. In the first approach, a high variability group is exposed to a greater number of unique instances during training, while a low variability group receives fewer unique instances with more repetitions. Alternatively, both groups may receive the same number of unique instances, but the high variability group's instances are more widely distributed or spread out in the relevant psychological space, while the low variability group's instances are clustered more tightly together. Researchers then compare the training groups in terms of their performance during the training phase, as well as their generalization performance during a testing phase. Researchers usually compare the performance of the two groups during both the training phase and a subsequent testing phase. The primary theoretical interest is often to assess the influence of training variability on generalization to novel testing items or conditions. However, the test may also include some or all of the items that were used during the training stage, allowing for an assessment of whether the variability manipulation influenced the learning of the trained items themselves, or to easily measure how much performance degrades as a function of how far away testing items are from the training items.

The influence of training variability has received a large amount of attention in the domain of sensorimotor skill learning. Much of this research has been influenced by the work of @schmidtSchemaTheoryDiscrete1975, who proposed a schema-based account of motor learning as an attempt to address the longstanding problem of how novel movements are produced. Schema theory presumes that learners possess general motor programs for a class of movements (e.g., an underhand throw). When called up for use motor programs are parameterized by schema rules which determine how the motor program is parameterized or scaled to the particular demands of the current task. Schema theory predicts that variable training facilitates the formation of more robust schemas, which will result in improved generalization or transfer. Experiments that test this hypothesis are often designed to compare the transfer performance of a constant-trained group against that of a varied-trained group. Both groups train on the same task, but the varied group practices with multiple instances along some task-relevant dimension that remains invariant for the constant group. For example, studies using a projectile throwing task might assign participants to either constant training that practice throwing from a single location, or to a varied group that throws from multiple locations. Following training, both groups are then tested from novel throwing locations [@pigottMotorSchemaStructure1984; @willeyLimitedGeneralizationVaried2018; @pachecoLearningSpecificIndividual2018; @wulfEffectTypePractice1991].

One of the earliest and still often cited investigations of Schmidt's benefits of variability hypothesis was the work of @kerrSpecificVariedPractice1978. Two groups of children, aged 8 and 12, were assigned to either constant or varied training of a bean bag throwing task. The constant group practiced throwing a bean-bag at a small target placed 3 feet in front of them, and the varied group practiced throwing from a distance of both 2 feet and 4 feet (see @fig-ex-design1). Participants were blindfolded and unable to see the target while making each throw but would receive feedback by looking at where the beanbag had landed in between each training trial. 12 weeks later, all of the children were given a final test from a distance of 3 feet which was novel for the varied participants and repeated for the constant participants. Participants were also blindfolded for testing and did not receive trial by trial feedback in this stage. In both age groups, participants performed significantly better in the varied condition than the constant condition, though the effect was larger for the younger, 8-year-old children. This result provides particularly strong evidence for the benefits of varied practice, as the varied group outperformed the constant group even when tested at the "home-turf" distance that the constant group had exclusively practiced. A similar pattern of results was observed in another study wherein varied participants trained with tennis, squash, badminton, and short-tennis rackets were compared against constant subjects trained with only a tennis racket [@greenPracticeVariabilityTransfer1995a]. One of the testing conditions had subjects repeat the use of the tennis racket, which had been used on all 128 training trials for the constant group, and only 32 training trials for the varied group. Nevertheless, the varied group outperformed the constant group when using the tennis racket at testing, and also performed better in conditions with several novel racket lengths. However, as is the case with many of the patterns commonly observed in the "benefits of variability" literature, the pattern wherein the varied group outperforms the constant group even from the constants group's home turf has not been consistently replicated. One recent study attempted a near replication of the  Kerr & Booth study [@willeyLongtermMotorLearning2018], having subjects throw beanbags at a target, with the varied group training from positions (5 and 9 feet) on either side of the constant group (7 feet). This study did not find a varied advantage from the constant training position, though the varied group did perform better at distances novel to both groups. However, this study diverged from the original in that the participants were adults; and the amount of training was much greater (20 sessions with 60 practice trials each, spread out over 5-7 weeks). 





```{r}
#| label: fig-ex-design1
#| fig-width: 7
#| fig-height: 5
#| fig-cap: "A schematic representation of the @kerrSpecificVariedPractice1978 study design. The varied group trained from two distances (2 and 4 feet), while the constant group trained from a single distance (3 feet). Both groups were tested from a distance of 3 feet. The varied group outperformed the constant group at testing, despite the constant group having exclusively practiced from the testing distance."
pacman::p_load(dplyr, ggplot2, patchwork)

create_data <- function(varied_train, constant_train, both_test) {
  varied_labels <- "Varied-Train"
  constant_labels <- "Constant-Train"
  test_labels <- "Testing"
  
  data <- data.frame(
    x = c(varied_train, constant_train, both_test),
    y = c(rep(0.55, length(varied_train)), rep(0.75, length(constant_train)), rep(0.4, length(both_test))),
    label = c(rep(varied_labels, length(varied_train)),
              rep(constant_labels, length(constant_train)),
              rep(test_labels, length(both_test))),
    color = c(rep("#00A08A", length(varied_train)), rep("#FF0000", length(constant_train)), rep("#ECCBAE", length(both_test)))
  )
  
  data
}

create_segments <- function(data) {
  min_x <- min(data$x)
  max_x <- max(data$x)
  segments <- data.frame(
    x = c(min_x - 0.5, min_x - 0.5, min_x - 0.5),
    xend = c(max_x + 0.5, max_x + 0.5, max_x + 0.5),
    y = c(0.55, 0.75, 0.4),
    yend = c(0.55, 0.75, 0.4)
  )
  segments
}

create_plot <- function(varied_train, constant_train, both_test) {
  data <- create_data(varied_train, constant_train, both_test)
  segments <- create_segments(data)
  
  # Variables for Training segment position
  train_x <- min(data$x) - .5
  train_y_start <- 0.5
  train_y_end <- 0.8
  train_text_y <- 0.75
  
  # Variables for Testing segment position
  test_x <- min(data$x) - .5
  test_y_start <- 0.35
  test_y_end <- 0.45
  test_text_y <- 0.48
  
  ggplot() +
    geom_segment(data = segments, aes(x = x, xend = xend, y = y, yend = yend),
                 linewidth = 1.2, color = "gray40") +
    geom_point(data = data, aes(x = x, y = y, color = color), size = 12) +
    geom_text(data = data, aes(x = x, y = y, label = label),
              color = "black", size = 4.5, vjust = -1.8, hjust = 0.5, fontface = "italic") +
    geom_segment(aes(x = train_x - 0.3, xend = train_x - 0.3, y = train_y_start, yend = train_y_end), color = "black", linewidth = 1.2) +
    geom_segment(aes(x = train_x - 0.5, xend = train_x, y = train_y_end, yend = train_y_end), color = "black", linewidth = 1.2) +
    geom_segment(aes(x = train_x - 0.5, xend = train_x, y = train_y_start, yend = train_y_start), color = "black", linewidth = 1.2) +
    geom_text(aes(x = train_x - .6, y = train_text_y, label = "Training"), angle = 90, vjust = 0.5, hjust = 1.5, size = 4.5, fontface = "bold") +
    geom_segment(aes(x = test_x - 0.3, xend = test_x - 0.3, y = test_y_start, yend = test_y_end), color = "black", linewidth = 1.2) +
    geom_segment(aes(x = test_x - 0.5, xend = test_x, y = test_y_end, yend = test_y_end), color = "black", linewidth = 1.2) +
    geom_segment(aes(x = test_x - 0.5, xend = test_x, y = test_y_start, yend = test_y_start), color = "black", linewidth = 1.2) +
    geom_text(aes(x = test_x - .6, y = test_text_y, label = "Testing"), angle = 90, vjust = 0.5, hjust = 1.5, size = 4.5, fontface = "bold") +
    scale_color_manual(values = c("#00A08A","#ECCBAE", "#FF0000")) +
    xlim(min(data$x) - 2, max(data$x) + 1) +
    ylim(0.3, 1) +
    theme_void() +
    theme(legend.position = "none",
          plot.margin = margin(10, 20, 10, 20),
          plot.background = element_rect(fill = "white", color = NA),
          panel.background = element_rect(fill = "white", color = NA))
}

kerr_booth_1978 <- create_plot(varied_train = c(2, 4), constant_train = c(3), both_test = c(3))
kerr_booth_1978
```



Pitting varied against constant practice against each other on the home turf of the constant group provides a compelling argument for the benefits of varied training, as well as an interesting challenge for theoretical accounts that posit generalization to occur as some function of distance. However, despite its appeal this contrast is relatively uncommon in the literature. It is unclear whether this may be cause for concern over publication bias, or just researchers feeling the design is too risky. A far more common design is to have separate constant groups that each train exclusively from each of the conditions that the varied group encounters [@catalanoDistantTransferCoincident1984a; @chuaPracticeVariabilityPromotes2019; @newellVariabilityPracticeTransfer1976; @moxleySchemaVariabilityPractice1979; @mccrackenTestSchemaTheory1977], or for a single constant group to train from just one of the conditions experienced by the varied participants [@pigottMotorSchemaStructure1984; @rollerVariablePracticeLenses2001; @wrisbergTrainingProductionNovel1984; @wrisbergDevelopingCoincidentTiming1983]. A less common contrast places the constant group training in a region of the task space outside of the range of examples experienced by the varied group, but distinct from the transfer condition [@wrisbergVariabilityPracticeHypothesis1987; @wulfVariabilityPracticeImplicit1997]. Of particular relevance to the current work is the early study of @catalanoDistantTransferCoincident1984a, as theirs was one of the earliest studies to investigate the influence of varied vs. constant training on multiple testing locations of graded distance from the training condition. Participants were trained on coincident timing task, in which subjects observe a series of lightbulbs turning on sequentially at a consistent rate and attempt to time a button response with the onset of the final bulb. The constant groups trained with a single velocity of either 5,7,9, or 11 mph, while the varied group trained from all 4 of these velocities. Participants were then assigned to one of four possible generalization conditions, all of which fell outside of the range of the varied training conditions -- 1, 3, 13 or 15 mph. As is often the case, the varied group performed worse during the training phase. In the testing phase, the general pattern was for all participants to perform worse as the testing conditions became further away from the training conditions, but since the drop off in performance as a function of distance was far less steep for the varied group, the authors suggested that varied training induced a decremented generalization gradient, such that the varied participants were less affected by the change between training and testing conditions.

Benefits of varied training have also been observed in many studies outside of the sensorimotor domain. @goodeSuperiorityVariableRepeated2008 trained participants to solve anagrams of 40 different words ranging in length from 5 to 11 letters, with an anagram of each word repeated 3 times throughout training, for a total of 120 training trials. Although subjects in all conditions were exposed to the same 40 unique words (i.e. the solution to an anagram), participants in the varied group saw 3 different arrangements for each solution-word, such as DOLOF, FOLOD, and OOFLD for the solution word FLOOD, whereas constant subjects would train on three repetitions of LDOOF (spread evenly across training). Two different constant groups were used. Both constant groups trained with three repetitions of the same word scramble, but for constant group A, the testing phase consisted of the identical letter arrangement to that seen during training (e.g., LDOOF), whereas for constant group B, the testing phase consisted of an arrangement they had not seen during training, thus presenting them with a testing situation similar situation to the varied group. At the testing stage, the varied group outperformed both constant groups, a particularly impressive result, given that constant group A had three prior exposures to the word arrangement (i.e. the particular permutation of letters) which the varied group had not explicitly seen. However varied subjects in this study did not exhibit the typical decrement in the training phase typical of other varied manipulations in the literature, and achieved higher levels of anagram solving accuracy by the end of training than either of the constant groups -- solving two more anagrams on average than the constant group. This might suggest that for tasks of this nature where the learner can simply get stuck with a particular word scramble, repeated exposure to the identical scramble might be less helpful towards finding the solution than being given a different arrangement of the same letters. This contention is supported by the fact that constant group A, who was tested on the identical arrangement as they experienced during training, performed no better at testing than did constant group B, who had trained on a different arrangement of the same word solution -- further suggesting that there may not have been a strong identity advantage in this task.

In the domain of category learning, the constant vs. varied comparison is much less suitable. Instead, researchers will typically employ designs where all training groups encounter numerous stimuli, but one group experiences a greater number of unique exemplars [@wahlheimMetacognitiveJudgmentsRepetition2012; @nosofskyModelguidedSearchOptimal2019; @doyleMetacognitiveMonitoringCategory2016; @hoschPriorExperienceVariability2023; @brunsteinPreparingNoveltyDiverse2011], or designs where the number of unique training exemplars is held constant, but one group trains with items that are more dispersed, or spread out across the category space [@posnerGenesisAbstractIdeas1968; @homaCategoryBreadthAbstraction1976; @huHighvariabilityTrainingDoes2024; @bowmanTrainingSetCoherence2020; @maddoxStimulusRangeDiscontinuity2011].

Much of the earlier work in this sub-area trained subjects on artificial categories, such as dot patterns [@homaCategoryBreadthAbstraction1976; @posnerGenesisAbstractIdeas1968]. A seminal study by @posnerGenesisAbstractIdeas1968 trained participants to categorize artificial dot patterns, manipulating whether learners were trained with low variability examples clustered close to the category prototypes (i.e. low distortion training patterns), or higher-variability patterns spread further away from the prototype (i.e. high-distortion patterns). Participants that received training on more highly-distorted items showed superior generalization to novel high distortion patterns in the subsequent testing phase. It should be noted that unlike the sensorimotor studies discussed earlier, the @posnerGenesisAbstractIdeas1968 study did not present low-varied and high-varied participants with an equal number of training trials, but instead had participants remain in the training stage of the experiment until they reached a criterion level of performance. This train-until-criterion procedure led to the high-variability condition participants tending to complete a larger number of training trials before switching to the testing stage. More recent work [@huHighvariabilityTrainingDoes2024] also used dot pattern categories, but matched the number of training trials across conditions. Under this procedure, higher-variability participants tended to reach lower levels of performance by the end of the training stage. The results in the testing phase were the opposite of @posnerGenesisAbstractIdeas1968, with the low-variability training group showing superior generalization to novel high-distortion patterns (as well as generalization to novel patterns of low or medium distortion levels). However, whether this discrepancy is solely a result of the different training procedures is unclear, as the studies also differed in the nature of the prototype patterns used. @posnerGenesisAbstractIdeas1968 utilized simpler, recognizable prototypes (e.g., a triangle, the letter M, the letter F), while @huHighvariabilityTrainingDoes2024 employed random prototype patterns.

Recent studies have also begun utilizing more complex or realistic stimuli when assessing the influence of variability on category learning. @wahlheimMetacognitiveJudgmentsRepetition2012 conducted one such study. In a within-participants design, participants were trained on bird categories with either many repetitions of a few exemplars, or with few repetitions of many exemplars. Across four different experiments, which were conducted to address an unrelated question on metacognitive judgements, the researchers consistently found that participants generalized better to novel species following training with more unique exemplars (i.e. higher variability), while high repetition training produced significantly better performance categorizing the specific species they had trained on. A variability advantage was also found in the relatively complex domain of rock categorization [@nosofskyModelguidedSearchOptimal2019]. For 10 different rock categories, participants were trained with either many repetitions of 3 unique examples of each category, or few repetitions of 9 unique examples, with an equal number of total training trials in each group (the design also included 2 other conditions less amenable to considering the impact of variation). The high-variability group, trained with 9 unique examples, showed significantly better generalization performance than the other conditions. 

A distinct sub-literature within the category learning domain has examined how the variability or dispersion of the categories themselves influences generalization to ambiguous regions of the category space (e.g., the region between the two categories). The general approach is to train participants with examples from a high variability category and a low variability category. Participants are then tested with novel items located within ambiguous regions of the category space which allow the experimenters to assess whether the difference in category variability influenced how far participants generalize the category boundaries. @cohenCategoryVariabilityExemplar2001 conducted two experiments with this basic paradigm. In experiment 1, a low variability category composed of 1 instance was compared against a high-variability category of 2 instances in one condition, and 7 instances in another. In experiment 2 both categories were composed of 3 instances, but for the low-variability group the instances were clustered close to each other, whereas the high-variability groups instances were spread much further apart. Participants were tested on an ambiguous novel instance that was located in between the two trained categories. Both experiments provided evidence that participants were much more likely to categorize the novel middle stimulus into the category with greater variation.

Further observations of widened generalization following varied training have since been observed in numerous investigations [@hahnEffectsCategoryDiversity2005; @perlmanFurtherAttemptsClarify2012; @sakamotoPuttingPsychologyBack2008; @hsuEffectsGenerativeDiscriminative2010; @hoschPriorExperienceVariability2023; but see @stewartEffectCategoryVariability2002; @yangCategoryVariabilityEffect2014; and @seitzModelingCategoryVariability2023]. The results of @sakamotoPuttingPsychologyBack2008 are noteworthy. They first reproduced the basic finding of participants being more likely to categorize an unknown middle stimulus into a training category with higher variability. In a second experiment, they held the variability between the two training categories constant and instead manipulated the training sequence, such that the examples of one category appeared in an ordered fashion, with very small changes from one example to the other (the stimuli were lines that varied only in length), whereas examples in the alternate category were shown in a random order and thus included larger jumps in the stimulus space from trial to trial. They found that the middle stimulus was more likely to be categorized into the category that had been learned with a random sequence, which was attributed to an increased perception of variability which resulted from the larger trial to trial discrepancies.

The work of @hahnEffectsCategoryDiversity2005, is also of particular interest to the present work. Their experimental design was similar to previous studies, but they included a larger set of testing items which were used to assess generalization both between the two training categories as well as novel items located in the outer edges of the training categories. During generalization testing, participants were given the option to respond with "neither", in addition to responses to the two training categories. The "neither" response was included to test how far away in the stimulus space participants would continue to categorize novel items as belonging to a trained category. Consistent with prior findings, high-variability training resulted in an increased probability of categorizing items in between the training categories as belong to the high variability category. Additionally, participants trained with higher variability also extended the category boundary further out into the periphery than participants trained with a lower variability category were willing to do. The author compared a variety of similarity-based models based around the Generalized Context Model [@nosofskyAttentionSimilarityIdentificationcategorization1986] to account for their results, manipulating whether a response-bias or similarity-scaling parameter was fit separately between variability conditions. No improvement in model fit was found by allowing the response-bias parameter to differ between groups, however the model performance did improve significantly when the similarity scaling parameter was fit separately. The best fitting similarity-scaling parameters were such that the high-variability group was less sensitive to the distances between stimuli, resulting in greater similarity values between their training items and testing items. This model accounted for both the extended generalization gradients of the varied participants, and for their poorer performance in a recognition condition.

Variability has also been examined in the learning of higher-order linguistic categories [@perryLearnLocallyThink2010]. In nine training sessions spread out over nine weeks infants were trained on object labels in a naturalistic play setting. All infants were introduced to three novel objects of the same category, with participants in the "tight" condition being exposed to three similar exemplars of the category, and participants in the varied condition being exposed to three dissimilar objects of the same category. Importantly, the similarity of the objects was carefully controlled for by having a separate group of adult subjects provide pairwise similarity judgements of the category objects prior to the study onset. Multidimensional scaling was then performed to obtain the coordinates of the objects psychological space, and out of the 10 objects for each category, the 3 most similar objects were selected for the tight group and the three least similar objects for the varied group, with the leftover four objects being retained for testing. By the end of the nine weeks, all of the infants had learned the labels of the training objects. In the testing phase, the varied group demonstrated superior ability to correctly generalize the object labels to untrained exemplars of the same category. More interesting was the superior performance of the varied group on a higher order generalization task -- such that they were able to appropriately generalize the bias they had learned during training for attending to the shape of objects to novel solid objects, but not to non-solids. The tight training group, on the other hand, tended to overgeneralize the shape bias, leading the researchers to suggest that the varied training induced a more context-sensitive understanding of when to apply their knowledge.

Of course, the relationship between training variability and transfer is unlikely to be a simple function wherein increased variation is always beneficial. Numerous studies have found null, or in some cases negative effects of training variation [@deloshExtrapolationSineQua1997; @sinkeviciuteRoleInputVariability2019; @wrisbergVariabilityPracticeHypothesis1987; @vanrossumSchmidtSchemaTheory1990], and many more have suggested that the benefits of variability may depend on additional factors such as prior task experience, the order of training trials, or the type of transfer being measured [@bernikerEffectsTrainingBreadth2014; @braithwaiteEffectsVariationPrior2015; @hahnEffectsCategoryDiversity2005; @lavanEffectsHighVariability2019; @northEffectConsistentVaried2019; @sadakataIndividualAptitudeMandarin2014; @zamanPerceptualVariabilityImplications2021].

In an example of a more complex influence of training variation, [@braithwaiteEffectsVariationPrior2015] trained participants on example problems involving the concept of sampling with replacement (SWR). Training consisted of examples that were either highly similar in their semantic context (e.g., all involving people selecting objects) or in which the surface features were varied between examples (e.g., people choosing objects AND objects selected in a sequence). The experimenters also surveyed how much prior knowledge each participant had with SWR. They found that whether variation was beneficial depended on the prior knowledge of the participants -- such that participants with some prior knowledge benefited from varied training, whereas participants with minimal prior knowledge performed better after training with similar examples. The authors hypothesized that to benefit from varied examples, participants must be able to detect the structure common to the diverse examples, and that participants with prior knowledge are more likely to be sensitive to such structure, and thus to benefit from varied training. To test this hypothesis more directly, the authors conducted a 2nd experiment, wherein they controlled prior knowledge by exposing some subjects to a short graphical or verbal pre-training lesson, designed to increase sensitivity to the training examples. Consistent with their hypothesis, participants exposed to the structural sensitivity pre-training benefited more from varied training than the controls participants who benefited more from training with similar examples. Interactions between prior experience and the influence of varied training have also been observed in sensorimotor learning [@guadagnoliRelationshipContextualInterference1999;  @delreyEffectsContextualInterference1982].  @delreyEffectsContextualInterference1982 recruited participants who self-reported either extensive, or very little experience with athletic activities, and then trained participants on a coincident timing task with either a single constant training velocity, or with one of several varied training procedures. Unsurprisingly, athlete participants had superior performance during training, regardless of  condition, and training performance was superior for all subjects in the constant group. Of greater interest is the pattern of testing results from novel transfer conditions. Among the athlete-participants, transfer performance was best for those who received variable training. Non-athletes showed the opposite pattern, with superior performance for those who had constant training. 

## Existing Theoretical Frameworks

Several theoretical frameworks have been proposed to conceptually explain the effects of varied training on learning and generalization. Schema theory (described in more detail above), posts that varied practice leads to the formation of more flexible motor schemas, which then facilitate generalization [@schmidtSchemaTheoryDiscrete1975]. The desirable difficulties framework [@bjorkMakingThingsHard2011; @soderstromLearningPerformanceIntegrative2015] proposes that variable practice conditions may impair initial performance but then enhance longer-term retention and transfer. Similarly, the challenge point framework [@guadagnoliChallengePointFramework2004] contends that training variation induces optimal learning occurs insofar as it causes the difficulty of practice tasks to be appropriately matched to the learner's capabilities, but may also be detrimental if the amount of variation causes the task to be too difficult.

While these frameworks offer valuable conceptual accounts, there has been a limited application of computational modeling efforts aimed at quantitatively assessing and comparing the learning and generalization mechanisms which may be underlying the influence of variability in visuomotor skill learning. In contrast, the effects of variability have received more formal computational treatment in other domains, such as category learning [@hahnEffectsCategoryDiversity2005 @huHighvariabilityTrainingDoes2024], language learning [@jonesDensityDistinctivenessEarly2020], and function learning [@deloshExtrapolationSineQua1997]. A primary goal of the current dissertation is to address this gap by adapting and applying modeling approaches from these other domains to investigate the effects of training variability in visuomotor skill learning and function learning tasks.

## The current work

The overarching purpose of this dissertation is to investigate the effects of training variability on learning and generalization within visuomotor skill learning and function learning. Our investigation is structured into two main projects, each employing distinct experimental paradigms and computational modeling frameworks to elucidate how and when variability in training enhances or impedes subsequent generalization.

In Project 1, we investigated the influence of varied practice in a simple visuomotor projectile launching task. Experiments 1 and 2 compared the performance of constant and varied training groups to assess potential benefits of variability on transfer to novel testing conditions. To account for the observed empirical effects, we introduced the Instance-based Generalization with Adaptive Similarity (IGAS) model. IGAS provides a novel computational approach for quantifying the similarity between training experiences and transfer conditions, while also allowing for variability to influence the generalization gradient itself.

Project 2 will focus on the domain of function learning and in particular the issue of extrapolation. Function learning research examines how people acquire and generalize knowledge about continuous input-output relationships, and the factors influencing extrapolation to novel inputs following an initial learning phase. The domain of function learning has yielded influential computational models, including the Associative Learning Model (ALM) and the Extrapolation-Association Model (EXAM)[@busemeyerLearningFunctionalRelations1997], which have successfully accounted for human learning, interpolation, and extrapolation in numerous investigations[@deloshExtrapolationSineQua1997; @mcdanielConceptualBasisFunction2005; @mcdanielPredictingTransferPerformance2009]. However, the influence of training variability on function learning, particularly in visuomotor function learning tasks, remains relatively unexplored. Project 2 of this dissertation will address this gap by investigating how constant and varied training regimes affect learning, discrimination, and extrapolation in a novel visuomotor function learning task. We will leverage the ALM and EXAM models, fitted to individual participant data using advanced Bayesian techniques, to provide a detailed computational account of the observed empirical patterns.